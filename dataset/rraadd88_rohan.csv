repository_name,func_path_in_repository,func_name,whole_func_string,language,func_code_string,func_code_tokens,func_documentation_string,func_documentation_tokens,split_name,func_code_url
rraadd88/rohan,rohan/dandage/align/align_annot.py,dqueries2queriessam,"def dqueries2queriessam(cfg,dqueries):    
    """"""
    Aligns queries to genome and gets SAM file
    step#1

    :param cfg: configuration dict
    :param dqueries: dataframe of queries
    """"""
    datatmpd=cfg['datatmpd']
    dqueries=set_index(dqueries,'query id')
    queryls=dqueries.loc[:,'query sequence'].apply(len).unique()
    for queryl in queryls:
        logging.debug(f""now aligning queries of length {queryl}"")
        queriesfap = f'{datatmpd}/01_queries_queryl{queryl:02}.fa'
        logging.info(basename(queriesfap))
        if not exists(queriesfap) or cfg['force']:
            with open(queriesfap,'w') as f:
                for gi in dqueries.index:
                    f.write('>{}\n{}\n'.format(gi.replace(' ','_'),dqueries.loc[gi,'query sequence']))
        ## BWA alignment command is adapted from cripror 
        ## https://github.com/rraadd88/crisporWebsite/blob/master/crispor.py
        # BWA allow up to X mismatches
        # maximum number of occurences in the genome to get flagged as repeats. 
        # This is used in bwa samse, when converting the sam file
        # and for warnings in the table output.
        MAXOCC = 60000

        # the BWA queue size is 2M by default. We derive the queue size from MAXOCC
        MFAC = 2000000/MAXOCC

        genomep=cfg['genomep']
        genomed = dirname(genomep) # make var local, see below
        genomegffp=cfg['genomegffp']

        # increase MAXOCC if there is only a single query, but only in CGI mode
        bwaM = MFAC*MAXOCC # -m is queue size in bwa
        queriessap = f'{datatmpd}/01_queries_queryl{queryl:02}.sa'
        logging.info(basename(queriessap))
        if not exists(queriessap) or cfg['force']:
            cmd=f""{cfg['bwa']} aln -t 1 -o 0 -m {bwaM} -n {cfg['mismatches_max']} -k {cfg['mismatches_max']} -N -l {queryl} {genomep} {queriesfap} > {queriessap} 2> {queriessap}.log""
            runbashcmd(cmd)

        queriessamp = f'{datatmpd}/01_queries_queryl{queryl:02}.sam'
        logging.info(basename(queriessamp))        
        if not exists(queriessamp) or cfg['force']:
            cmd=f""{cfg['bwa']} samse -n {MAXOCC} {genomep} {queriessap} {queriesfap} > {queriessamp} 2> {queriessamp}.log""
            runbashcmd(cmd)
    return cfg",python,"def dqueries2queriessam(cfg,dqueries):    
    """"""
    Aligns queries to genome and gets SAM file
    step#1

    :param cfg: configuration dict
    :param dqueries: dataframe of queries
    """"""
    datatmpd=cfg['datatmpd']
    dqueries=set_index(dqueries,'query id')
    queryls=dqueries.loc[:,'query sequence'].apply(len).unique()
    for queryl in queryls:
        logging.debug(f""now aligning queries of length {queryl}"")
        queriesfap = f'{datatmpd}/01_queries_queryl{queryl:02}.fa'
        logging.info(basename(queriesfap))
        if not exists(queriesfap) or cfg['force']:
            with open(queriesfap,'w') as f:
                for gi in dqueries.index:
                    f.write('>{}\n{}\n'.format(gi.replace(' ','_'),dqueries.loc[gi,'query sequence']))
        ## BWA alignment command is adapted from cripror 
        ## https://github.com/rraadd88/crisporWebsite/blob/master/crispor.py
        # BWA allow up to X mismatches
        # maximum number of occurences in the genome to get flagged as repeats. 
        # This is used in bwa samse, when converting the sam file
        # and for warnings in the table output.
        MAXOCC = 60000

        # the BWA queue size is 2M by default. We derive the queue size from MAXOCC
        MFAC = 2000000/MAXOCC

        genomep=cfg['genomep']
        genomed = dirname(genomep) # make var local, see below
        genomegffp=cfg['genomegffp']

        # increase MAXOCC if there is only a single query, but only in CGI mode
        bwaM = MFAC*MAXOCC # -m is queue size in bwa
        queriessap = f'{datatmpd}/01_queries_queryl{queryl:02}.sa'
        logging.info(basename(queriessap))
        if not exists(queriessap) or cfg['force']:
            cmd=f""{cfg['bwa']} aln -t 1 -o 0 -m {bwaM} -n {cfg['mismatches_max']} -k {cfg['mismatches_max']} -N -l {queryl} {genomep} {queriesfap} > {queriessap} 2> {queriessap}.log""
            runbashcmd(cmd)

        queriessamp = f'{datatmpd}/01_queries_queryl{queryl:02}.sam'
        logging.info(basename(queriessamp))        
        if not exists(queriessamp) or cfg['force']:
            cmd=f""{cfg['bwa']} samse -n {MAXOCC} {genomep} {queriessap} {queriesfap} > {queriessamp} 2> {queriessamp}.log""
            runbashcmd(cmd)
    return cfg","['def' 'dqueries2queriessam' '(' 'cfg' ',' 'dqueries' ')' ':' 'datatmpd'
 '=' 'cfg' '[' ""'datatmpd'"" ']' 'dqueries' '=' 'set_index' '(' 'dqueries'
 ',' ""'query id'"" ')' 'queryls' '=' 'dqueries' '.' 'loc' '[' ':' ','
 ""'query sequence'"" ']' '.' 'apply' '(' 'len' ')' '.' 'unique' '(' ')'
 'for' 'queryl' 'in' 'queryls' ':' 'logging' '.' 'debug' '('
 'f""now aligning queries of length {queryl}""' ')' 'queriesfap' '='
 ""f'{datatmpd}/01_queries_queryl{queryl:02}.fa'"" 'logging' '.' 'info' '('
 'basename' '(' 'queriesfap' ')' ')' 'if' 'not' 'exists' '(' 'queriesfap'
 ')' 'or' 'cfg' '[' ""'force'"" ']' ':' 'with' 'open' '(' 'queriesfap' ','
 ""'w'"" ')' 'as' 'f' ':' 'for' 'gi' 'in' 'dqueries' '.' 'index' ':' 'f' '.'
 'write' '(' ""'>{}\\n{}\\n'"" '.' 'format' '(' 'gi' '.' 'replace' '(' ""' '""
 ',' ""'_'"" ')' ',' 'dqueries' '.' 'loc' '[' 'gi' ',' ""'query sequence'""
 ']' ')' ')' '## BWA alignment command is adapted from cripror '
 '## https://github.com/rraadd88/crisporWebsite/blob/master/crispor.py'
 '# BWA allow up to X mismatches'
 '# maximum number of occurences in the genome to get flagged as repeats. '
 '# This is used in bwa samse, when converting the sam file'
 '# and for warnings in the table output.' 'MAXOCC' '=' '60000'
 '# the BWA queue size is 2M by default. We derive the queue size from MAXOCC'
 'MFAC' '=' '2000000' '/' 'MAXOCC' 'genomep' '=' 'cfg' '[' ""'genomep'"" ']'
 'genomed' '=' 'dirname' '(' 'genomep' ')' '# make var local, see below'
 'genomegffp' '=' 'cfg' '[' ""'genomegffp'"" ']'
 '# increase MAXOCC if there is only a single query, but only in CGI mode'
 'bwaM' '=' 'MFAC' '*' 'MAXOCC' '# -m is queue size in bwa' 'queriessap'
 '=' ""f'{datatmpd}/01_queries_queryl{queryl:02}.sa'"" 'logging' '.' 'info'
 '(' 'basename' '(' 'queriessap' ')' ')' 'if' 'not' 'exists' '('
 'queriessap' ')' 'or' 'cfg' '[' ""'force'"" ']' ':' 'cmd' '='
 'f""{cfg[\'bwa\']} aln -t 1 -o 0 -m {bwaM} -n {cfg[\'mismatches_max\']} -k {cfg[\'mismatches_max\']} -N -l {queryl} {genomep} {queriesfap} > {queriessap} 2> {queriessap}.log""'
 'runbashcmd' '(' 'cmd' ')' 'queriessamp' '='
 ""f'{datatmpd}/01_queries_queryl{queryl:02}.sam'"" 'logging' '.' 'info' '('
 'basename' '(' 'queriessamp' ')' ')' 'if' 'not' 'exists' '('
 'queriessamp' ')' 'or' 'cfg' '[' ""'force'"" ']' ':' 'cmd' '='
 'f""{cfg[\'bwa\']} samse -n {MAXOCC} {genomep} {queriessap} {queriesfap} > {queriessamp} 2> {queriessamp}.log""'
 'runbashcmd' '(' 'cmd' ')' 'return' 'cfg']","Aligns queries to genome and gets SAM file
    step#1

    :param cfg: configuration dict
    :param dqueries: dataframe of queries",['Aligns' 'queries' 'to' 'genome' 'and' 'gets' 'SAM' 'file' 'step#1'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L25-L72
rraadd88/rohan,rohan/dandage/align/align_annot.py,queriessam2dalignbed,"def queriessam2dalignbed(cfg):
    """"""
    Processes SAM file to get the genomic coordinates in BED format
    step#2

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    alignmentbedp=cfg['alignmentbedp']
    dalignbedp=cfg['dalignbedp']
    logging.info(basename(dalignbedp))
    if not exists(alignmentbedp) or cfg['force']:
        #input/s
        queriessamps=glob(f'{datatmpd}/01_queries_queryl*.sam')
        for queriessamp in queriessamps:
            if stat(queriessamp).st_size != 0:
                samfile=pysam.AlignmentFile(queriessamp, ""rb"")
                dalignbed=pd.DataFrame(columns=bed_colns)
                for read in samfile.fetch():
                    algnids=[]
                    
#                     read_position=read.positions[0]
#                     tag_NM=read.get_tag('NM')                    
                    if len(read.positions)!=0:
                        read_position=read.positions[0]
                    else:
                        logging.error('no alignments found')
                        print(read)
                        continue
#                         read_position=[None]
                    try: 
                        tag_NM=read.get_tag('NM')
                    except: 
                        logging.error('no NM tag found')
                        print(read)
                        continue
#                         tag_NM=None
                    algnids.append(f""{read.reference_name}|{'-' if read.is_reverse else '+'}{read_position}|{read.cigarstring}|{tag_NM}"")
                    if read.has_tag('XA'):
                        algnids+=['|'.join(s.split(',')) for s in read.get_tag('XA').split(';') if len(s.split(','))>1]
#                     print(len(algnids))
                    chroms=[]
                    starts=[]
                    ends=[]
                    algnids=algnids[:20]
                    NMs=[]
                    strands=[]
                    for a in algnids:
                        strand=a.split('|')[1][0]
                        chroms.append(a.split('|')[0])
                        if strand=='+':
                            offset=0
                        elif strand=='-':
                            offset=0                    
                        starts.append(int(a.split('|')[1][1:])+offset)
                        ends.append(int(a.split('|')[1][1:])+str2num(a.split('|')[2])+offset)
                        NMs.append(a.split('|')[3])
                        strands.append(strand)
                        del strand,offset
                    col2dalignbed={'chromosome':chroms,
                                   'start':starts,
                                   'end':ends,
                                   'id':algnids,
                                   'NM':NMs,
                                   'strand':strands}
                #     col2dalignbed=dict(zip(cols,[a.split('|')[0],a.split('|')[1],a.split('|')[2],a,a.split('|')[3],a.split('|')[4] for a in algnids]))
                    dalignbed_=pd.DataFrame(col2dalignbed)
                    dalignbed_['query id']=read.qname.replace('_',' ')
                    dalignbed = dalignbed.append(dalignbed_,ignore_index=True,sort=True)
                #     break
                samfile.close()
            else:
                logging.warning(f""file is empty: {queriessamp}"")
        dalignbed.to_csv(dalignbedp,sep='\t')
        from rohan.dandage.io_nums import str2numorstr
        dalignbed['chromosome']=dalignbed.apply(lambda x : str2numorstr(x['chromosome']),axis=1)
        dalignbed=dalignbed.sort_values(['chromosome','start','end'], ascending=[True, True, True])
        dalignbed.loc[:,bed_colns].to_csv(alignmentbedp,sep='\t',
                        header=False,index=False,
                        chunksize=5000)
    return cfg",python,"def queriessam2dalignbed(cfg):
    """"""
    Processes SAM file to get the genomic coordinates in BED format
    step#2

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    alignmentbedp=cfg['alignmentbedp']
    dalignbedp=cfg['dalignbedp']
    logging.info(basename(dalignbedp))
    if not exists(alignmentbedp) or cfg['force']:
        #input/s
        queriessamps=glob(f'{datatmpd}/01_queries_queryl*.sam')
        for queriessamp in queriessamps:
            if stat(queriessamp).st_size != 0:
                samfile=pysam.AlignmentFile(queriessamp, ""rb"")
                dalignbed=pd.DataFrame(columns=bed_colns)
                for read in samfile.fetch():
                    algnids=[]
                    
#                     read_position=read.positions[0]
#                     tag_NM=read.get_tag('NM')                    
                    if len(read.positions)!=0:
                        read_position=read.positions[0]
                    else:
                        logging.error('no alignments found')
                        print(read)
                        continue
#                         read_position=[None]
                    try: 
                        tag_NM=read.get_tag('NM')
                    except: 
                        logging.error('no NM tag found')
                        print(read)
                        continue
#                         tag_NM=None
                    algnids.append(f""{read.reference_name}|{'-' if read.is_reverse else '+'}{read_position}|{read.cigarstring}|{tag_NM}"")
                    if read.has_tag('XA'):
                        algnids+=['|'.join(s.split(',')) for s in read.get_tag('XA').split(';') if len(s.split(','))>1]
#                     print(len(algnids))
                    chroms=[]
                    starts=[]
                    ends=[]
                    algnids=algnids[:20]
                    NMs=[]
                    strands=[]
                    for a in algnids:
                        strand=a.split('|')[1][0]
                        chroms.append(a.split('|')[0])
                        if strand=='+':
                            offset=0
                        elif strand=='-':
                            offset=0                    
                        starts.append(int(a.split('|')[1][1:])+offset)
                        ends.append(int(a.split('|')[1][1:])+str2num(a.split('|')[2])+offset)
                        NMs.append(a.split('|')[3])
                        strands.append(strand)
                        del strand,offset
                    col2dalignbed={'chromosome':chroms,
                                   'start':starts,
                                   'end':ends,
                                   'id':algnids,
                                   'NM':NMs,
                                   'strand':strands}
                #     col2dalignbed=dict(zip(cols,[a.split('|')[0],a.split('|')[1],a.split('|')[2],a,a.split('|')[3],a.split('|')[4] for a in algnids]))
                    dalignbed_=pd.DataFrame(col2dalignbed)
                    dalignbed_['query id']=read.qname.replace('_',' ')
                    dalignbed = dalignbed.append(dalignbed_,ignore_index=True,sort=True)
                #     break
                samfile.close()
            else:
                logging.warning(f""file is empty: {queriessamp}"")
        dalignbed.to_csv(dalignbedp,sep='\t')
        from rohan.dandage.io_nums import str2numorstr
        dalignbed['chromosome']=dalignbed.apply(lambda x : str2numorstr(x['chromosome']),axis=1)
        dalignbed=dalignbed.sort_values(['chromosome','start','end'], ascending=[True, True, True])
        dalignbed.loc[:,bed_colns].to_csv(alignmentbedp,sep='\t',
                        header=False,index=False,
                        chunksize=5000)
    return cfg","['def' 'queriessam2dalignbed' '(' 'cfg' ')' ':' 'datatmpd' '=' 'cfg' '['
 ""'datatmpd'"" ']' 'alignmentbedp' '=' 'cfg' '[' ""'alignmentbedp'"" ']'
 'dalignbedp' '=' 'cfg' '[' ""'dalignbedp'"" ']' 'logging' '.' 'info' '('
 'basename' '(' 'dalignbedp' ')' ')' 'if' 'not' 'exists' '('
 'alignmentbedp' ')' 'or' 'cfg' '[' ""'force'"" ']' ':' '#input/s'
 'queriessamps' '=' 'glob' '(' ""f'{datatmpd}/01_queries_queryl*.sam'"" ')'
 'for' 'queriessamp' 'in' 'queriessamps' ':' 'if' 'stat' '(' 'queriessamp'
 ')' '.' 'st_size' '!=' '0' ':' 'samfile' '=' 'pysam' '.' 'AlignmentFile'
 '(' 'queriessamp' ',' '""rb""' ')' 'dalignbed' '=' 'pd' '.' 'DataFrame' '('
 'columns' '=' 'bed_colns' ')' 'for' 'read' 'in' 'samfile' '.' 'fetch' '('
 ')' ':' 'algnids' '=' '[' ']'
 '#                     read_position=read.positions[0]'
 ""#                     tag_NM=read.get_tag('NM')                    ""
 'if' 'len' '(' 'read' '.' 'positions' ')' '!=' '0' ':' 'read_position'
 '=' 'read' '.' 'positions' '[' '0' ']' 'else' ':' 'logging' '.' 'error'
 '(' ""'no alignments found'"" ')' 'print' '(' 'read' ')' 'continue'
 '#                         read_position=[None]' 'try' ':' 'tag_NM' '='
 'read' '.' 'get_tag' '(' ""'NM'"" ')' 'except' ':' 'logging' '.' 'error'
 '(' ""'no NM tag found'"" ')' 'print' '(' 'read' ')' 'continue'
 '#                         tag_NM=None' 'algnids' '.' 'append' '('
 'f""{read.reference_name}|{\'-\' if read.is_reverse else \'+\'}{read_position}|{read.cigarstring}|{tag_NM}""'
 ')' 'if' 'read' '.' 'has_tag' '(' ""'XA'"" ')' ':' 'algnids' '+=' '[' ""'|'""
 '.' 'join' '(' 's' '.' 'split' '(' ""','"" ')' ')' 'for' 's' 'in' 'read'
 '.' 'get_tag' '(' ""'XA'"" ')' '.' 'split' '(' ""';'"" ')' 'if' 'len' '(' 's'
 '.' 'split' '(' ""','"" ')' ')' '>' '1' ']'
 '#                     print(len(algnids))' 'chroms' '=' '[' ']' 'starts'
 '=' '[' ']' 'ends' '=' '[' ']' 'algnids' '=' 'algnids' '[' ':' '20' ']'
 'NMs' '=' '[' ']' 'strands' '=' '[' ']' 'for' 'a' 'in' 'algnids' ':'
 'strand' '=' 'a' '.' 'split' '(' ""'|'"" ')' '[' '1' ']' '[' '0' ']'
 'chroms' '.' 'append' '(' 'a' '.' 'split' '(' ""'|'"" ')' '[' '0' ']' ')'
 'if' 'strand' '==' ""'+'"" ':' 'offset' '=' '0' 'elif' 'strand' '==' ""'-'""
 ':' 'offset' '=' '0' 'starts' '.' 'append' '(' 'int' '(' 'a' '.' 'split'
 '(' ""'|'"" ')' '[' '1' ']' '[' '1' ':' ']' ')' '+' 'offset' ')' 'ends' '.'
 'append' '(' 'int' '(' 'a' '.' 'split' '(' ""'|'"" ')' '[' '1' ']' '[' '1'
 ':' ']' ')' '+' 'str2num' '(' 'a' '.' 'split' '(' ""'|'"" ')' '[' '2' ']'
 ')' '+' 'offset' ')' 'NMs' '.' 'append' '(' 'a' '.' 'split' '(' ""'|'"" ')'
 '[' '3' ']' ')' 'strands' '.' 'append' '(' 'strand' ')' 'del' 'strand'
 ',' 'offset' 'col2dalignbed' '=' '{' ""'chromosome'"" ':' 'chroms' ','
 ""'start'"" ':' 'starts' ',' ""'end'"" ':' 'ends' ',' ""'id'"" ':' 'algnids'
 ',' ""'NM'"" ':' 'NMs' ',' ""'strand'"" ':' 'strands' '}'
 ""#     col2dalignbed=dict(zip(cols,[a.split('|')[0],a.split('|')[1],a.split('|')[2],a,a.split('|')[3],a.split('|')[4] for a in algnids]))""
 'dalignbed_' '=' 'pd' '.' 'DataFrame' '(' 'col2dalignbed' ')'
 'dalignbed_' '[' ""'query id'"" ']' '=' 'read' '.' 'qname' '.' 'replace'
 '(' ""'_'"" ',' ""' '"" ')' 'dalignbed' '=' 'dalignbed' '.' 'append' '('
 'dalignbed_' ',' 'ignore_index' '=' 'True' ',' 'sort' '=' 'True' ')'
 '#     break' 'samfile' '.' 'close' '(' ')' 'else' ':' 'logging' '.'
 'warning' '(' 'f""file is empty: {queriessamp}""' ')' 'dalignbed' '.'
 'to_csv' '(' 'dalignbedp' ',' 'sep' '=' ""'\\t'"" ')' 'from' 'rohan' '.'
 'dandage' '.' 'io_nums' 'import' 'str2numorstr' 'dalignbed' '['
 ""'chromosome'"" ']' '=' 'dalignbed' '.' 'apply' '(' 'lambda' 'x' ':'
 'str2numorstr' '(' 'x' '[' ""'chromosome'"" ']' ')' ',' 'axis' '=' '1' ')'
 'dalignbed' '=' 'dalignbed' '.' 'sort_values' '(' '[' ""'chromosome'"" ','
 ""'start'"" ',' ""'end'"" ']' ',' 'ascending' '=' '[' 'True' ',' 'True' ','
 'True' ']' ')' 'dalignbed' '.' 'loc' '[' ':' ',' 'bed_colns' ']' '.'
 'to_csv' '(' 'alignmentbedp' ',' 'sep' '=' ""'\\t'"" ',' 'header' '='
 'False' ',' 'index' '=' 'False' ',' 'chunksize' '=' '5000' ')' 'return'
 'cfg']","Processes SAM file to get the genomic coordinates in BED format
    step#2

    :param cfg: configuration dict","['Processes' 'SAM' 'file' 'to' 'get' 'the' 'genomic' 'coordinates' 'in'
 'BED' 'format' 'step#2']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L74-L154
rraadd88/rohan,rohan/dandage/align/align_annot.py,dalignbed2annotationsbed,"def dalignbed2annotationsbed(cfg):
    """"""
    Get annotations from the aligned BED file
    step#3

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    alignmentbedp=cfg['alignmentbedp']    
    alignmentbedsortedp=alignmentbedp+'.sorted.bed'
    logging.info(basename(alignmentbedsortedp))
    if not exists(alignmentbedsortedp) or cfg['force']:
        cmd=f""{cfg['bedtools']} sort -i {alignmentbedp} > {alignmentbedsortedp}""
        runbashcmd(cmd)
    
    genomegffsortedp=cfg['genomegffp']+'.sorted.gff3.gz'
    logging.info(basename(genomegffsortedp))
    if not exists(genomegffsortedp):    
        cmd=f""{cfg['bedtools']} sort -i {cfg['genomegffp']} > {genomegffsortedp}""
        runbashcmd(cmd)

    annotationsbedp=f'{datatmpd}/03_annotations.bed'
    cfg['annotationsbedp']=annotationsbedp
    logging.info(basename(annotationsbedp))
    if not exists(annotationsbedp) or cfg['force']:    
        cmd=f""{cfg['bedtools']} intersect -wa -wb -loj -a {alignmentbedsortedp} -b {genomegffsortedp} > {annotationsbedp}""
        runbashcmd(cmd)
    return cfg",python,"def dalignbed2annotationsbed(cfg):
    """"""
    Get annotations from the aligned BED file
    step#3

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    alignmentbedp=cfg['alignmentbedp']    
    alignmentbedsortedp=alignmentbedp+'.sorted.bed'
    logging.info(basename(alignmentbedsortedp))
    if not exists(alignmentbedsortedp) or cfg['force']:
        cmd=f""{cfg['bedtools']} sort -i {alignmentbedp} > {alignmentbedsortedp}""
        runbashcmd(cmd)
    
    genomegffsortedp=cfg['genomegffp']+'.sorted.gff3.gz'
    logging.info(basename(genomegffsortedp))
    if not exists(genomegffsortedp):    
        cmd=f""{cfg['bedtools']} sort -i {cfg['genomegffp']} > {genomegffsortedp}""
        runbashcmd(cmd)

    annotationsbedp=f'{datatmpd}/03_annotations.bed'
    cfg['annotationsbedp']=annotationsbedp
    logging.info(basename(annotationsbedp))
    if not exists(annotationsbedp) or cfg['force']:    
        cmd=f""{cfg['bedtools']} intersect -wa -wb -loj -a {alignmentbedsortedp} -b {genomegffsortedp} > {annotationsbedp}""
        runbashcmd(cmd)
    return cfg","['def' 'dalignbed2annotationsbed' '(' 'cfg' ')' ':' 'datatmpd' '=' 'cfg'
 '[' ""'datatmpd'"" ']' 'alignmentbedp' '=' 'cfg' '[' ""'alignmentbedp'"" ']'
 'alignmentbedsortedp' '=' 'alignmentbedp' '+' ""'.sorted.bed'"" 'logging'
 '.' 'info' '(' 'basename' '(' 'alignmentbedsortedp' ')' ')' 'if' 'not'
 'exists' '(' 'alignmentbedsortedp' ')' 'or' 'cfg' '[' ""'force'"" ']' ':'
 'cmd' '='
 'f""{cfg[\'bedtools\']} sort -i {alignmentbedp} > {alignmentbedsortedp}""'
 'runbashcmd' '(' 'cmd' ')' 'genomegffsortedp' '=' 'cfg' '['
 ""'genomegffp'"" ']' '+' ""'.sorted.gff3.gz'"" 'logging' '.' 'info' '('
 'basename' '(' 'genomegffsortedp' ')' ')' 'if' 'not' 'exists' '('
 'genomegffsortedp' ')' ':' 'cmd' '='
 'f""{cfg[\'bedtools\']} sort -i {cfg[\'genomegffp\']} > {genomegffsortedp}""'
 'runbashcmd' '(' 'cmd' ')' 'annotationsbedp' '='
 ""f'{datatmpd}/03_annotations.bed'"" 'cfg' '[' ""'annotationsbedp'"" ']' '='
 'annotationsbedp' 'logging' '.' 'info' '(' 'basename' '('
 'annotationsbedp' ')' ')' 'if' 'not' 'exists' '(' 'annotationsbedp' ')'
 'or' 'cfg' '[' ""'force'"" ']' ':' 'cmd' '='
 'f""{cfg[\'bedtools\']} intersect -wa -wb -loj -a {alignmentbedsortedp} -b {genomegffsortedp} > {annotationsbedp}""'
 'runbashcmd' '(' 'cmd' ')' 'return' 'cfg']","Get annotations from the aligned BED file
    step#3

    :param cfg: configuration dict",['Get' 'annotations' 'from' 'the' 'aligned' 'BED' 'file' 'step#3'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L156-L183
rraadd88/rohan,rohan/dandage/align/align_annot.py,dalignbed2dalignbedqueries,"def dalignbed2dalignbedqueries(cfg):
    """"""
    Get query seqeunces from the BED file
    step#4

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    dalignbed=del_Unnamed(pd.read_csv(cfg['dalignbedp'],sep='\t'))
    dqueries=set_index(del_Unnamed(pd.read_csv(cfg['dqueriesp'],sep='\t')),'query id')
    
#     if the error in human, use: `cut -f 1 data/alignment.bed.sorted.bed | sort| uniq -c | grep -v CHR | grep -v GL | grep -v KI`
    dalignbedqueriesp=cfg['dalignbedqueriesp']
    logging.info(basename(dalignbedqueriesp))
    if not exists(dalignbedqueriesp) or cfg['force']:
        dalignbed=pd.merge(dalignbed,dqueries,on='query id',suffixes=('', '.1'))
        dalignbed.to_csv(dalignbedqueriesp,'\t')
    return cfg",python,"def dalignbed2dalignbedqueries(cfg):
    """"""
    Get query seqeunces from the BED file
    step#4

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    dalignbed=del_Unnamed(pd.read_csv(cfg['dalignbedp'],sep='\t'))
    dqueries=set_index(del_Unnamed(pd.read_csv(cfg['dqueriesp'],sep='\t')),'query id')
    
#     if the error in human, use: `cut -f 1 data/alignment.bed.sorted.bed | sort| uniq -c | grep -v CHR | grep -v GL | grep -v KI`
    dalignbedqueriesp=cfg['dalignbedqueriesp']
    logging.info(basename(dalignbedqueriesp))
    if not exists(dalignbedqueriesp) or cfg['force']:
        dalignbed=pd.merge(dalignbed,dqueries,on='query id',suffixes=('', '.1'))
        dalignbed.to_csv(dalignbedqueriesp,'\t')
    return cfg","['def' 'dalignbed2dalignbedqueries' '(' 'cfg' ')' ':' 'datatmpd' '=' 'cfg'
 '[' ""'datatmpd'"" ']' 'dalignbed' '=' 'del_Unnamed' '(' 'pd' '.'
 'read_csv' '(' 'cfg' '[' ""'dalignbedp'"" ']' ',' 'sep' '=' ""'\\t'"" ')' ')'
 'dqueries' '=' 'set_index' '(' 'del_Unnamed' '(' 'pd' '.' 'read_csv' '('
 'cfg' '[' ""'dqueriesp'"" ']' ',' 'sep' '=' ""'\\t'"" ')' ')' ','
 ""'query id'"" ')'
 '#     if the error in human, use: `cut -f 1 data/alignment.bed.sorted.bed | sort| uniq -c | grep -v CHR | grep -v GL | grep -v KI`'
 'dalignbedqueriesp' '=' 'cfg' '[' ""'dalignbedqueriesp'"" ']' 'logging' '.'
 'info' '(' 'basename' '(' 'dalignbedqueriesp' ')' ')' 'if' 'not' 'exists'
 '(' 'dalignbedqueriesp' ')' 'or' 'cfg' '[' ""'force'"" ']' ':' 'dalignbed'
 '=' 'pd' '.' 'merge' '(' 'dalignbed' ',' 'dqueries' ',' 'on' '='
 ""'query id'"" ',' 'suffixes' '=' '(' ""''"" ',' ""'.1'"" ')' ')' 'dalignbed'
 '.' 'to_csv' '(' 'dalignbedqueriesp' ',' ""'\\t'"" ')' 'return' 'cfg']","Get query seqeunces from the BED file
    step#4

    :param cfg: configuration dict",['Get' 'query' 'seqeunces' 'from' 'the' 'BED' 'file' 'step#4'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L185-L202
rraadd88/rohan,rohan/dandage/align/align_annot.py,alignmentbed2dalignedfasta,"def alignmentbed2dalignedfasta(cfg):
    """"""
    Get sequences in FASTA format from BED file
    step#5

    :param cfg: configuration dict
    """"""    
    datatmpd=cfg['datatmpd']
    alignmentbedp=cfg['alignmentbedp']    
    dalignedfastap=cfg['dalignedfastap']
    logging.info(basename(dalignedfastap))
    if not exists(dalignedfastap) or cfg['force']:
        alignedfastap='{}/05_alignment.fa'.format(datatmpd)
        if not exists(alignedfastap) or cfg['force']:
            cmd=f""{cfg['bedtools']} getfasta -s -name -fi {cfg['genomep']} -bed {alignmentbedp} -fo {alignedfastap}""
            runbashcmd(cmd)

        dalignedfasta=fa2df(alignedfastap)
        dalignedfasta.columns=['aligned sequence']
        dalignedfasta=dalignedfasta.loc[(dalignedfasta.apply(lambda x: not 'N' in x['aligned sequence'],axis=1)),:] #FIXME bwa aligns to NNNNNs
        dalignedfasta.index=[i.split('(')[0] for i in dalignedfasta.index] # for bedtools 2.27, the fasta header now has hanging (+) or (-)
        dalignedfasta.index.name='id'
        dalignedfasta.to_csv(dalignedfastap,sep='\t')
    return cfg",python,"def alignmentbed2dalignedfasta(cfg):
    """"""
    Get sequences in FASTA format from BED file
    step#5

    :param cfg: configuration dict
    """"""    
    datatmpd=cfg['datatmpd']
    alignmentbedp=cfg['alignmentbedp']    
    dalignedfastap=cfg['dalignedfastap']
    logging.info(basename(dalignedfastap))
    if not exists(dalignedfastap) or cfg['force']:
        alignedfastap='{}/05_alignment.fa'.format(datatmpd)
        if not exists(alignedfastap) or cfg['force']:
            cmd=f""{cfg['bedtools']} getfasta -s -name -fi {cfg['genomep']} -bed {alignmentbedp} -fo {alignedfastap}""
            runbashcmd(cmd)

        dalignedfasta=fa2df(alignedfastap)
        dalignedfasta.columns=['aligned sequence']
        dalignedfasta=dalignedfasta.loc[(dalignedfasta.apply(lambda x: not 'N' in x['aligned sequence'],axis=1)),:] #FIXME bwa aligns to NNNNNs
        dalignedfasta.index=[i.split('(')[0] for i in dalignedfasta.index] # for bedtools 2.27, the fasta header now has hanging (+) or (-)
        dalignedfasta.index.name='id'
        dalignedfasta.to_csv(dalignedfastap,sep='\t')
    return cfg","['def' 'alignmentbed2dalignedfasta' '(' 'cfg' ')' ':' 'datatmpd' '=' 'cfg'
 '[' ""'datatmpd'"" ']' 'alignmentbedp' '=' 'cfg' '[' ""'alignmentbedp'"" ']'
 'dalignedfastap' '=' 'cfg' '[' ""'dalignedfastap'"" ']' 'logging' '.'
 'info' '(' 'basename' '(' 'dalignedfastap' ')' ')' 'if' 'not' 'exists'
 '(' 'dalignedfastap' ')' 'or' 'cfg' '[' ""'force'"" ']' ':' 'alignedfastap'
 '=' ""'{}/05_alignment.fa'"" '.' 'format' '(' 'datatmpd' ')' 'if' 'not'
 'exists' '(' 'alignedfastap' ')' 'or' 'cfg' '[' ""'force'"" ']' ':' 'cmd'
 '='
 'f""{cfg[\'bedtools\']} getfasta -s -name -fi {cfg[\'genomep\']} -bed {alignmentbedp} -fo {alignedfastap}""'
 'runbashcmd' '(' 'cmd' ')' 'dalignedfasta' '=' 'fa2df' '('
 'alignedfastap' ')' 'dalignedfasta' '.' 'columns' '=' '['
 ""'aligned sequence'"" ']' 'dalignedfasta' '=' 'dalignedfasta' '.' 'loc'
 '[' '(' 'dalignedfasta' '.' 'apply' '(' 'lambda' 'x' ':' 'not' ""'N'"" 'in'
 'x' '[' ""'aligned sequence'"" ']' ',' 'axis' '=' '1' ')' ')' ',' ':' ']'
 '#FIXME bwa aligns to NNNNNs' 'dalignedfasta' '.' 'index' '=' '[' 'i' '.'
 'split' '(' ""'('"" ')' '[' '0' ']' 'for' 'i' 'in' 'dalignedfasta' '.'
 'index' ']'
 '# for bedtools 2.27, the fasta header now has hanging (+) or (-)'
 'dalignedfasta' '.' 'index' '.' 'name' '=' ""'id'"" 'dalignedfasta' '.'
 'to_csv' '(' 'dalignedfastap' ',' 'sep' '=' ""'\\t'"" ')' 'return' 'cfg']","Get sequences in FASTA format from BED file
    step#5

    :param cfg: configuration dict",['Get' 'sequences' 'in' 'FASTA' 'format' 'from' 'BED' 'file' 'step#5'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L204-L227
rraadd88/rohan,rohan/dandage/align/align_annot.py,dalignbed2dalignbedqueriesseq,"def dalignbed2dalignbedqueriesseq(cfg):
    """"""
    Get sequences from BED file
    step#6

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    dalignbedqueries=del_Unnamed(pd.read_csv(cfg['dalignbedqueriesp'],sep='\t'))
    dalignedfasta=del_Unnamed(pd.read_csv(cfg['dalignedfastap'],sep='\t'))
    dalignbedqueriesseqp=cfg['dalignbedqueriesseqp']
    logging.info(basename(dalignbedqueriesseqp))
    if not exists(dalignbedqueriesseqp) or cfg['force']:        
        dalignbedqueriesseq=pd.merge(dalignbedqueries,dalignedfasta,on='id',suffixes=('', '.2'))
        dalignbedqueriesseq=dalignbedqueriesseq.dropna(subset=['aligned sequence'],axis=0)

        # dalignbed.index.name='id'
        dalignbedqueriesseq=dalignbedqueriesseq.drop_duplicates()
        dalignbedqueriesseq.to_csv(dalignbedqueriesseqp,sep='\t')
    return cfg",python,"def dalignbed2dalignbedqueriesseq(cfg):
    """"""
    Get sequences from BED file
    step#6

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    dalignbedqueries=del_Unnamed(pd.read_csv(cfg['dalignbedqueriesp'],sep='\t'))
    dalignedfasta=del_Unnamed(pd.read_csv(cfg['dalignedfastap'],sep='\t'))
    dalignbedqueriesseqp=cfg['dalignbedqueriesseqp']
    logging.info(basename(dalignbedqueriesseqp))
    if not exists(dalignbedqueriesseqp) or cfg['force']:        
        dalignbedqueriesseq=pd.merge(dalignbedqueries,dalignedfasta,on='id',suffixes=('', '.2'))
        dalignbedqueriesseq=dalignbedqueriesseq.dropna(subset=['aligned sequence'],axis=0)

        # dalignbed.index.name='id'
        dalignbedqueriesseq=dalignbedqueriesseq.drop_duplicates()
        dalignbedqueriesseq.to_csv(dalignbedqueriesseqp,sep='\t')
    return cfg","['def' 'dalignbed2dalignbedqueriesseq' '(' 'cfg' ')' ':' 'datatmpd' '='
 'cfg' '[' ""'datatmpd'"" ']' 'dalignbedqueries' '=' 'del_Unnamed' '(' 'pd'
 '.' 'read_csv' '(' 'cfg' '[' ""'dalignbedqueriesp'"" ']' ',' 'sep' '='
 ""'\\t'"" ')' ')' 'dalignedfasta' '=' 'del_Unnamed' '(' 'pd' '.' 'read_csv'
 '(' 'cfg' '[' ""'dalignedfastap'"" ']' ',' 'sep' '=' ""'\\t'"" ')' ')'
 'dalignbedqueriesseqp' '=' 'cfg' '[' ""'dalignbedqueriesseqp'"" ']'
 'logging' '.' 'info' '(' 'basename' '(' 'dalignbedqueriesseqp' ')' ')'
 'if' 'not' 'exists' '(' 'dalignbedqueriesseqp' ')' 'or' 'cfg' '['
 ""'force'"" ']' ':' 'dalignbedqueriesseq' '=' 'pd' '.' 'merge' '('
 'dalignbedqueries' ',' 'dalignedfasta' ',' 'on' '=' ""'id'"" ',' 'suffixes'
 '=' '(' ""''"" ',' ""'.2'"" ')' ')' 'dalignbedqueriesseq' '='
 'dalignbedqueriesseq' '.' 'dropna' '(' 'subset' '=' '['
 ""'aligned sequence'"" ']' ',' 'axis' '=' '0' ')'
 ""# dalignbed.index.name='id'"" 'dalignbedqueriesseq' '='
 'dalignbedqueriesseq' '.' 'drop_duplicates' '(' ')' 'dalignbedqueriesseq'
 '.' 'to_csv' '(' 'dalignbedqueriesseqp' ',' 'sep' '=' ""'\\t'"" ')'
 'return' 'cfg']","Get sequences from BED file
    step#6

    :param cfg: configuration dict",['Get' 'sequences' 'from' 'BED' 'file' 'step#6'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L229-L248
rraadd88/rohan,rohan/dandage/align/align_annot.py,dalignbedqueriesseq2dalignbedstats,"def dalignbedqueriesseq2dalignbedstats(cfg):
    """"""
    Gets scores for queries
    step#7

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    dalignbedqueriesseq=del_Unnamed(pd.read_csv(cfg['dalignbedqueriesseqp'],sep='\t'))
    
    dalignbedstatsp=cfg['dalignbedstatsp']  
    logging.info(basename(dalignbedstatsp))
    if not exists(dalignbedstatsp) or cfg['force']:
        df=dalignbedqueriesseq.apply(lambda x: align(x['query sequence'],x['aligned sequence'],
                                                    psm=2,pmm=0.5,pgo=-3,pge=-1,),
                           axis=1).apply(pd.Series)
        print(df.head())
        df.columns=['alignment','alignment: score']
        dalignbedstats=dalignbedqueriesseq.join(df)
        del df
        dalignbedstats.to_csv(dalignbedstatsp,sep='\t')
    return cfg",python,"def dalignbedqueriesseq2dalignbedstats(cfg):
    """"""
    Gets scores for queries
    step#7

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    dalignbedqueriesseq=del_Unnamed(pd.read_csv(cfg['dalignbedqueriesseqp'],sep='\t'))
    
    dalignbedstatsp=cfg['dalignbedstatsp']  
    logging.info(basename(dalignbedstatsp))
    if not exists(dalignbedstatsp) or cfg['force']:
        df=dalignbedqueriesseq.apply(lambda x: align(x['query sequence'],x['aligned sequence'],
                                                    psm=2,pmm=0.5,pgo=-3,pge=-1,),
                           axis=1).apply(pd.Series)
        print(df.head())
        df.columns=['alignment','alignment: score']
        dalignbedstats=dalignbedqueriesseq.join(df)
        del df
        dalignbedstats.to_csv(dalignbedstatsp,sep='\t')
    return cfg","['def' 'dalignbedqueriesseq2dalignbedstats' '(' 'cfg' ')' ':' 'datatmpd'
 '=' 'cfg' '[' ""'datatmpd'"" ']' 'dalignbedqueriesseq' '=' 'del_Unnamed'
 '(' 'pd' '.' 'read_csv' '(' 'cfg' '[' ""'dalignbedqueriesseqp'"" ']' ','
 'sep' '=' ""'\\t'"" ')' ')' 'dalignbedstatsp' '=' 'cfg' '['
 ""'dalignbedstatsp'"" ']' 'logging' '.' 'info' '(' 'basename' '('
 'dalignbedstatsp' ')' ')' 'if' 'not' 'exists' '(' 'dalignbedstatsp' ')'
 'or' 'cfg' '[' ""'force'"" ']' ':' 'df' '=' 'dalignbedqueriesseq' '.'
 'apply' '(' 'lambda' 'x' ':' 'align' '(' 'x' '[' ""'query sequence'"" ']'
 ',' 'x' '[' ""'aligned sequence'"" ']' ',' 'psm' '=' '2' ',' 'pmm' '='
 '0.5' ',' 'pgo' '=' '-' '3' ',' 'pge' '=' '-' '1' ',' ')' ',' 'axis' '='
 '1' ')' '.' 'apply' '(' 'pd' '.' 'Series' ')' 'print' '(' 'df' '.' 'head'
 '(' ')' ')' 'df' '.' 'columns' '=' '[' ""'alignment'"" ','
 ""'alignment: score'"" ']' 'dalignbedstats' '=' 'dalignbedqueriesseq' '.'
 'join' '(' 'df' ')' 'del' 'df' 'dalignbedstats' '.' 'to_csv' '('
 'dalignbedstatsp' ',' 'sep' '=' ""'\\t'"" ')' 'return' 'cfg']","Gets scores for queries
    step#7

    :param cfg: configuration dict",['Gets' 'scores' 'for' 'queries' 'step#7'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L250-L271
rraadd88/rohan,rohan/dandage/align/align_annot.py,dannots2dalignbed2dannotsagg,"def dannots2dalignbed2dannotsagg(cfg):
    """"""
    Aggregate annotations per query
    step#8

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    
    daannotp=f'{datatmpd}/08_dannot.tsv'
    cfg['daannotp']=daannotp
    dannotsaggp=cfg['dannotsaggp']
    logging.info(basename(daannotp))
    if ((not exists(daannotp)) and (not exists(dannotsaggp))) or cfg['force']:
        gff_renamed_cols=[c+' annotation' if c in set(bed_colns).intersection(gff_colns) else c for c in gff_colns]
        dannots=pd.read_csv(cfg['annotationsbedp'],sep='\t',
                   names=bed_colns+gff_renamed_cols,
                           low_memory=False)
        dannots=del_Unnamed(dannots)

        dannots=dannots.set_index('id')
        dannots['annotations count']=1
        # separate ids from attribute columns
        dannots=lambda2cols(dannots,lambdaf=gffatributes2ids,
                    in_coln='attributes',
                to_colns=['gene name','gene id','transcript id','protein id','exon id'])
        dannots=dannots.drop(['attributes']+[c for c in gff_renamed_cols if 'annotation' in c],axis=1)
        logging.debug('or this step takes more time?')
        to_table(dannots,daannotp)
#         to_table_pqt(dannots,daannotp)
    else:
#         dannots=read_table_pqt(daannotp)
        dannots=read_table(daannotp)
        dannots=del_Unnamed(dannots)
        
    logging.info(basename(dannotsaggp))
    if not exists(dannotsaggp) or cfg['force']:
        if not 'dannots' in locals():
#             dannots=read_table_pqt(daannotp)
            dannots=pd.read_table(daannotp,low_memory=False)
        dannots=del_Unnamed(dannots)
        dannots=dannots.reset_index()
        logging.debug('aggregating the annotations')
        from rohan.dandage.io_sets import unique 
        cols2aggf={'annotations count':np.sum,
                  'type': unique,
                  'gene name': unique,
                  'gene id': unique,
                  'transcript id': unique,
                  'protein id': unique,
                  'exon id': unique}
        dannotsagg=dannots.groupby('id').agg(cols2aggf)
        dannotsagg['annotations count']=dannotsagg['annotations count']-1
        dannotsagg.loc[dannotsagg['annotations count']==0,'region']='intergenic'
        dannotsagg.loc[dannotsagg['annotations count']!=0,'region']='genic'
        logging.debug('end of the slowest step')            
        del dannots    
        dannotsagg=dannotsagg.reset_index()
#         to_table_pqt(dannotsagg,dannotsaggp)
        dannotsagg.to_csv(dannotsaggp,sep='\t')        
    return cfg",python,"def dannots2dalignbed2dannotsagg(cfg):
    """"""
    Aggregate annotations per query
    step#8

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    
    daannotp=f'{datatmpd}/08_dannot.tsv'
    cfg['daannotp']=daannotp
    dannotsaggp=cfg['dannotsaggp']
    logging.info(basename(daannotp))
    if ((not exists(daannotp)) and (not exists(dannotsaggp))) or cfg['force']:
        gff_renamed_cols=[c+' annotation' if c in set(bed_colns).intersection(gff_colns) else c for c in gff_colns]
        dannots=pd.read_csv(cfg['annotationsbedp'],sep='\t',
                   names=bed_colns+gff_renamed_cols,
                           low_memory=False)
        dannots=del_Unnamed(dannots)

        dannots=dannots.set_index('id')
        dannots['annotations count']=1
        # separate ids from attribute columns
        dannots=lambda2cols(dannots,lambdaf=gffatributes2ids,
                    in_coln='attributes',
                to_colns=['gene name','gene id','transcript id','protein id','exon id'])
        dannots=dannots.drop(['attributes']+[c for c in gff_renamed_cols if 'annotation' in c],axis=1)
        logging.debug('or this step takes more time?')
        to_table(dannots,daannotp)
#         to_table_pqt(dannots,daannotp)
    else:
#         dannots=read_table_pqt(daannotp)
        dannots=read_table(daannotp)
        dannots=del_Unnamed(dannots)
        
    logging.info(basename(dannotsaggp))
    if not exists(dannotsaggp) or cfg['force']:
        if not 'dannots' in locals():
#             dannots=read_table_pqt(daannotp)
            dannots=pd.read_table(daannotp,low_memory=False)
        dannots=del_Unnamed(dannots)
        dannots=dannots.reset_index()
        logging.debug('aggregating the annotations')
        from rohan.dandage.io_sets import unique 
        cols2aggf={'annotations count':np.sum,
                  'type': unique,
                  'gene name': unique,
                  'gene id': unique,
                  'transcript id': unique,
                  'protein id': unique,
                  'exon id': unique}
        dannotsagg=dannots.groupby('id').agg(cols2aggf)
        dannotsagg['annotations count']=dannotsagg['annotations count']-1
        dannotsagg.loc[dannotsagg['annotations count']==0,'region']='intergenic'
        dannotsagg.loc[dannotsagg['annotations count']!=0,'region']='genic'
        logging.debug('end of the slowest step')            
        del dannots    
        dannotsagg=dannotsagg.reset_index()
#         to_table_pqt(dannotsagg,dannotsaggp)
        dannotsagg.to_csv(dannotsaggp,sep='\t')        
    return cfg","['def' 'dannots2dalignbed2dannotsagg' '(' 'cfg' ')' ':' 'datatmpd' '='
 'cfg' '[' ""'datatmpd'"" ']' 'daannotp' '=' ""f'{datatmpd}/08_dannot.tsv'""
 'cfg' '[' ""'daannotp'"" ']' '=' 'daannotp' 'dannotsaggp' '=' 'cfg' '['
 ""'dannotsaggp'"" ']' 'logging' '.' 'info' '(' 'basename' '(' 'daannotp'
 ')' ')' 'if' '(' '(' 'not' 'exists' '(' 'daannotp' ')' ')' 'and' '('
 'not' 'exists' '(' 'dannotsaggp' ')' ')' ')' 'or' 'cfg' '[' ""'force'"" ']'
 ':' 'gff_renamed_cols' '=' '[' 'c' '+' ""' annotation'"" 'if' 'c' 'in'
 'set' '(' 'bed_colns' ')' '.' 'intersection' '(' 'gff_colns' ')' 'else'
 'c' 'for' 'c' 'in' 'gff_colns' ']' 'dannots' '=' 'pd' '.' 'read_csv' '('
 'cfg' '[' ""'annotationsbedp'"" ']' ',' 'sep' '=' ""'\\t'"" ',' 'names' '='
 'bed_colns' '+' 'gff_renamed_cols' ',' 'low_memory' '=' 'False' ')'
 'dannots' '=' 'del_Unnamed' '(' 'dannots' ')' 'dannots' '=' 'dannots' '.'
 'set_index' '(' ""'id'"" ')' 'dannots' '[' ""'annotations count'"" ']' '='
 '1' '# separate ids from attribute columns' 'dannots' '=' 'lambda2cols'
 '(' 'dannots' ',' 'lambdaf' '=' 'gffatributes2ids' ',' 'in_coln' '='
 ""'attributes'"" ',' 'to_colns' '=' '[' ""'gene name'"" ',' ""'gene id'"" ','
 ""'transcript id'"" ',' ""'protein id'"" ',' ""'exon id'"" ']' ')' 'dannots'
 '=' 'dannots' '.' 'drop' '(' '[' ""'attributes'"" ']' '+' '[' 'c' 'for' 'c'
 'in' 'gff_renamed_cols' 'if' ""'annotation'"" 'in' 'c' ']' ',' 'axis' '='
 '1' ')' 'logging' '.' 'debug' '(' ""'or this step takes more time?'"" ')'
 'to_table' '(' 'dannots' ',' 'daannotp' ')'
 '#         to_table_pqt(dannots,daannotp)' 'else' ':'
 '#         dannots=read_table_pqt(daannotp)' 'dannots' '=' 'read_table'
 '(' 'daannotp' ')' 'dannots' '=' 'del_Unnamed' '(' 'dannots' ')'
 'logging' '.' 'info' '(' 'basename' '(' 'dannotsaggp' ')' ')' 'if' 'not'
 'exists' '(' 'dannotsaggp' ')' 'or' 'cfg' '[' ""'force'"" ']' ':' 'if'
 'not' ""'dannots'"" 'in' 'locals' '(' ')' ':'
 '#             dannots=read_table_pqt(daannotp)' 'dannots' '=' 'pd' '.'
 'read_table' '(' 'daannotp' ',' 'low_memory' '=' 'False' ')' 'dannots'
 '=' 'del_Unnamed' '(' 'dannots' ')' 'dannots' '=' 'dannots' '.'
 'reset_index' '(' ')' 'logging' '.' 'debug' '('
 ""'aggregating the annotations'"" ')' 'from' 'rohan' '.' 'dandage' '.'
 'io_sets' 'import' 'unique' 'cols2aggf' '=' '{' ""'annotations count'"" ':'
 'np' '.' 'sum' ',' ""'type'"" ':' 'unique' ',' ""'gene name'"" ':' 'unique'
 ',' ""'gene id'"" ':' 'unique' ',' ""'transcript id'"" ':' 'unique' ','
 ""'protein id'"" ':' 'unique' ',' ""'exon id'"" ':' 'unique' '}' 'dannotsagg'
 '=' 'dannots' '.' 'groupby' '(' ""'id'"" ')' '.' 'agg' '(' 'cols2aggf' ')'
 'dannotsagg' '[' ""'annotations count'"" ']' '=' 'dannotsagg' '['
 ""'annotations count'"" ']' '-' '1' 'dannotsagg' '.' 'loc' '[' 'dannotsagg'
 '[' ""'annotations count'"" ']' '==' '0' ',' ""'region'"" ']' '='
 ""'intergenic'"" 'dannotsagg' '.' 'loc' '[' 'dannotsagg' '['
 ""'annotations count'"" ']' '!=' '0' ',' ""'region'"" ']' '=' ""'genic'""
 'logging' '.' 'debug' '(' ""'end of the slowest step'"" ')' 'del' 'dannots'
 'dannotsagg' '=' 'dannotsagg' '.' 'reset_index' '(' ')'
 '#         to_table_pqt(dannotsagg,dannotsaggp)' 'dannotsagg' '.'
 'to_csv' '(' 'dannotsaggp' ',' 'sep' '=' ""'\\t'"" ')' 'return' 'cfg']","Aggregate annotations per query
    step#8

    :param cfg: configuration dict",['Aggregate' 'annotations' 'per' 'query' 'step#8'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L272-L332
rraadd88/rohan,rohan/dandage/align/align_annot.py,dannotsagg2dannots2dalignbedannot,"def dannotsagg2dannots2dalignbedannot(cfg):
    """"""
    Map aggregated annotations to queries
    step#9

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    
    dannotsagg=del_Unnamed(pd.read_csv(cfg['dannotsaggp'],sep='\t'))
    dalignbedstats=del_Unnamed(pd.read_csv(cfg['dalignbedstatsp'],sep='\t'))
    dalignbedannotp=cfg['dalignbedannotp']
    logging.info(basename(dalignbedannotp))
    if not exists(dalignbedannotp) or cfg['force']:
        # df2info(dalignbed)
        # df2info(dannotsagg)
        dalignbedannot=dalignbedstats.set_index('id').join(set_index(dannotsagg,'id'),
                                              rsuffix=' annotation')
        dalignbedannot['NM']=dalignbedannot['NM'].apply(int)
#         from rohan.dandage.get_scores import get_beditorscore_per_alignment,get_cfdscore
#         dalignbedannot['beditor score']=dalignbedannot.apply(lambda x : get_beditorscore_per_alignment(NM=x['NM'],
#                                genic=True if x['region']=='genic' else False,
#                                alignment=x['alignment'],
#                                pam_length=len(x['PAM']),
#                                pam_position=x['original position'],
#                                # test=cfg['test'],
#                                 ),axis=1) 
#         dalignbedannot['CFD score']=dalignbedannot.apply(lambda x : get_cfdscore(x['query sequence'].upper(), x['aligned sequence'].upper()), axis=1)            
        dalignbedannot.to_csv(dalignbedannotp,sep='\t')
    return cfg",python,"def dannotsagg2dannots2dalignbedannot(cfg):
    """"""
    Map aggregated annotations to queries
    step#9

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']
    
    dannotsagg=del_Unnamed(pd.read_csv(cfg['dannotsaggp'],sep='\t'))
    dalignbedstats=del_Unnamed(pd.read_csv(cfg['dalignbedstatsp'],sep='\t'))
    dalignbedannotp=cfg['dalignbedannotp']
    logging.info(basename(dalignbedannotp))
    if not exists(dalignbedannotp) or cfg['force']:
        # df2info(dalignbed)
        # df2info(dannotsagg)
        dalignbedannot=dalignbedstats.set_index('id').join(set_index(dannotsagg,'id'),
                                              rsuffix=' annotation')
        dalignbedannot['NM']=dalignbedannot['NM'].apply(int)
#         from rohan.dandage.get_scores import get_beditorscore_per_alignment,get_cfdscore
#         dalignbedannot['beditor score']=dalignbedannot.apply(lambda x : get_beditorscore_per_alignment(NM=x['NM'],
#                                genic=True if x['region']=='genic' else False,
#                                alignment=x['alignment'],
#                                pam_length=len(x['PAM']),
#                                pam_position=x['original position'],
#                                # test=cfg['test'],
#                                 ),axis=1) 
#         dalignbedannot['CFD score']=dalignbedannot.apply(lambda x : get_cfdscore(x['query sequence'].upper(), x['aligned sequence'].upper()), axis=1)            
        dalignbedannot.to_csv(dalignbedannotp,sep='\t')
    return cfg","['def' 'dannotsagg2dannots2dalignbedannot' '(' 'cfg' ')' ':' 'datatmpd'
 '=' 'cfg' '[' ""'datatmpd'"" ']' 'dannotsagg' '=' 'del_Unnamed' '(' 'pd'
 '.' 'read_csv' '(' 'cfg' '[' ""'dannotsaggp'"" ']' ',' 'sep' '=' ""'\\t'""
 ')' ')' 'dalignbedstats' '=' 'del_Unnamed' '(' 'pd' '.' 'read_csv' '('
 'cfg' '[' ""'dalignbedstatsp'"" ']' ',' 'sep' '=' ""'\\t'"" ')' ')'
 'dalignbedannotp' '=' 'cfg' '[' ""'dalignbedannotp'"" ']' 'logging' '.'
 'info' '(' 'basename' '(' 'dalignbedannotp' ')' ')' 'if' 'not' 'exists'
 '(' 'dalignbedannotp' ')' 'or' 'cfg' '[' ""'force'"" ']' ':'
 '# df2info(dalignbed)' '# df2info(dannotsagg)' 'dalignbedannot' '='
 'dalignbedstats' '.' 'set_index' '(' ""'id'"" ')' '.' 'join' '('
 'set_index' '(' 'dannotsagg' ',' ""'id'"" ')' ',' 'rsuffix' '='
 ""' annotation'"" ')' 'dalignbedannot' '[' ""'NM'"" ']' '=' 'dalignbedannot'
 '[' ""'NM'"" ']' '.' 'apply' '(' 'int' ')'
 '#         from rohan.dandage.get_scores import get_beditorscore_per_alignment,get_cfdscore'
 ""#         dalignbedannot['beditor score']=dalignbedannot.apply(lambda x : get_beditorscore_per_alignment(NM=x['NM'],""
 ""#                                genic=True if x['region']=='genic' else False,""
 ""#                                alignment=x['alignment'],""
 ""#                                pam_length=len(x['PAM']),""
 ""#                                pam_position=x['original position'],""
 ""#                                # test=cfg['test'],""
 '#                                 ),axis=1) '
 ""#         dalignbedannot['CFD score']=dalignbedannot.apply(lambda x : get_cfdscore(x['query sequence'].upper(), x['aligned sequence'].upper()), axis=1)            ""
 'dalignbedannot' '.' 'to_csv' '(' 'dalignbedannotp' ',' 'sep' '=' ""'\\t'""
 ')' 'return' 'cfg']","Map aggregated annotations to queries
    step#9

    :param cfg: configuration dict",['Map' 'aggregated' 'annotations' 'to' 'queries' 'step#9'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L334-L363
rraadd88/rohan,rohan/dandage/align/align_annot.py,dalignbedannot2daggbyquery,"def dalignbedannot2daggbyquery(cfg):
    """"""
    Aggregate annotations per alignment to annotations per query.
    step#10

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']

    dalignbedannot=del_Unnamed(pd.read_csv(cfg['dalignbedannotp'],sep='\t',low_memory=False))
    
    daggbyqueryp=f'{datatmpd}/10_daggbyquery.tsv'      
    logging.info(basename(daggbyqueryp))
    if not exists(daggbyqueryp) or cfg['force']:
        dalignbedannot=dfliststr2dflist(dalignbedannot,
                                ['type', 'gene name', 'gene id', 'transcript id', 'protein id', 'exon id'],
                                colfmt='tuple')    
        dalignbedannot['alternate alignments count']=1
        import itertools
        from rohan.dandage.io_sets import unique
        def unique_dropna(l): return unique(l,drop='nan')
        def merge_unique_dropna(l): return unique(list(itertools.chain(*l)),drop='nan')
        cols2aggf={'id':unique_dropna,
         'type':merge_unique_dropna,
         'gene name':merge_unique_dropna,
         'gene id':merge_unique_dropna,
         'transcript id':merge_unique_dropna,
         'protein id':merge_unique_dropna,
         'exon id':merge_unique_dropna,
         'region':unique_dropna,
         'alternate alignments count':sum,
        }
        daggbyquery=dalignbedannot.groupby('query id').agg(cols2aggf)
        daggbyquery.to_csv(daggbyqueryp,sep='\t')
        daggbyquery.to_csv(cfg['dalignannotedp'],sep='\t')
    return cfg",python,"def dalignbedannot2daggbyquery(cfg):
    """"""
    Aggregate annotations per alignment to annotations per query.
    step#10

    :param cfg: configuration dict
    """"""
    datatmpd=cfg['datatmpd']

    dalignbedannot=del_Unnamed(pd.read_csv(cfg['dalignbedannotp'],sep='\t',low_memory=False))
    
    daggbyqueryp=f'{datatmpd}/10_daggbyquery.tsv'      
    logging.info(basename(daggbyqueryp))
    if not exists(daggbyqueryp) or cfg['force']:
        dalignbedannot=dfliststr2dflist(dalignbedannot,
                                ['type', 'gene name', 'gene id', 'transcript id', 'protein id', 'exon id'],
                                colfmt='tuple')    
        dalignbedannot['alternate alignments count']=1
        import itertools
        from rohan.dandage.io_sets import unique
        def unique_dropna(l): return unique(l,drop='nan')
        def merge_unique_dropna(l): return unique(list(itertools.chain(*l)),drop='nan')
        cols2aggf={'id':unique_dropna,
         'type':merge_unique_dropna,
         'gene name':merge_unique_dropna,
         'gene id':merge_unique_dropna,
         'transcript id':merge_unique_dropna,
         'protein id':merge_unique_dropna,
         'exon id':merge_unique_dropna,
         'region':unique_dropna,
         'alternate alignments count':sum,
        }
        daggbyquery=dalignbedannot.groupby('query id').agg(cols2aggf)
        daggbyquery.to_csv(daggbyqueryp,sep='\t')
        daggbyquery.to_csv(cfg['dalignannotedp'],sep='\t')
    return cfg","['def' 'dalignbedannot2daggbyquery' '(' 'cfg' ')' ':' 'datatmpd' '=' 'cfg'
 '[' ""'datatmpd'"" ']' 'dalignbedannot' '=' 'del_Unnamed' '(' 'pd' '.'
 'read_csv' '(' 'cfg' '[' ""'dalignbedannotp'"" ']' ',' 'sep' '=' ""'\\t'""
 ',' 'low_memory' '=' 'False' ')' ')' 'daggbyqueryp' '='
 ""f'{datatmpd}/10_daggbyquery.tsv'"" 'logging' '.' 'info' '(' 'basename'
 '(' 'daggbyqueryp' ')' ')' 'if' 'not' 'exists' '(' 'daggbyqueryp' ')'
 'or' 'cfg' '[' ""'force'"" ']' ':' 'dalignbedannot' '=' 'dfliststr2dflist'
 '(' 'dalignbedannot' ',' '[' ""'type'"" ',' ""'gene name'"" ',' ""'gene id'""
 ',' ""'transcript id'"" ',' ""'protein id'"" ',' ""'exon id'"" ']' ',' 'colfmt'
 '=' ""'tuple'"" ')' 'dalignbedannot' '[' ""'alternate alignments count'"" ']'
 '=' '1' 'import' 'itertools' 'from' 'rohan' '.' 'dandage' '.' 'io_sets'
 'import' 'unique' 'def' 'unique_dropna' '(' 'l' ')' ':' 'return' 'unique'
 '(' 'l' ',' 'drop' '=' ""'nan'"" ')' 'def' 'merge_unique_dropna' '(' 'l'
 ')' ':' 'return' 'unique' '(' 'list' '(' 'itertools' '.' 'chain' '(' '*'
 'l' ')' ')' ',' 'drop' '=' ""'nan'"" ')' 'cols2aggf' '=' '{' ""'id'"" ':'
 'unique_dropna' ',' ""'type'"" ':' 'merge_unique_dropna' ',' ""'gene name'""
 ':' 'merge_unique_dropna' ',' ""'gene id'"" ':' 'merge_unique_dropna' ','
 ""'transcript id'"" ':' 'merge_unique_dropna' ',' ""'protein id'"" ':'
 'merge_unique_dropna' ',' ""'exon id'"" ':' 'merge_unique_dropna' ','
 ""'region'"" ':' 'unique_dropna' ',' ""'alternate alignments count'"" ':'
 'sum' ',' '}' 'daggbyquery' '=' 'dalignbedannot' '.' 'groupby' '('
 ""'query id'"" ')' '.' 'agg' '(' 'cols2aggf' ')' 'daggbyquery' '.' 'to_csv'
 '(' 'daggbyqueryp' ',' 'sep' '=' ""'\\t'"" ')' 'daggbyquery' '.' 'to_csv'
 '(' 'cfg' '[' ""'dalignannotedp'"" ']' ',' 'sep' '=' ""'\\t'"" ')' 'return'
 'cfg']","Aggregate annotations per alignment to annotations per query.
    step#10

    :param cfg: configuration dict","['Aggregate' 'annotations' 'per' 'alignment' 'to' 'annotations' 'per'
 'query' '.' 'step#10']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L365-L400
rraadd88/rohan,rohan/dandage/align/align_annot.py,queries2alignments,"def queries2alignments(cfg):
    """"""
    All the processes in alignannoted detection are here.
    
    :param cfg: Configuration settings provided in .yml file
    """"""
    from rohan.dandage.align import get_genomes
    get_genomes(cfg)
    
    cfg['datad']=cfg['prjd']
    cfg['plotd']=cfg['datad']
    dalignannotedp=f""{cfg['datad']}/dalignannoted.tsv""  
    
#     stepn='04_alignannoteds'
#     logging.info(stepn)
    
    cfg['datatmpd']=f""{cfg['datad']}/tmp""
    for dp in [cfg['datatmpd']]:
        if not exists(dp):
            makedirs(dp)
            
    step2doutp={
    1:'01_queries_queryl*.fa',
    2:'02_dalignbed.tsv',
    3:'03_annotations.bed',
    4:'04_dalignbedqueries.tsv',
    5:'05_dalignedfasta.tsv',
    6:'06_dalignbedqueriesseq.tsv',
    7:'07_dalignbedstats.tsv',
    8:'08_dannotsagg.tsv',
    9:'09_dalignbedannot.tsv',
    10:'10_daggbyquery.tsv',
    }
    cfg['dqueriesp']=cfg['dinp']
    cfg['alignmentbedp']=f""{cfg['datatmpd']}/02_alignment.bed""
    cfg['dalignbedp']=f""{cfg['datatmpd']}/02_dalignbed.tsv""
    cfg['dalignbedqueriesp']=f""{cfg['datatmpd']}/04_dalignbedqueries.tsv""
    cfg['dalignedfastap']=f""{cfg['datatmpd']}/05_dalignedfasta.tsv""
    cfg['dalignbedqueriesseqp']=f""{cfg['datatmpd']}/06_dalignbedqueriesseq.tsv""
    cfg['dalignbedstatsp']=f""{cfg['datatmpd']}/07_dalignbedstats.tsv""
    cfg['dannotsaggp']=f""{cfg['datatmpd']}/08_dannotsagg.tsv""
    cfg['dalignbedannotp']=f""{cfg['datatmpd']}/09_dalignbedannot.tsv""
    cfg['daggbyqueryp']=f""{cfg['datatmpd']}/10_daggbyquery.tsv""

    annotationsbedp=f""{cfg['datatmpd']}/03_annotations.bed""
    cfg['annotationsbedp']=annotationsbedp
    
    dqueries=read_table(cfg['dqueriesp'])
    print(dqueries.head())
    #check which step to process
    for step in range(2,10+1,1):
        if not exists(f""{cfg['datatmpd']}/{step2doutp[step]}""):
            if step==2:
                step=-1
            break
    logging.info(f'process from step:{step}')
    cfg['dalignannotedp']='{}/dalignannoted.tsv'.format(cfg['datad'])
    if not exists(cfg['dalignannotedp']) or cfg['force']:
        if step<=1:
            cfg=dqueries2queriessam(cfg,dqueries)
        if step<=2:
            cfg=queriessam2dalignbed(cfg)
        if step<=3:
            cfg=dalignbed2annotationsbed(cfg)
        if step<=4:
            cfg=dalignbed2dalignbedqueries(cfg)
        if step<=5:
            cfg=alignmentbed2dalignedfasta(cfg)
        if step<=6:
            cfg=dalignbed2dalignbedqueriesseq(cfg)
        if step<=7:
            cfg=dalignbedqueriesseq2dalignbedstats(cfg)
        if step<=8:
            cfg=dannots2dalignbed2dannotsagg(cfg)
        if step<=9:
            cfg=dannotsagg2dannots2dalignbedannot(cfg)
        if step<=10:
            cfg=dalignbedannot2daggbyquery(cfg)

        if cfg is None:        
            logging.warning(f""no alignment found"")
            cfg['step']=4
            return saveemptytable(cfg,cfg['dalignannotedp'])
        import gc
        gc.collect()",python,"def queries2alignments(cfg):
    """"""
    All the processes in alignannoted detection are here.
    
    :param cfg: Configuration settings provided in .yml file
    """"""
    from rohan.dandage.align import get_genomes
    get_genomes(cfg)
    
    cfg['datad']=cfg['prjd']
    cfg['plotd']=cfg['datad']
    dalignannotedp=f""{cfg['datad']}/dalignannoted.tsv""  
    
#     stepn='04_alignannoteds'
#     logging.info(stepn)
    
    cfg['datatmpd']=f""{cfg['datad']}/tmp""
    for dp in [cfg['datatmpd']]:
        if not exists(dp):
            makedirs(dp)
            
    step2doutp={
    1:'01_queries_queryl*.fa',
    2:'02_dalignbed.tsv',
    3:'03_annotations.bed',
    4:'04_dalignbedqueries.tsv',
    5:'05_dalignedfasta.tsv',
    6:'06_dalignbedqueriesseq.tsv',
    7:'07_dalignbedstats.tsv',
    8:'08_dannotsagg.tsv',
    9:'09_dalignbedannot.tsv',
    10:'10_daggbyquery.tsv',
    }
    cfg['dqueriesp']=cfg['dinp']
    cfg['alignmentbedp']=f""{cfg['datatmpd']}/02_alignment.bed""
    cfg['dalignbedp']=f""{cfg['datatmpd']}/02_dalignbed.tsv""
    cfg['dalignbedqueriesp']=f""{cfg['datatmpd']}/04_dalignbedqueries.tsv""
    cfg['dalignedfastap']=f""{cfg['datatmpd']}/05_dalignedfasta.tsv""
    cfg['dalignbedqueriesseqp']=f""{cfg['datatmpd']}/06_dalignbedqueriesseq.tsv""
    cfg['dalignbedstatsp']=f""{cfg['datatmpd']}/07_dalignbedstats.tsv""
    cfg['dannotsaggp']=f""{cfg['datatmpd']}/08_dannotsagg.tsv""
    cfg['dalignbedannotp']=f""{cfg['datatmpd']}/09_dalignbedannot.tsv""
    cfg['daggbyqueryp']=f""{cfg['datatmpd']}/10_daggbyquery.tsv""

    annotationsbedp=f""{cfg['datatmpd']}/03_annotations.bed""
    cfg['annotationsbedp']=annotationsbedp
    
    dqueries=read_table(cfg['dqueriesp'])
    print(dqueries.head())
    #check which step to process
    for step in range(2,10+1,1):
        if not exists(f""{cfg['datatmpd']}/{step2doutp[step]}""):
            if step==2:
                step=-1
            break
    logging.info(f'process from step:{step}')
    cfg['dalignannotedp']='{}/dalignannoted.tsv'.format(cfg['datad'])
    if not exists(cfg['dalignannotedp']) or cfg['force']:
        if step<=1:
            cfg=dqueries2queriessam(cfg,dqueries)
        if step<=2:
            cfg=queriessam2dalignbed(cfg)
        if step<=3:
            cfg=dalignbed2annotationsbed(cfg)
        if step<=4:
            cfg=dalignbed2dalignbedqueries(cfg)
        if step<=5:
            cfg=alignmentbed2dalignedfasta(cfg)
        if step<=6:
            cfg=dalignbed2dalignbedqueriesseq(cfg)
        if step<=7:
            cfg=dalignbedqueriesseq2dalignbedstats(cfg)
        if step<=8:
            cfg=dannots2dalignbed2dannotsagg(cfg)
        if step<=9:
            cfg=dannotsagg2dannots2dalignbedannot(cfg)
        if step<=10:
            cfg=dalignbedannot2daggbyquery(cfg)

        if cfg is None:        
            logging.warning(f""no alignment found"")
            cfg['step']=4
            return saveemptytable(cfg,cfg['dalignannotedp'])
        import gc
        gc.collect()","['def' 'queries2alignments' '(' 'cfg' ')' ':' 'from' 'rohan' '.' 'dandage'
 '.' 'align' 'import' 'get_genomes' 'get_genomes' '(' 'cfg' ')' 'cfg' '['
 ""'datad'"" ']' '=' 'cfg' '[' ""'prjd'"" ']' 'cfg' '[' ""'plotd'"" ']' '='
 'cfg' '[' ""'datad'"" ']' 'dalignannotedp' '='
 'f""{cfg[\'datad\']}/dalignannoted.tsv""' ""#     stepn='04_alignannoteds'""
 '#     logging.info(stepn)' 'cfg' '[' ""'datatmpd'"" ']' '='
 'f""{cfg[\'datad\']}/tmp""' 'for' 'dp' 'in' '[' 'cfg' '[' ""'datatmpd'"" ']'
 ']' ':' 'if' 'not' 'exists' '(' 'dp' ')' ':' 'makedirs' '(' 'dp' ')'
 'step2doutp' '=' '{' '1' ':' ""'01_queries_queryl*.fa'"" ',' '2' ':'
 ""'02_dalignbed.tsv'"" ',' '3' ':' ""'03_annotations.bed'"" ',' '4' ':'
 ""'04_dalignbedqueries.tsv'"" ',' '5' ':' ""'05_dalignedfasta.tsv'"" ',' '6'
 ':' ""'06_dalignbedqueriesseq.tsv'"" ',' '7' ':' ""'07_dalignbedstats.tsv'""
 ',' '8' ':' ""'08_dannotsagg.tsv'"" ',' '9' ':' ""'09_dalignbedannot.tsv'""
 ',' '10' ':' ""'10_daggbyquery.tsv'"" ',' '}' 'cfg' '[' ""'dqueriesp'"" ']'
 '=' 'cfg' '[' ""'dinp'"" ']' 'cfg' '[' ""'alignmentbedp'"" ']' '='
 'f""{cfg[\'datatmpd\']}/02_alignment.bed""' 'cfg' '[' ""'dalignbedp'"" ']'
 '=' 'f""{cfg[\'datatmpd\']}/02_dalignbed.tsv""' 'cfg' '['
 ""'dalignbedqueriesp'"" ']' '='
 'f""{cfg[\'datatmpd\']}/04_dalignbedqueries.tsv""' 'cfg' '['
 ""'dalignedfastap'"" ']' '=' 'f""{cfg[\'datatmpd\']}/05_dalignedfasta.tsv""'
 'cfg' '[' ""'dalignbedqueriesseqp'"" ']' '='
 'f""{cfg[\'datatmpd\']}/06_dalignbedqueriesseq.tsv""' 'cfg' '['
 ""'dalignbedstatsp'"" ']' '='
 'f""{cfg[\'datatmpd\']}/07_dalignbedstats.tsv""' 'cfg' '[' ""'dannotsaggp'""
 ']' '=' 'f""{cfg[\'datatmpd\']}/08_dannotsagg.tsv""' 'cfg' '['
 ""'dalignbedannotp'"" ']' '='
 'f""{cfg[\'datatmpd\']}/09_dalignbedannot.tsv""' 'cfg' '[' ""'daggbyqueryp'""
 ']' '=' 'f""{cfg[\'datatmpd\']}/10_daggbyquery.tsv""' 'annotationsbedp' '='
 'f""{cfg[\'datatmpd\']}/03_annotations.bed""' 'cfg' '[' ""'annotationsbedp'""
 ']' '=' 'annotationsbedp' 'dqueries' '=' 'read_table' '(' 'cfg' '['
 ""'dqueriesp'"" ']' ')' 'print' '(' 'dqueries' '.' 'head' '(' ')' ')'
 '#check which step to process' 'for' 'step' 'in' 'range' '(' '2' ',' '10'
 '+' '1' ',' '1' ')' ':' 'if' 'not' 'exists' '('
 'f""{cfg[\'datatmpd\']}/{step2doutp[step]}""' ')' ':' 'if' 'step' '==' '2'
 ':' 'step' '=' '-' '1' 'break' 'logging' '.' 'info' '('
 ""f'process from step:{step}'"" ')' 'cfg' '[' ""'dalignannotedp'"" ']' '='
 ""'{}/dalignannoted.tsv'"" '.' 'format' '(' 'cfg' '[' ""'datad'"" ']' ')'
 'if' 'not' 'exists' '(' 'cfg' '[' ""'dalignannotedp'"" ']' ')' 'or' 'cfg'
 '[' ""'force'"" ']' ':' 'if' 'step' '<=' '1' ':' 'cfg' '='
 'dqueries2queriessam' '(' 'cfg' ',' 'dqueries' ')' 'if' 'step' '<=' '2'
 ':' 'cfg' '=' 'queriessam2dalignbed' '(' 'cfg' ')' 'if' 'step' '<=' '3'
 ':' 'cfg' '=' 'dalignbed2annotationsbed' '(' 'cfg' ')' 'if' 'step' '<='
 '4' ':' 'cfg' '=' 'dalignbed2dalignbedqueries' '(' 'cfg' ')' 'if' 'step'
 '<=' '5' ':' 'cfg' '=' 'alignmentbed2dalignedfasta' '(' 'cfg' ')' 'if'
 'step' '<=' '6' ':' 'cfg' '=' 'dalignbed2dalignbedqueriesseq' '(' 'cfg'
 ')' 'if' 'step' '<=' '7' ':' 'cfg' '='
 'dalignbedqueriesseq2dalignbedstats' '(' 'cfg' ')' 'if' 'step' '<=' '8'
 ':' 'cfg' '=' 'dannots2dalignbed2dannotsagg' '(' 'cfg' ')' 'if' 'step'
 '<=' '9' ':' 'cfg' '=' 'dannotsagg2dannots2dalignbedannot' '(' 'cfg' ')'
 'if' 'step' '<=' '10' ':' 'cfg' '=' 'dalignbedannot2daggbyquery' '('
 'cfg' ')' 'if' 'cfg' 'is' 'None' ':' 'logging' '.' 'warning' '('
 'f""no alignment found""' ')' 'cfg' '[' ""'step'"" ']' '=' '4' 'return'
 'saveemptytable' '(' 'cfg' ',' 'cfg' '[' ""'dalignannotedp'"" ']' ')'
 'import' 'gc' 'gc' '.' 'collect' '(' ')']","All the processes in alignannoted detection are here.
    
    :param cfg: Configuration settings provided in .yml file","['All' 'the' 'processes' 'in' 'alignannoted' 'detection' 'are' 'here' '.'
 ':' 'param' 'cfg' ':' 'Configuration' 'settings' 'provided' 'in' '.'
 'yml' 'file']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L402-L486
rraadd88/rohan,rohan/dandage/plot/shape.py,df2plotshape,"def df2plotshape(dlen,xlabel_unit,ylabel_unit,
                 suptitle='',fix='h',xlabel_skip=[],
                 test=False):
    """"""
    _xlen: 
    _ylen:
    title:
    """"""
    dlen['xlabel']=dlen.apply(lambda x : f""{x['_xlen']}"" if not x['title'] in xlabel_skip else '',axis=1)
    dlen['ylabel']=dlen.apply(lambda x : """",axis=1)
    ylen=dlen['_ylen'].unique()[0]
    
    if test: print(dlen.columns)
    if fix=='h':    
        dlen['xlen']=dlen['_xlen']/dlen['_xlen'].max()/len(dlen)*0.8
        dlen['ylen']=0.8
        subsets=[]
        for subset in [c for c in dlen if c.startswith('_ylen ')]:
            subsetcol=subset.replace('_ylen','ylen')
            dlen[subsetcol]=0.25
            subsets.append(subsetcol)
        subsets2cols=dict(zip([subsetcol.replace('ylen ','') for s in subsets],subsets))
        if test: print(dlen.columns)
        if test: print(subsets2cols)
    elif fix=='w':    
        dlen['xlen']=0.8
        dlen['ylen']=dlen['_ylen']/dlen['_ylen'].max()/len(dlen)*0.85
   
    dlen=dlen.drop([c for c in dlen if c.startswith('_')],axis=1)    
    if test: print(dlen.columns)
    if fig is None:fig = plt.figure(figsize=[4,4])
    for idx in dlen.index:
        if idx==0:x_=0
        kws_plot_rect=makekws_plot_rect(dlen,fig,idx,x_)
        if test: print(kws_plot_rect)
        kws_plot_rect_big={k:kws_plot_rect[k] for k in kws_plot_rect if not 'ylen ' in k}
        kws_plot_rect_big['color']='gray'
        ax=plot_rect(**kws_plot_rect_big)
        for subset in subsets2cols:
            kws_plot_rect=makekws_plot_rect(dlen.drop('ylen',axis=1).rename(columns={subsets2cols[subset]:'ylen'}),fig,idx,x_)
            kws_plot_rect['title']=''
            kws_plot_rect['xlabel']=''
            kws_plot_rect['ylabel']=subset
            if idx!=0:kws_plot_rect['ylabel']=''
            if test: print(kws_plot_rect)
            ax=plot_rect(**kws_plot_rect)            
        x_=kws_plot_rect['x']+dlen.loc[idx,'xlen']+0.1  
    
    ax.text(x_/2.3,-0.1,xlabel_unit,ha='center')
    ax.text(x_/2.3,0.9,suptitle,ha='center')
    ax.text(-0.1,0.4,f""total ~{ylen}{ylabel_unit}"",va='center',rotation=90)
    if fig is not None:
        return fig,ax
    else:
        return ax",python,"def df2plotshape(dlen,xlabel_unit,ylabel_unit,
                 suptitle='',fix='h',xlabel_skip=[],
                 test=False):
    """"""
    _xlen: 
    _ylen:
    title:
    """"""
    dlen['xlabel']=dlen.apply(lambda x : f""{x['_xlen']}"" if not x['title'] in xlabel_skip else '',axis=1)
    dlen['ylabel']=dlen.apply(lambda x : """",axis=1)
    ylen=dlen['_ylen'].unique()[0]
    
    if test: print(dlen.columns)
    if fix=='h':    
        dlen['xlen']=dlen['_xlen']/dlen['_xlen'].max()/len(dlen)*0.8
        dlen['ylen']=0.8
        subsets=[]
        for subset in [c for c in dlen if c.startswith('_ylen ')]:
            subsetcol=subset.replace('_ylen','ylen')
            dlen[subsetcol]=0.25
            subsets.append(subsetcol)
        subsets2cols=dict(zip([subsetcol.replace('ylen ','') for s in subsets],subsets))
        if test: print(dlen.columns)
        if test: print(subsets2cols)
    elif fix=='w':    
        dlen['xlen']=0.8
        dlen['ylen']=dlen['_ylen']/dlen['_ylen'].max()/len(dlen)*0.85
   
    dlen=dlen.drop([c for c in dlen if c.startswith('_')],axis=1)    
    if test: print(dlen.columns)
    if fig is None:fig = plt.figure(figsize=[4,4])
    for idx in dlen.index:
        if idx==0:x_=0
        kws_plot_rect=makekws_plot_rect(dlen,fig,idx,x_)
        if test: print(kws_plot_rect)
        kws_plot_rect_big={k:kws_plot_rect[k] for k in kws_plot_rect if not 'ylen ' in k}
        kws_plot_rect_big['color']='gray'
        ax=plot_rect(**kws_plot_rect_big)
        for subset in subsets2cols:
            kws_plot_rect=makekws_plot_rect(dlen.drop('ylen',axis=1).rename(columns={subsets2cols[subset]:'ylen'}),fig,idx,x_)
            kws_plot_rect['title']=''
            kws_plot_rect['xlabel']=''
            kws_plot_rect['ylabel']=subset
            if idx!=0:kws_plot_rect['ylabel']=''
            if test: print(kws_plot_rect)
            ax=plot_rect(**kws_plot_rect)            
        x_=kws_plot_rect['x']+dlen.loc[idx,'xlen']+0.1  
    
    ax.text(x_/2.3,-0.1,xlabel_unit,ha='center')
    ax.text(x_/2.3,0.9,suptitle,ha='center')
    ax.text(-0.1,0.4,f""total ~{ylen}{ylabel_unit}"",va='center',rotation=90)
    if fig is not None:
        return fig,ax
    else:
        return ax","['def' 'df2plotshape' '(' 'dlen' ',' 'xlabel_unit' ',' 'ylabel_unit' ','
 'suptitle' '=' ""''"" ',' 'fix' '=' ""'h'"" ',' 'xlabel_skip' '=' '[' ']' ','
 'test' '=' 'False' ')' ':' 'dlen' '[' ""'xlabel'"" ']' '=' 'dlen' '.'
 'apply' '(' 'lambda' 'x' ':' 'f""{x[\'_xlen\']}""' 'if' 'not' 'x' '['
 ""'title'"" ']' 'in' 'xlabel_skip' 'else' ""''"" ',' 'axis' '=' '1' ')'
 'dlen' '[' ""'ylabel'"" ']' '=' 'dlen' '.' 'apply' '(' 'lambda' 'x' ':'
 '""""' ',' 'axis' '=' '1' ')' 'ylen' '=' 'dlen' '[' ""'_ylen'"" ']' '.'
 'unique' '(' ')' '[' '0' ']' 'if' 'test' ':' 'print' '(' 'dlen' '.'
 'columns' ')' 'if' 'fix' '==' ""'h'"" ':' 'dlen' '[' ""'xlen'"" ']' '='
 'dlen' '[' ""'_xlen'"" ']' '/' 'dlen' '[' ""'_xlen'"" ']' '.' 'max' '(' ')'
 '/' 'len' '(' 'dlen' ')' '*' '0.8' 'dlen' '[' ""'ylen'"" ']' '=' '0.8'
 'subsets' '=' '[' ']' 'for' 'subset' 'in' '[' 'c' 'for' 'c' 'in' 'dlen'
 'if' 'c' '.' 'startswith' '(' ""'_ylen '"" ')' ']' ':' 'subsetcol' '='
 'subset' '.' 'replace' '(' ""'_ylen'"" ',' ""'ylen'"" ')' 'dlen' '['
 'subsetcol' ']' '=' '0.25' 'subsets' '.' 'append' '(' 'subsetcol' ')'
 'subsets2cols' '=' 'dict' '(' 'zip' '(' '[' 'subsetcol' '.' 'replace' '('
 ""'ylen '"" ',' ""''"" ')' 'for' 's' 'in' 'subsets' ']' ',' 'subsets' ')' ')'
 'if' 'test' ':' 'print' '(' 'dlen' '.' 'columns' ')' 'if' 'test' ':'
 'print' '(' 'subsets2cols' ')' 'elif' 'fix' '==' ""'w'"" ':' 'dlen' '['
 ""'xlen'"" ']' '=' '0.8' 'dlen' '[' ""'ylen'"" ']' '=' 'dlen' '[' ""'_ylen'""
 ']' '/' 'dlen' '[' ""'_ylen'"" ']' '.' 'max' '(' ')' '/' 'len' '(' 'dlen'
 ')' '*' '0.85' 'dlen' '=' 'dlen' '.' 'drop' '(' '[' 'c' 'for' 'c' 'in'
 'dlen' 'if' 'c' '.' 'startswith' '(' ""'_'"" ')' ']' ',' 'axis' '=' '1' ')'
 'if' 'test' ':' 'print' '(' 'dlen' '.' 'columns' ')' 'if' 'fig' 'is'
 'None' ':' 'fig' '=' 'plt' '.' 'figure' '(' 'figsize' '=' '[' '4' ',' '4'
 ']' ')' 'for' 'idx' 'in' 'dlen' '.' 'index' ':' 'if' 'idx' '==' '0' ':'
 'x_' '=' '0' 'kws_plot_rect' '=' 'makekws_plot_rect' '(' 'dlen' ',' 'fig'
 ',' 'idx' ',' 'x_' ')' 'if' 'test' ':' 'print' '(' 'kws_plot_rect' ')'
 'kws_plot_rect_big' '=' '{' 'k' ':' 'kws_plot_rect' '[' 'k' ']' 'for' 'k'
 'in' 'kws_plot_rect' 'if' 'not' ""'ylen '"" 'in' 'k' '}'
 'kws_plot_rect_big' '[' ""'color'"" ']' '=' ""'gray'"" 'ax' '=' 'plot_rect'
 '(' '*' '*' 'kws_plot_rect_big' ')' 'for' 'subset' 'in' 'subsets2cols'
 ':' 'kws_plot_rect' '=' 'makekws_plot_rect' '(' 'dlen' '.' 'drop' '('
 ""'ylen'"" ',' 'axis' '=' '1' ')' '.' 'rename' '(' 'columns' '=' '{'
 'subsets2cols' '[' 'subset' ']' ':' ""'ylen'"" '}' ')' ',' 'fig' ',' 'idx'
 ',' 'x_' ')' 'kws_plot_rect' '[' ""'title'"" ']' '=' ""''"" 'kws_plot_rect'
 '[' ""'xlabel'"" ']' '=' ""''"" 'kws_plot_rect' '[' ""'ylabel'"" ']' '='
 'subset' 'if' 'idx' '!=' '0' ':' 'kws_plot_rect' '[' ""'ylabel'"" ']' '='
 ""''"" 'if' 'test' ':' 'print' '(' 'kws_plot_rect' ')' 'ax' '=' 'plot_rect'
 '(' '*' '*' 'kws_plot_rect' ')' 'x_' '=' 'kws_plot_rect' '[' ""'x'"" ']'
 '+' 'dlen' '.' 'loc' '[' 'idx' ',' ""'xlen'"" ']' '+' '0.1' 'ax' '.' 'text'
 '(' 'x_' '/' '2.3' ',' '-' '0.1' ',' 'xlabel_unit' ',' 'ha' '='
 ""'center'"" ')' 'ax' '.' 'text' '(' 'x_' '/' '2.3' ',' '0.9' ','
 'suptitle' ',' 'ha' '=' ""'center'"" ')' 'ax' '.' 'text' '(' '-' '0.1' ','
 '0.4' ',' 'f""total ~{ylen}{ylabel_unit}""' ',' 'va' '=' ""'center'"" ','
 'rotation' '=' '90' ')' 'if' 'fig' 'is' 'not' 'None' ':' 'return' 'fig'
 ',' 'ax' 'else' ':' 'return' 'ax']","_xlen: 
    _ylen:
    title:",['_xlen' ':' '_ylen' ':' 'title' ':'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/plot/shape.py#L53-L107
rraadd88/rohan,rohan/dandage/plot/annot.py,annot_boxplot,"def annot_boxplot(ax,dmetrics,xoffwithin=0.85,xoff=1.6,
                  yoff=0,annotby='xs',
                  test=False):
    """"""
    :param dmetrics: hue in index, x in columns
    
    #todos
    #x|y off in %
    xmin,xmax=ax.get_xlim()
    (xmax-xmin)+(xmax-xmin)*0.35+xmin
    """"""
    xlabel=ax.get_xlabel()
    ylabel=ax.get_ylabel()
    if test:
        dmetrics.index.name='index'
        dmetrics.columns.name='columns'
        dm=dmetrics.melt()
        dm['value']=1
        ax=sns.boxplot(data=dm,x='columns',y='value')
    for huei,hue in enumerate(dmetrics.index):  
        for xi,x in enumerate(dmetrics.columns):
            if not pd.isnull(dmetrics.loc[hue,x]):
                xco=xi+(huei*xoffwithin/len(dmetrics.index)+(xoff/len(dmetrics.index)))
                yco=ax.get_ylim()[1]+yoff
                if annotby=='ys':
                    xco,yco=yco,xco
                ax.text(xco,yco,dmetrics.loc[hue,x],ha='center')
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    return ax",python,"def annot_boxplot(ax,dmetrics,xoffwithin=0.85,xoff=1.6,
                  yoff=0,annotby='xs',
                  test=False):
    """"""
    :param dmetrics: hue in index, x in columns
    
    #todos
    #x|y off in %
    xmin,xmax=ax.get_xlim()
    (xmax-xmin)+(xmax-xmin)*0.35+xmin
    """"""
    xlabel=ax.get_xlabel()
    ylabel=ax.get_ylabel()
    if test:
        dmetrics.index.name='index'
        dmetrics.columns.name='columns'
        dm=dmetrics.melt()
        dm['value']=1
        ax=sns.boxplot(data=dm,x='columns',y='value')
    for huei,hue in enumerate(dmetrics.index):  
        for xi,x in enumerate(dmetrics.columns):
            if not pd.isnull(dmetrics.loc[hue,x]):
                xco=xi+(huei*xoffwithin/len(dmetrics.index)+(xoff/len(dmetrics.index)))
                yco=ax.get_ylim()[1]+yoff
                if annotby=='ys':
                    xco,yco=yco,xco
                ax.text(xco,yco,dmetrics.loc[hue,x],ha='center')
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    return ax","['def' 'annot_boxplot' '(' 'ax' ',' 'dmetrics' ',' 'xoffwithin' '=' '0.85'
 ',' 'xoff' '=' '1.6' ',' 'yoff' '=' '0' ',' 'annotby' '=' ""'xs'"" ','
 'test' '=' 'False' ')' ':' 'xlabel' '=' 'ax' '.' 'get_xlabel' '(' ')'
 'ylabel' '=' 'ax' '.' 'get_ylabel' '(' ')' 'if' 'test' ':' 'dmetrics' '.'
 'index' '.' 'name' '=' ""'index'"" 'dmetrics' '.' 'columns' '.' 'name' '='
 ""'columns'"" 'dm' '=' 'dmetrics' '.' 'melt' '(' ')' 'dm' '[' ""'value'"" ']'
 '=' '1' 'ax' '=' 'sns' '.' 'boxplot' '(' 'data' '=' 'dm' ',' 'x' '='
 ""'columns'"" ',' 'y' '=' ""'value'"" ')' 'for' 'huei' ',' 'hue' 'in'
 'enumerate' '(' 'dmetrics' '.' 'index' ')' ':' 'for' 'xi' ',' 'x' 'in'
 'enumerate' '(' 'dmetrics' '.' 'columns' ')' ':' 'if' 'not' 'pd' '.'
 'isnull' '(' 'dmetrics' '.' 'loc' '[' 'hue' ',' 'x' ']' ')' ':' 'xco' '='
 'xi' '+' '(' 'huei' '*' 'xoffwithin' '/' 'len' '(' 'dmetrics' '.' 'index'
 ')' '+' '(' 'xoff' '/' 'len' '(' 'dmetrics' '.' 'index' ')' ')' ')' 'yco'
 '=' 'ax' '.' 'get_ylim' '(' ')' '[' '1' ']' '+' 'yoff' 'if' 'annotby'
 '==' ""'ys'"" ':' 'xco' ',' 'yco' '=' 'yco' ',' 'xco' 'ax' '.' 'text' '('
 'xco' ',' 'yco' ',' 'dmetrics' '.' 'loc' '[' 'hue' ',' 'x' ']' ',' 'ha'
 '=' ""'center'"" ')' 'ax' '.' 'set_xlabel' '(' 'xlabel' ')' 'ax' '.'
 'set_ylabel' '(' 'ylabel' ')' 'return' 'ax']",":param dmetrics: hue in index, x in columns
    
    #todos
    #x|y off in %
    xmin,xmax=ax.get_xlim()
    (xmax-xmin)+(xmax-xmin)*0.35+xmin","[':' 'param' 'dmetrics' ':' 'hue' 'in' 'index' 'x' 'in' 'columns' '#todos'
 '#x|y' 'off' 'in' '%' 'xmin' 'xmax' '=' 'ax' '.' 'get_xlim' '()' '('
 'xmax' '-' 'xmin' ')' '+' '(' 'xmax' '-' 'xmin' ')' '*' '0' '.' '35' '+'
 'xmin']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/plot/annot.py#L139-L168
rraadd88/rohan,rohan/dandage/plot/annot.py,annot_heatmap,"def annot_heatmap(ax,dannot,
                  xoff=0,yoff=0,
                  kws_text={},# zip
                  annot_left='(',annot_right=')',
                  annothalf='upper',
                ):
    """"""
    kws_text={'marker','s','linewidth','facecolors','edgecolors'}
    """"""
    for xtli,xtl in enumerate(ax.get_xticklabels()):
        xtl=xtl.get_text()
        for ytli,ytl in enumerate(ax.get_yticklabels()):
            ytl=ytl.get_text()
            if annothalf=='upper':
                ax.text(xtli+0.5+xoff,ytli+0.5+yoff,dannot.loc[xtl,ytl],**kws_text,ha='center')
            else:
                ax.text(ytli+0.5+yoff,xtli+0.5+xoff,dannot.loc[xtl,ytl],**kws_text,ha='center')                
    return ax",python,"def annot_heatmap(ax,dannot,
                  xoff=0,yoff=0,
                  kws_text={},# zip
                  annot_left='(',annot_right=')',
                  annothalf='upper',
                ):
    """"""
    kws_text={'marker','s','linewidth','facecolors','edgecolors'}
    """"""
    for xtli,xtl in enumerate(ax.get_xticklabels()):
        xtl=xtl.get_text()
        for ytli,ytl in enumerate(ax.get_yticklabels()):
            ytl=ytl.get_text()
            if annothalf=='upper':
                ax.text(xtli+0.5+xoff,ytli+0.5+yoff,dannot.loc[xtl,ytl],**kws_text,ha='center')
            else:
                ax.text(ytli+0.5+yoff,xtli+0.5+xoff,dannot.loc[xtl,ytl],**kws_text,ha='center')                
    return ax","['def' 'annot_heatmap' '(' 'ax' ',' 'dannot' ',' 'xoff' '=' '0' ',' 'yoff'
 '=' '0' ',' 'kws_text' '=' '{' '}' ',' '# zip' 'annot_left' '=' ""'('"" ','
 'annot_right' '=' ""')'"" ',' 'annothalf' '=' ""'upper'"" ',' ')' ':' 'for'
 'xtli' ',' 'xtl' 'in' 'enumerate' '(' 'ax' '.' 'get_xticklabels' '(' ')'
 ')' ':' 'xtl' '=' 'xtl' '.' 'get_text' '(' ')' 'for' 'ytli' ',' 'ytl'
 'in' 'enumerate' '(' 'ax' '.' 'get_yticklabels' '(' ')' ')' ':' 'ytl' '='
 'ytl' '.' 'get_text' '(' ')' 'if' 'annothalf' '==' ""'upper'"" ':' 'ax' '.'
 'text' '(' 'xtli' '+' '0.5' '+' 'xoff' ',' 'ytli' '+' '0.5' '+' 'yoff'
 ',' 'dannot' '.' 'loc' '[' 'xtl' ',' 'ytl' ']' ',' '*' '*' 'kws_text' ','
 'ha' '=' ""'center'"" ')' 'else' ':' 'ax' '.' 'text' '(' 'ytli' '+' '0.5'
 '+' 'yoff' ',' 'xtli' '+' '0.5' '+' 'xoff' ',' 'dannot' '.' 'loc' '['
 'xtl' ',' 'ytl' ']' ',' '*' '*' 'kws_text' ',' 'ha' '=' ""'center'"" ')'
 'return' 'ax']","kws_text={'marker','s','linewidth','facecolors','edgecolors'}",['kws_text' '=' '{' 'marker' 's' 'linewidth' 'facecolors' 'edgecolors' '}'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/plot/annot.py#L170-L187
rraadd88/rohan,rohan/dandage/plot/annot.py,pval2annot,"def pval2annot(pval,alternative=None,alpha=None,fmt='*',#swarm=False
               linebreak=True,
              ):
    """"""
    fmt: *|<|'num'
    """"""
    if alternative is None and alpha is None:
        ValueError('both alternative and alpha are None')
    if alpha is None:
        alpha=0.025 if alternative=='two-sided' else alternative if is_numeric(alternative) else 0.05
    if pd.isnull(pval):
        return ''
    elif pval < 0.0001:
        return ""****"" if fmt=='*' else f""P<\n{0.0001:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}""  if linebreak else f""P={pval:.1g}""
    elif (pval < 0.001):
        return ""***""  if fmt=='*' else f""P<\n{0.001:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""
    elif (pval < 0.01):
        return ""**"" if fmt=='*' else f""P<\n{0.01:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""
    elif (pval < alpha):
        return ""*"" if fmt=='*' else f""P<\n{alpha:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""
    else:
        return ""ns"" if fmt=='*' else f""P=\n{pval:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""",python,"def pval2annot(pval,alternative=None,alpha=None,fmt='*',#swarm=False
               linebreak=True,
              ):
    """"""
    fmt: *|<|'num'
    """"""
    if alternative is None and alpha is None:
        ValueError('both alternative and alpha are None')
    if alpha is None:
        alpha=0.025 if alternative=='two-sided' else alternative if is_numeric(alternative) else 0.05
    if pd.isnull(pval):
        return ''
    elif pval < 0.0001:
        return ""****"" if fmt=='*' else f""P<\n{0.0001:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}""  if linebreak else f""P={pval:.1g}""
    elif (pval < 0.001):
        return ""***""  if fmt=='*' else f""P<\n{0.001:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""
    elif (pval < 0.01):
        return ""**"" if fmt=='*' else f""P<\n{0.01:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""
    elif (pval < alpha):
        return ""*"" if fmt=='*' else f""P<\n{alpha:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""
    else:
        return ""ns"" if fmt=='*' else f""P=\n{pval:.0e}"" if fmt=='<' else f""P={pval:.1g}"" if len(f""P={pval:.1g}"")<6 else f""P=\n{pval:.1g}"" if linebreak else f""P={pval:.1g}""","['def' 'pval2annot' '(' 'pval' ',' 'alternative' '=' 'None' ',' 'alpha'
 '=' 'None' ',' 'fmt' '=' ""'*'"" ',' '#swarm=False' 'linebreak' '=' 'True'
 ',' ')' ':' 'if' 'alternative' 'is' 'None' 'and' 'alpha' 'is' 'None' ':'
 'ValueError' '(' ""'both alternative and alpha are None'"" ')' 'if' 'alpha'
 'is' 'None' ':' 'alpha' '=' '0.025' 'if' 'alternative' '==' ""'two-sided'""
 'else' 'alternative' 'if' 'is_numeric' '(' 'alternative' ')' 'else'
 '0.05' 'if' 'pd' '.' 'isnull' '(' 'pval' ')' ':' 'return' ""''"" 'elif'
 'pval' '<' '0.0001' ':' 'return' '""****""' 'if' 'fmt' '==' ""'*'"" 'else'
 'f""P<\\n{0.0001:.0e}""' 'if' 'fmt' '==' ""'<'"" 'else' 'f""P={pval:.1g}""'
 'if' 'len' '(' 'f""P={pval:.1g}""' ')' '<' '6' 'else' 'f""P=\\n{pval:.1g}""'
 'if' 'linebreak' 'else' 'f""P={pval:.1g}""' 'elif' '(' 'pval' '<' '0.001'
 ')' ':' 'return' '""***""' 'if' 'fmt' '==' ""'*'"" 'else'
 'f""P<\\n{0.001:.0e}""' 'if' 'fmt' '==' ""'<'"" 'else' 'f""P={pval:.1g}""' 'if'
 'len' '(' 'f""P={pval:.1g}""' ')' '<' '6' 'else' 'f""P=\\n{pval:.1g}""' 'if'
 'linebreak' 'else' 'f""P={pval:.1g}""' 'elif' '(' 'pval' '<' '0.01' ')' ':'
 'return' '""**""' 'if' 'fmt' '==' ""'*'"" 'else' 'f""P<\\n{0.01:.0e}""' 'if'
 'fmt' '==' ""'<'"" 'else' 'f""P={pval:.1g}""' 'if' 'len' '('
 'f""P={pval:.1g}""' ')' '<' '6' 'else' 'f""P=\\n{pval:.1g}""' 'if'
 'linebreak' 'else' 'f""P={pval:.1g}""' 'elif' '(' 'pval' '<' 'alpha' ')'
 ':' 'return' '""*""' 'if' 'fmt' '==' ""'*'"" 'else' 'f""P<\\n{alpha:.0e}""'
 'if' 'fmt' '==' ""'<'"" 'else' 'f""P={pval:.1g}""' 'if' 'len' '('
 'f""P={pval:.1g}""' ')' '<' '6' 'else' 'f""P=\\n{pval:.1g}""' 'if'
 'linebreak' 'else' 'f""P={pval:.1g}""' 'else' ':' 'return' '""ns""' 'if'
 'fmt' '==' ""'*'"" 'else' 'f""P=\\n{pval:.0e}""' 'if' 'fmt' '==' ""'<'"" 'else'
 'f""P={pval:.1g}""' 'if' 'len' '(' 'f""P={pval:.1g}""' ')' '<' '6' 'else'
 'f""P=\\n{pval:.1g}""' 'if' 'linebreak' 'else' 'f""P={pval:.1g}""']",fmt: *|<|'num',['fmt' ':' '*' '|<|' 'num'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/plot/annot.py#L190-L211
rraadd88/rohan,rohan/dandage/io_nums.py,is_numeric,"def is_numeric(obj):
    """"""
    This detects whether an input object is numeric or not.

    :param obj: object to be tested.
    """"""
    try:
        obj+obj, obj-obj, obj*obj, obj**obj, obj/obj
    except ZeroDivisionError:
        return True
    except Exception:
        return False
    else:
        return True",python,"def is_numeric(obj):
    """"""
    This detects whether an input object is numeric or not.

    :param obj: object to be tested.
    """"""
    try:
        obj+obj, obj-obj, obj*obj, obj**obj, obj/obj
    except ZeroDivisionError:
        return True
    except Exception:
        return False
    else:
        return True","['def' 'is_numeric' '(' 'obj' ')' ':' 'try' ':' 'obj' '+' 'obj' ',' 'obj'
 '-' 'obj' ',' 'obj' '*' 'obj' ',' 'obj' '**' 'obj' ',' 'obj' '/' 'obj'
 'except' 'ZeroDivisionError' ':' 'return' 'True' 'except' 'Exception' ':'
 'return' 'False' 'else' ':' 'return' 'True']","This detects whether an input object is numeric or not.

    :param obj: object to be tested.","['This' 'detects' 'whether' 'an' 'input' 'object' 'is' 'numeric' 'or'
 'not' '.']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_nums.py#L14-L27
rraadd88/rohan,rohan/dandage/io_nums.py,glog,"def glog(x,l = 2):
    """"""
    Generalised logarithm

    :param x: number
    :param p: number added befor logarithm 

    """"""
    return np.log((x+np.sqrt(x**2+l**2))/2)/np.log(l)",python,"def glog(x,l = 2):
    """"""
    Generalised logarithm

    :param x: number
    :param p: number added befor logarithm 

    """"""
    return np.log((x+np.sqrt(x**2+l**2))/2)/np.log(l)","['def' 'glog' '(' 'x' ',' 'l' '=' '2' ')' ':' 'return' 'np' '.' 'log' '('
 '(' 'x' '+' 'np' '.' 'sqrt' '(' 'x' '**' '2' '+' 'l' '**' '2' ')' ')' '/'
 '2' ')' '/' 'np' '.' 'log' '(' 'l' ')']","Generalised logarithm

    :param x: number
    :param p: number added befor logarithm",['Generalised' 'logarithm'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_nums.py#L59-L67
rraadd88/rohan,rohan/dandage/io_nums.py,float2int,"def float2int(x):
    """"""
    converts floats to int when only float() is not enough.

    :param x: float
    """"""
    if not pd.isnull(x):
        if is_numeric(x):
            x=int(x)
    return x",python,"def float2int(x):
    """"""
    converts floats to int when only float() is not enough.

    :param x: float
    """"""
    if not pd.isnull(x):
        if is_numeric(x):
            x=int(x)
    return x","['def' 'float2int' '(' 'x' ')' ':' 'if' 'not' 'pd' '.' 'isnull' '(' 'x'
 ')' ':' 'if' 'is_numeric' '(' 'x' ')' ':' 'x' '=' 'int' '(' 'x' ')'
 'return' 'x']","converts floats to int when only float() is not enough.

    :param x: float","['converts' 'floats' 'to' 'int' 'when' 'only' 'float' '()' 'is' 'not'
 'enough' '.']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_nums.py#L69-L78
rraadd88/rohan,rohan/dandage/db/uniprot.py,get_sequence,"def get_sequence(queries,fap=None,fmt='fasta',
            organism_taxid=9606,
                 test=False):
    """"""
    http://www.ebi.ac.uk/Tools/dbfetch/dbfetch?db=uniprotkb&id=P14060+P26439&format=fasta&style=raw&Retrieve=Retrieve
    https://www.uniprot.org/uniprot/?format=fasta&organism=9606&query=O75116+O75116+P35548+O14944+O14944
    """"""
    url = 'http://www.ebi.ac.uk/Tools/dbfetch/dbfetch'
    params = {
    'id':' '.join(queries),
    'db':'uniprotkb',
    'organism':organism_taxid,    
    'format':fmt,
    'style':'raw',
    'Retrieve':'Retrieve',
    }
    response = requests.get(url, params=params)
    if test:
        print(response.url)
    if response.ok:
        if not fap is None:
            with open(fap,'w') as f:
                f.write(response.text)
            return fap
        else:
            return response.text            
    else:
        print('Something went wrong ', response.status_code)",python,"def get_sequence(queries,fap=None,fmt='fasta',
            organism_taxid=9606,
                 test=False):
    """"""
    http://www.ebi.ac.uk/Tools/dbfetch/dbfetch?db=uniprotkb&id=P14060+P26439&format=fasta&style=raw&Retrieve=Retrieve
    https://www.uniprot.org/uniprot/?format=fasta&organism=9606&query=O75116+O75116+P35548+O14944+O14944
    """"""
    url = 'http://www.ebi.ac.uk/Tools/dbfetch/dbfetch'
    params = {
    'id':' '.join(queries),
    'db':'uniprotkb',
    'organism':organism_taxid,    
    'format':fmt,
    'style':'raw',
    'Retrieve':'Retrieve',
    }
    response = requests.get(url, params=params)
    if test:
        print(response.url)
    if response.ok:
        if not fap is None:
            with open(fap,'w') as f:
                f.write(response.text)
            return fap
        else:
            return response.text            
    else:
        print('Something went wrong ', response.status_code)","['def' 'get_sequence' '(' 'queries' ',' 'fap' '=' 'None' ',' 'fmt' '='
 ""'fasta'"" ',' 'organism_taxid' '=' '9606' ',' 'test' '=' 'False' ')' ':'
 'url' '=' ""'http://www.ebi.ac.uk/Tools/dbfetch/dbfetch'"" 'params' '=' '{'
 ""'id'"" ':' ""' '"" '.' 'join' '(' 'queries' ')' ',' ""'db'"" ':'
 ""'uniprotkb'"" ',' ""'organism'"" ':' 'organism_taxid' ',' ""'format'"" ':'
 'fmt' ',' ""'style'"" ':' ""'raw'"" ',' ""'Retrieve'"" ':' ""'Retrieve'"" ',' '}'
 'response' '=' 'requests' '.' 'get' '(' 'url' ',' 'params' '=' 'params'
 ')' 'if' 'test' ':' 'print' '(' 'response' '.' 'url' ')' 'if' 'response'
 '.' 'ok' ':' 'if' 'not' 'fap' 'is' 'None' ':' 'with' 'open' '(' 'fap' ','
 ""'w'"" ')' 'as' 'f' ':' 'f' '.' 'write' '(' 'response' '.' 'text' ')'
 'return' 'fap' 'else' ':' 'return' 'response' '.' 'text' 'else' ':'
 'print' '(' ""'Something went wrong '"" ',' 'response' '.' 'status_code'
 ')']","http://www.ebi.ac.uk/Tools/dbfetch/dbfetch?db=uniprotkb&id=P14060+P26439&format=fasta&style=raw&Retrieve=Retrieve
    https://www.uniprot.org/uniprot/?format=fasta&organism=9606&query=O75116+O75116+P35548+O14944+O14944","['http' ':' '//' 'www' '.' 'ebi' '.' 'ac' '.' 'uk' '/' 'Tools' '/'
 'dbfetch' '/' 'dbfetch?db' '=' 'uniprotkb&id' '=' 'P14060' '+'
 'P26439&format' '=' 'fasta&style' '=' 'raw&Retrieve' '=' 'Retrieve'
 'https' ':' '//' 'www' '.' 'uniprot' '.' 'org' '/' 'uniprot' '/'
 '?format' '=' 'fasta&organism' '=' '9606&query' '=' 'O75116' '+' 'O75116'
 '+' 'P35548' '+' 'O14944' '+' 'O14944']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/db/uniprot.py#L5-L32
rraadd88/rohan,rohan/dandage/db/uniprot.py,map_ids,"def map_ids(queries,frm='ACC',to='ENSEMBL_PRO_ID',
            organism_taxid=9606,test=False):
    """"""
    https://www.uniprot.org/help/api_idmapping
    """"""
    url = 'https://www.uniprot.org/uploadlists/'
    params = {
    'from':frm,
    'to':to,
    'format':'tab',
    'organism':organism_taxid,    
    'query':' '.join(queries),
    }
    response = requests.get(url, params=params)
    if test:
        print(response.url)
    if response.ok:
        df=pd.read_table(response.url)
        df.columns=[frm,to]
        return df
    else:
        print('Something went wrong ', response.status_code)",python,"def map_ids(queries,frm='ACC',to='ENSEMBL_PRO_ID',
            organism_taxid=9606,test=False):
    """"""
    https://www.uniprot.org/help/api_idmapping
    """"""
    url = 'https://www.uniprot.org/uploadlists/'
    params = {
    'from':frm,
    'to':to,
    'format':'tab',
    'organism':organism_taxid,    
    'query':' '.join(queries),
    }
    response = requests.get(url, params=params)
    if test:
        print(response.url)
    if response.ok:
        df=pd.read_table(response.url)
        df.columns=[frm,to]
        return df
    else:
        print('Something went wrong ', response.status_code)","['def' 'map_ids' '(' 'queries' ',' 'frm' '=' ""'ACC'"" ',' 'to' '='
 ""'ENSEMBL_PRO_ID'"" ',' 'organism_taxid' '=' '9606' ',' 'test' '=' 'False'
 ')' ':' 'url' '=' ""'https://www.uniprot.org/uploadlists/'"" 'params' '='
 '{' ""'from'"" ':' 'frm' ',' ""'to'"" ':' 'to' ',' ""'format'"" ':' ""'tab'"" ','
 ""'organism'"" ':' 'organism_taxid' ',' ""'query'"" ':' ""' '"" '.' 'join' '('
 'queries' ')' ',' '}' 'response' '=' 'requests' '.' 'get' '(' 'url' ','
 'params' '=' 'params' ')' 'if' 'test' ':' 'print' '(' 'response' '.'
 'url' ')' 'if' 'response' '.' 'ok' ':' 'df' '=' 'pd' '.' 'read_table' '('
 'response' '.' 'url' ')' 'df' '.' 'columns' '=' '[' 'frm' ',' 'to' ']'
 'return' 'df' 'else' ':' 'print' '(' ""'Something went wrong '"" ','
 'response' '.' 'status_code' ')']",https://www.uniprot.org/help/api_idmapping,"['https' ':' '//' 'www' '.' 'uniprot' '.' 'org' '/' 'help' '/'
 'api_idmapping']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/db/uniprot.py#L46-L67
rraadd88/rohan,rohan/dandage/io_sets.py,rankwithlist,"def rankwithlist(l,lwith,test=False):
    """"""
    rank l wrt lwith
    """"""
    if not (isinstance(l,list) and isinstance(lwith,list)):
        l,lwith=list(l),list(lwith)
    from scipy.stats import rankdata
    if test:
        print(l,lwith)
        print(rankdata(l),rankdata(lwith))
        print(rankdata(l+lwith))
    return rankdata(l+lwith)[:len(l)]",python,"def rankwithlist(l,lwith,test=False):
    """"""
    rank l wrt lwith
    """"""
    if not (isinstance(l,list) and isinstance(lwith,list)):
        l,lwith=list(l),list(lwith)
    from scipy.stats import rankdata
    if test:
        print(l,lwith)
        print(rankdata(l),rankdata(lwith))
        print(rankdata(l+lwith))
    return rankdata(l+lwith)[:len(l)]","['def' 'rankwithlist' '(' 'l' ',' 'lwith' ',' 'test' '=' 'False' ')' ':'
 'if' 'not' '(' 'isinstance' '(' 'l' ',' 'list' ')' 'and' 'isinstance' '('
 'lwith' ',' 'list' ')' ')' ':' 'l' ',' 'lwith' '=' 'list' '(' 'l' ')' ','
 'list' '(' 'lwith' ')' 'from' 'scipy' '.' 'stats' 'import' 'rankdata'
 'if' 'test' ':' 'print' '(' 'l' ',' 'lwith' ')' 'print' '(' 'rankdata'
 '(' 'l' ')' ',' 'rankdata' '(' 'lwith' ')' ')' 'print' '(' 'rankdata' '('
 'l' '+' 'lwith' ')' ')' 'return' 'rankdata' '(' 'l' '+' 'lwith' ')' '['
 ':' 'len' '(' 'l' ')' ']']",rank l wrt lwith,['rank' 'l' 'wrt' 'lwith'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_sets.py#L38-L49
rraadd88/rohan,rohan/dandage/io_sets.py,dfbool2intervals,"def dfbool2intervals(df,colbool):
    """"""
    ds contains bool values
    """"""
    df.index=range(len(df))
    intervals=bools2intervals(df[colbool])
    for intervali,interval in enumerate(intervals):
        df.loc[interval[0]:interval[1],f'{colbool} interval id']=intervali
        df.loc[interval[0]:interval[1],f'{colbool} interval start']=interval[0]
        df.loc[interval[0]:interval[1],f'{colbool} interval stop']=interval[1]
        df.loc[interval[0]:interval[1],f'{colbool} interval length']=interval[1]-interval[0]+1
        df.loc[interval[0]:interval[1],f'{colbool} interval within index']=range(interval[1]-interval[0]+1)    
    df[f'{colbool} interval index']=df.index    
    return df",python,"def dfbool2intervals(df,colbool):
    """"""
    ds contains bool values
    """"""
    df.index=range(len(df))
    intervals=bools2intervals(df[colbool])
    for intervali,interval in enumerate(intervals):
        df.loc[interval[0]:interval[1],f'{colbool} interval id']=intervali
        df.loc[interval[0]:interval[1],f'{colbool} interval start']=interval[0]
        df.loc[interval[0]:interval[1],f'{colbool} interval stop']=interval[1]
        df.loc[interval[0]:interval[1],f'{colbool} interval length']=interval[1]-interval[0]+1
        df.loc[interval[0]:interval[1],f'{colbool} interval within index']=range(interval[1]-interval[0]+1)    
    df[f'{colbool} interval index']=df.index    
    return df","['def' 'dfbool2intervals' '(' 'df' ',' 'colbool' ')' ':' 'df' '.' 'index'
 '=' 'range' '(' 'len' '(' 'df' ')' ')' 'intervals' '=' 'bools2intervals'
 '(' 'df' '[' 'colbool' ']' ')' 'for' 'intervali' ',' 'interval' 'in'
 'enumerate' '(' 'intervals' ')' ':' 'df' '.' 'loc' '[' 'interval' '[' '0'
 ']' ':' 'interval' '[' '1' ']' ',' ""f'{colbool} interval id'"" ']' '='
 'intervali' 'df' '.' 'loc' '[' 'interval' '[' '0' ']' ':' 'interval' '['
 '1' ']' ',' ""f'{colbool} interval start'"" ']' '=' 'interval' '[' '0' ']'
 'df' '.' 'loc' '[' 'interval' '[' '0' ']' ':' 'interval' '[' '1' ']' ','
 ""f'{colbool} interval stop'"" ']' '=' 'interval' '[' '1' ']' 'df' '.'
 'loc' '[' 'interval' '[' '0' ']' ':' 'interval' '[' '1' ']' ','
 ""f'{colbool} interval length'"" ']' '=' 'interval' '[' '1' ']' '-'
 'interval' '[' '0' ']' '+' '1' 'df' '.' 'loc' '[' 'interval' '[' '0' ']'
 ':' 'interval' '[' '1' ']' ',' ""f'{colbool} interval within index'"" ']'
 '=' 'range' '(' 'interval' '[' '1' ']' '-' 'interval' '[' '0' ']' '+' '1'
 ')' 'df' '[' ""f'{colbool} interval index'"" ']' '=' 'df' '.' 'index'
 'return' 'df']",ds contains bool values,['ds' 'contains' 'bool' 'values'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_sets.py#L54-L67
rraadd88/rohan,rohan/dandage/db/go.py,get_go_info,"def get_go_info(goterm,result='name'):
    """"""
    quickgo
    result: 'name','definition','synonyms'
    
    {'numberOfHits': 1,
 'results': [{'id': 'GO:0000006',
   'isObsolete': False,
   'name': 'high-affinity zinc transmembrane transporter activity',
   'definition': {'text': 'Enables the transfer of zinc ions (Zn2+) from one side of a membrane to the other, probably powered by proton motive force. In high-affinity transport the transporter is able to bind the solute even if it is only present at very low concentrations.',
    'xrefs': [{'dbCode': 'TC', 'dbId': '2.A.5.1.1'}]},
   'synonyms': [{'name': 'high-affinity zinc uptake transmembrane transporter activity',
     'type': 'related'},
    {'name': 'high affinity zinc uptake transmembrane transporter activity',
     'type': 'exact'}],
   'aspect': 'molecular_function',
   'usage': 'Unrestricted'}],
 'pageInfo': None}
    """"""
    response=requests.get(f'https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{goterm}')
    try:       
        return response.json()['results'][0][result]
    except:
        print(response.json())",python,"def get_go_info(goterm,result='name'):
    """"""
    quickgo
    result: 'name','definition','synonyms'
    
    {'numberOfHits': 1,
 'results': [{'id': 'GO:0000006',
   'isObsolete': False,
   'name': 'high-affinity zinc transmembrane transporter activity',
   'definition': {'text': 'Enables the transfer of zinc ions (Zn2+) from one side of a membrane to the other, probably powered by proton motive force. In high-affinity transport the transporter is able to bind the solute even if it is only present at very low concentrations.',
    'xrefs': [{'dbCode': 'TC', 'dbId': '2.A.5.1.1'}]},
   'synonyms': [{'name': 'high-affinity zinc uptake transmembrane transporter activity',
     'type': 'related'},
    {'name': 'high affinity zinc uptake transmembrane transporter activity',
     'type': 'exact'}],
   'aspect': 'molecular_function',
   'usage': 'Unrestricted'}],
 'pageInfo': None}
    """"""
    response=requests.get(f'https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{goterm}')
    try:       
        return response.json()['results'][0][result]
    except:
        print(response.json())","['def' 'get_go_info' '(' 'goterm' ',' 'result' '=' ""'name'"" ')' ':'
 'response' '=' 'requests' '.' 'get' '('
 ""f'https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{goterm}'""
 ')' 'try' ':' 'return' 'response' '.' 'json' '(' ')' '[' ""'results'"" ']'
 '[' '0' ']' '[' 'result' ']' 'except' ':' 'print' '(' 'response' '.'
 'json' '(' ')' ')']","quickgo
    result: 'name','definition','synonyms'
    
    {'numberOfHits': 1,
 'results': [{'id': 'GO:0000006',
   'isObsolete': False,
   'name': 'high-affinity zinc transmembrane transporter activity',
   'definition': {'text': 'Enables the transfer of zinc ions (Zn2+) from one side of a membrane to the other, probably powered by proton motive force. In high-affinity transport the transporter is able to bind the solute even if it is only present at very low concentrations.',
    'xrefs': [{'dbCode': 'TC', 'dbId': '2.A.5.1.1'}]},
   'synonyms': [{'name': 'high-affinity zinc uptake transmembrane transporter activity',
     'type': 'related'},
    {'name': 'high affinity zinc uptake transmembrane transporter activity',
     'type': 'exact'}],
   'aspect': 'molecular_function',
   'usage': 'Unrestricted'}],
 'pageInfo': None}","['quickgo' 'result' ':' 'name' 'definition' 'synonyms' '{' 'numberOfHits'
 ':' '1' 'results' ':' '[' '{' 'id' ':' 'GO' ':' '0000006' 'isObsolete'
 ':' 'False' 'name' ':' 'high' '-' 'affinity' 'zinc' 'transmembrane'
 'transporter' 'activity' 'definition' ':' '{' 'text' ':' 'Enables' 'the'
 'transfer' 'of' 'zinc' 'ions' '(' 'Zn2' '+' ')' 'from' 'one' 'side' 'of'
 'a' 'membrane' 'to' 'the' 'other' 'probably' 'powered' 'by' 'proton'
 'motive' 'force' '.' 'In' 'high' '-' 'affinity' 'transport' 'the'
 'transporter' 'is' 'able' 'to' 'bind' 'the' 'solute' 'even' 'if' 'it'
 'is' 'only' 'present' 'at' 'very' 'low' 'concentrations' '.' 'xrefs' ':'
 '[' '{' 'dbCode' ':' 'TC' 'dbId' ':' '2' '.' 'A' '.' '5' '.' '1' '.' '1'
 '}' ']' '}' 'synonyms' ':' '[' '{' 'name' ':' 'high' '-' 'affinity'
 'zinc' 'uptake' 'transmembrane' 'transporter' 'activity' 'type' ':'
 'related' '}' '{' 'name' ':' 'high' 'affinity' 'zinc' 'uptake'
 'transmembrane' 'transporter' 'activity' 'type' ':' 'exact' '}' ']'
 'aspect' ':' 'molecular_function' 'usage' ':' 'Unrestricted' '}' ']'
 'pageInfo' ':' 'None' '}']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/db/go.py#L2-L25
rraadd88/rohan,rohan/dandage/figs/configure.py,make_figs,"def make_figs(dcfg,dp='figs',force=False):
    """"""
    'figi','plotp'
    """"""
    doutp=abspath(dp)
    if isinstance(dcfg,str):
        dcfg=pd.read_table(dcfg,
                           error_bad_lines=False,
                          comment='#',)        
#     .dropna(subset=['figi','fign','plotn','plotp'])
    dcfg=configure(dcfg,doutp,force=force)
    templatep=f""{doutp}/masonry/index.html""
    if not exists(dirname(templatep)):
        runbashcmd(f""cd {doutp};git clone https://github.com/rraadd88/masonry.git"")
    else:
        try:
            runbashcmd(f""cd {dirname(templatep)};git pull"")    
        except:
            logging.error('can not pull in masonry.git')
    from rohan.dandage.io_files import fill_form   
    for figi in dcfg['figi'].unique():
#         if len(dcfg.loc[(dcfg['ploti']==figi),:])>0:
        htmlp=dcfg.loc[(dcfg['figi']==figi),'fightmlp'].unique()[0]
        print(basename(htmlp))
        if not exists(htmlp) or force:
            fill_form(dcfg.loc[(dcfg['figi']==figi),:],
               templatep=templatep,
               template_insert_line='<div class=""grid__item grid__item--width{plot width scale} grid__item--height{plot height scale}"">\n  <fig class=""plot""><img src=""{plotsvgp}""/><ploti>{ploti}</ploti></fig></div>',
               outp=htmlp,
               splitini='<div class=""grid__gutter-sizer""></div>',
               splitend='</div><!-- class=""grid are-images-unloaded"" -->',
               field2replace={'<link rel=""stylesheet"" href=""css/style.css"">':'<link rel=""stylesheet"" href=""masonry/css/style.css"">',
                             '<script  src=""js/index.js""></script>':'<script  src=""masonry/js/index.js""></script>',
                             f'{doutp}/':''})
            # make pdf
            runbashcmd(f'google-chrome --headless --disable-gpu --virtual-time-budget=10000 --print-to-pdf={htmlp}.pdf {htmlp}')
            print(htmlp)
            
    from rohan.dandage.figs.convert import vectors2rasters
    vectors2rasters(doutp,ext='pdf')",python,"def make_figs(dcfg,dp='figs',force=False):
    """"""
    'figi','plotp'
    """"""
    doutp=abspath(dp)
    if isinstance(dcfg,str):
        dcfg=pd.read_table(dcfg,
                           error_bad_lines=False,
                          comment='#',)        
#     .dropna(subset=['figi','fign','plotn','plotp'])
    dcfg=configure(dcfg,doutp,force=force)
    templatep=f""{doutp}/masonry/index.html""
    if not exists(dirname(templatep)):
        runbashcmd(f""cd {doutp};git clone https://github.com/rraadd88/masonry.git"")
    else:
        try:
            runbashcmd(f""cd {dirname(templatep)};git pull"")    
        except:
            logging.error('can not pull in masonry.git')
    from rohan.dandage.io_files import fill_form   
    for figi in dcfg['figi'].unique():
#         if len(dcfg.loc[(dcfg['ploti']==figi),:])>0:
        htmlp=dcfg.loc[(dcfg['figi']==figi),'fightmlp'].unique()[0]
        print(basename(htmlp))
        if not exists(htmlp) or force:
            fill_form(dcfg.loc[(dcfg['figi']==figi),:],
               templatep=templatep,
               template_insert_line='<div class=""grid__item grid__item--width{plot width scale} grid__item--height{plot height scale}"">\n  <fig class=""plot""><img src=""{plotsvgp}""/><ploti>{ploti}</ploti></fig></div>',
               outp=htmlp,
               splitini='<div class=""grid__gutter-sizer""></div>',
               splitend='</div><!-- class=""grid are-images-unloaded"" -->',
               field2replace={'<link rel=""stylesheet"" href=""css/style.css"">':'<link rel=""stylesheet"" href=""masonry/css/style.css"">',
                             '<script  src=""js/index.js""></script>':'<script  src=""masonry/js/index.js""></script>',
                             f'{doutp}/':''})
            # make pdf
            runbashcmd(f'google-chrome --headless --disable-gpu --virtual-time-budget=10000 --print-to-pdf={htmlp}.pdf {htmlp}')
            print(htmlp)
            
    from rohan.dandage.figs.convert import vectors2rasters
    vectors2rasters(doutp,ext='pdf')","['def' 'make_figs' '(' 'dcfg' ',' 'dp' '=' ""'figs'"" ',' 'force' '='
 'False' ')' ':' 'doutp' '=' 'abspath' '(' 'dp' ')' 'if' 'isinstance' '('
 'dcfg' ',' 'str' ')' ':' 'dcfg' '=' 'pd' '.' 'read_table' '(' 'dcfg' ','
 'error_bad_lines' '=' 'False' ',' 'comment' '=' ""'#'"" ',' ')'
 ""#     .dropna(subset=['figi','fign','plotn','plotp'])"" 'dcfg' '='
 'configure' '(' 'dcfg' ',' 'doutp' ',' 'force' '=' 'force' ')'
 'templatep' '=' 'f""{doutp}/masonry/index.html""' 'if' 'not' 'exists' '('
 'dirname' '(' 'templatep' ')' ')' ':' 'runbashcmd' '('
 'f""cd {doutp};git clone https://github.com/rraadd88/masonry.git""' ')'
 'else' ':' 'try' ':' 'runbashcmd' '('
 'f""cd {dirname(templatep)};git pull""' ')' 'except' ':' 'logging' '.'
 'error' '(' ""'can not pull in masonry.git'"" ')' 'from' 'rohan' '.'
 'dandage' '.' 'io_files' 'import' 'fill_form' 'for' 'figi' 'in' 'dcfg'
 '[' ""'figi'"" ']' '.' 'unique' '(' ')' ':'
 ""#         if len(dcfg.loc[(dcfg['ploti']==figi),:])>0:"" 'htmlp' '='
 'dcfg' '.' 'loc' '[' '(' 'dcfg' '[' ""'figi'"" ']' '==' 'figi' ')' ','
 ""'fightmlp'"" ']' '.' 'unique' '(' ')' '[' '0' ']' 'print' '(' 'basename'
 '(' 'htmlp' ')' ')' 'if' 'not' 'exists' '(' 'htmlp' ')' 'or' 'force' ':'
 'fill_form' '(' 'dcfg' '.' 'loc' '[' '(' 'dcfg' '[' ""'figi'"" ']' '=='
 'figi' ')' ',' ':' ']' ',' 'templatep' '=' 'templatep' ','
 'template_insert_line' '='
 '\'<div class=""grid__item grid__item--width{plot width scale} grid__item--height{plot height scale}"">\\n  <fig class=""plot""><img src=""{plotsvgp}""/><ploti>{ploti}</ploti></fig></div>\''
 ',' 'outp' '=' 'htmlp' ',' 'splitini' '='
 '\'<div class=""grid__gutter-sizer""></div>\'' ',' 'splitend' '='
 '\'</div><!-- class=""grid are-images-unloaded"" -->\'' ',' 'field2replace'
 '=' '{' '\'<link rel=""stylesheet"" href=""css/style.css"">\'' ':'
 '\'<link rel=""stylesheet"" href=""masonry/css/style.css"">\'' ','
 '\'<script  src=""js/index.js""></script>\'' ':'
 '\'<script  src=""masonry/js/index.js""></script>\'' ',' ""f'{doutp}/'"" ':'
 ""''"" '}' ')' '# make pdf' 'runbashcmd' '('
 ""f'google-chrome --headless --disable-gpu --virtual-time-budget=10000 --print-to-pdf={htmlp}.pdf {htmlp}'""
 ')' 'print' '(' 'htmlp' ')' 'from' 'rohan' '.' 'dandage' '.' 'figs' '.'
 'convert' 'import' 'vectors2rasters' 'vectors2rasters' '(' 'doutp' ','
 'ext' '=' ""'pdf'"" ')']","'figi','plotp'",['figi' 'plotp'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/figs/configure.py#L110-L149
rraadd88/rohan,rohan/dandage/io_strs.py,isstrallowed,"def isstrallowed(s,form):
    """"""
    Checks is input string conforms to input regex (`form`).

    :param s: input string.
    :param form: eg. for hdf5: `""^[a-zA-Z_][a-zA-Z0-9_]*$""`
    """"""
    import re
    match = re.match(form,s)
    return match is not None",python,"def isstrallowed(s,form):
    """"""
    Checks is input string conforms to input regex (`form`).

    :param s: input string.
    :param form: eg. for hdf5: `""^[a-zA-Z_][a-zA-Z0-9_]*$""`
    """"""
    import re
    match = re.match(form,s)
    return match is not None","['def' 'isstrallowed' '(' 's' ',' 'form' ')' ':' 'import' 're' 'match' '='
 're' '.' 'match' '(' 'form' ',' 's' ')' 'return' 'match' 'is' 'not'
 'None']","Checks is input string conforms to input regex (`form`).

    :param s: input string.
    :param form: eg. for hdf5: `""^[a-zA-Z_][a-zA-Z0-9_]*$""`","['Checks' 'is' 'input' 'string' 'conforms' 'to' 'input' 'regex' '(' 'form'
 ')' '.']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_strs.py#L90-L99
rraadd88/rohan,rohan/dandage/io_strs.py,convertstr2format,"def convertstr2format(col,form):
    """"""
    Convert input string to input regex (`form`).
    
    :param col: input string.
    :param form: eg. for hdf5: `""^[a-zA-Z_][a-zA-Z0-9_]*$""`
    """"""
    if not isstrallowed(col,form):
        col=col.replace("" "",""_"") 
        if not isstrallowed(col,form):
            chars_disallowed=[char for char in col if not isstrallowed(char,form)]
            for char in chars_disallowed:
                col=col.replace(char,""_"")
    return col",python,"def convertstr2format(col,form):
    """"""
    Convert input string to input regex (`form`).
    
    :param col: input string.
    :param form: eg. for hdf5: `""^[a-zA-Z_][a-zA-Z0-9_]*$""`
    """"""
    if not isstrallowed(col,form):
        col=col.replace("" "",""_"") 
        if not isstrallowed(col,form):
            chars_disallowed=[char for char in col if not isstrallowed(char,form)]
            for char in chars_disallowed:
                col=col.replace(char,""_"")
    return col","['def' 'convertstr2format' '(' 'col' ',' 'form' ')' ':' 'if' 'not'
 'isstrallowed' '(' 'col' ',' 'form' ')' ':' 'col' '=' 'col' '.' 'replace'
 '(' '"" ""' ',' '""_""' ')' 'if' 'not' 'isstrallowed' '(' 'col' ',' 'form'
 ')' ':' 'chars_disallowed' '=' '[' 'char' 'for' 'char' 'in' 'col' 'if'
 'not' 'isstrallowed' '(' 'char' ',' 'form' ')' ']' 'for' 'char' 'in'
 'chars_disallowed' ':' 'col' '=' 'col' '.' 'replace' '(' 'char' ',' '""_""'
 ')' 'return' 'col']","Convert input string to input regex (`form`).
    
    :param col: input string.
    :param form: eg. for hdf5: `""^[a-zA-Z_][a-zA-Z0-9_]*$""`","['Convert' 'input' 'string' 'to' 'input' 'regex' '(' 'form' ')' '.' ':'
 'param' 'col' ':' 'input' 'string' '.' ':' 'param' 'form' ':' 'eg' '.'
 'for' 'hdf5' ':' '^' '[' 'a' '-' 'zA' '-' 'Z_' ']' '[' 'a' '-' 'zA' '-'
 'Z0' '-' '9_' ']' '*' '$']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_strs.py#L101-L114
rraadd88/rohan,rohan/dandage/io_strs.py,make_pathable_string,"def make_pathable_string(s,replacewith='_'):
    """"""
    Removes symbols from a string to be compatible with directory structure.

    :param s: string
    """"""
    import re
    return re.sub(r'[^\w+/.]',replacewith, s.lower())",python,"def make_pathable_string(s,replacewith='_'):
    """"""
    Removes symbols from a string to be compatible with directory structure.

    :param s: string
    """"""
    import re
    return re.sub(r'[^\w+/.]',replacewith, s.lower())","['def' 'make_pathable_string' '(' 's' ',' 'replacewith' '=' ""'_'"" ')' ':'
 'import' 're' 'return' 're' '.' 'sub' '(' ""r'[^\\w+/.]'"" ','
 'replacewith' ',' 's' '.' 'lower' '(' ')' ')']","Removes symbols from a string to be compatible with directory structure.

    :param s: string","['Removes' 'symbols' 'from' 'a' 'string' 'to' 'be' 'compatible' 'with'
 'directory' 'structure' '.']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_strs.py#L120-L127
rraadd88/rohan,rohan/dandage/io_strs.py,linebreaker,"def linebreaker(l,break_pt=16):
    """"""
    used for adding labels in plots.

    :param l: list of strings
    :param break_pt: number, insert new line after this many letters 
    """"""

    l_out=[]
    for i in l:
        if len(i)>break_pt:
            i_words=i.split(' ')
            i_out=''
            line_len=0
            for w in i_words:
                line_len+=len(w)+1
                if i_words.index(w)==0:
                    i_out=w
                elif line_len>break_pt:
                    line_len=0
                    i_out=""%s\n%s"" % (i_out,w)
                else:
                    i_out=""%s %s"" % (i_out,w)
            l_out.append(i_out)    
#             l_out.append(""%s\n%s"" % (i[:break_pt],i[break_pt:]))
        else:
            l_out.append(i)
    return l_out",python,"def linebreaker(l,break_pt=16):
    """"""
    used for adding labels in plots.

    :param l: list of strings
    :param break_pt: number, insert new line after this many letters 
    """"""

    l_out=[]
    for i in l:
        if len(i)>break_pt:
            i_words=i.split(' ')
            i_out=''
            line_len=0
            for w in i_words:
                line_len+=len(w)+1
                if i_words.index(w)==0:
                    i_out=w
                elif line_len>break_pt:
                    line_len=0
                    i_out=""%s\n%s"" % (i_out,w)
                else:
                    i_out=""%s %s"" % (i_out,w)
            l_out.append(i_out)    
#             l_out.append(""%s\n%s"" % (i[:break_pt],i[break_pt:]))
        else:
            l_out.append(i)
    return l_out","['def' 'linebreaker' '(' 'l' ',' 'break_pt' '=' '16' ')' ':' 'l_out' '='
 '[' ']' 'for' 'i' 'in' 'l' ':' 'if' 'len' '(' 'i' ')' '>' 'break_pt' ':'
 'i_words' '=' 'i' '.' 'split' '(' ""' '"" ')' 'i_out' '=' ""''"" 'line_len'
 '=' '0' 'for' 'w' 'in' 'i_words' ':' 'line_len' '+=' 'len' '(' 'w' ')'
 '+' '1' 'if' 'i_words' '.' 'index' '(' 'w' ')' '==' '0' ':' 'i_out' '='
 'w' 'elif' 'line_len' '>' 'break_pt' ':' 'line_len' '=' '0' 'i_out' '='
 '""%s\\n%s""' '%' '(' 'i_out' ',' 'w' ')' 'else' ':' 'i_out' '=' '""%s %s""'
 '%' '(' 'i_out' ',' 'w' ')' 'l_out' '.' 'append' '(' 'i_out' ')'
 '#             l_out.append(""%s\\n%s"" % (i[:break_pt],i[break_pt:]))'
 'else' ':' 'l_out' '.' 'append' '(' 'i' ')' 'return' 'l_out']","used for adding labels in plots.

    :param l: list of strings
    :param break_pt: number, insert new line after this many letters",['used' 'for' 'adding' 'labels' 'in' 'plots' '.'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_strs.py#L130-L157
rraadd88/rohan,rohan/dandage/io_strs.py,splitlabel,"def splitlabel(label,splitby=' ',ctrl='__'):
    """"""
    used for adding labels in plots.

    :param label: string
    :param splitby: string split the label by this character/string
    :param ctrl: string, marker that denotes a control condition  
    """"""
    splits=label.split(splitby)
    if len(splits)==2:
        return splits
    elif len(splits)==1:

        return splits+[ctrl]",python,"def splitlabel(label,splitby=' ',ctrl='__'):
    """"""
    used for adding labels in plots.

    :param label: string
    :param splitby: string split the label by this character/string
    :param ctrl: string, marker that denotes a control condition  
    """"""
    splits=label.split(splitby)
    if len(splits)==2:
        return splits
    elif len(splits)==1:

        return splits+[ctrl]","['def' 'splitlabel' '(' 'label' ',' 'splitby' '=' ""' '"" ',' 'ctrl' '='
 ""'__'"" ')' ':' 'splits' '=' 'label' '.' 'split' '(' 'splitby' ')' 'if'
 'len' '(' 'splits' ')' '==' '2' ':' 'return' 'splits' 'elif' 'len' '('
 'splits' ')' '==' '1' ':' 'return' 'splits' '+' '[' 'ctrl' ']']","used for adding labels in plots.

    :param label: string
    :param splitby: string split the label by this character/string
    :param ctrl: string, marker that denotes a control condition",['used' 'for' 'adding' 'labels' 'in' 'plots' '.'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_strs.py#L159-L172
rraadd88/rohan,rohan/dandage/io_strs.py,get_time,"def get_time():
    """"""
    Gets current time in a form of a formated string. Used in logger function.

    """"""
    import datetime
    time=make_pathable_string('%s' % datetime.datetime.now())
    return time.replace('-','_').replace(':','_').replace('.','_')",python,"def get_time():
    """"""
    Gets current time in a form of a formated string. Used in logger function.

    """"""
    import datetime
    time=make_pathable_string('%s' % datetime.datetime.now())
    return time.replace('-','_').replace(':','_').replace('.','_')","['def' 'get_time' '(' ')' ':' 'import' 'datetime' 'time' '='
 'make_pathable_string' '(' ""'%s'"" '%' 'datetime' '.' 'datetime' '.' 'now'
 '(' ')' ')' 'return' 'time' '.' 'replace' '(' ""'-'"" ',' ""'_'"" ')' '.'
 'replace' '(' ""':'"" ',' ""'_'"" ')' '.' 'replace' '(' ""'.'"" ',' ""'_'"" ')']",Gets current time in a form of a formated string. Used in logger function.,"['Gets' 'current' 'time' 'in' 'a' 'form' 'of' 'a' 'formated' 'string' '.'
 'Used' 'in' 'logger' 'function' '.']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_strs.py#L174-L181
rraadd88/rohan,rohan/dandage/stat/norm.py,quantile_norm,"def quantile_norm(X):
    """"""Normalize the columns of X to each have the same distribution.

    Given an expression matrix (microarray data, read counts, etc) of M genes
    by N samples, quantile normalization ensures all samples have the same
    spread of data (by construction).

    The data across each row are averaged to obtain an average column. Each
    column quantile is replaced with the corresponding quantile of the average
    column.

    Parameters
    ----------
    X : 2D array of float, shape (M, N)
        The input data, with M rows (genes/features) and N columns (samples).

    Returns
    -------
    Xn : 2D array of float, shape (M, N)
        The normalized data.
    """"""
    # compute the quantiles
    quantiles = np.mean(np.sort(X, axis=0), axis=1)

    # compute the column-wise ranks. Each observation is replaced with its
    # rank in that column: the smallest observation is replaced by 1, the
    # second-smallest by 2, ..., and the largest by M, the number of rows.
    ranks = np.apply_along_axis(stats.rankdata, 0, X)

    # convert ranks to integer indices from 0 to M-1
    rank_indices = ranks.astype(int) - 1

    # index the quantiles for each rank with the ranks matrix
    Xn = quantiles[rank_indices]

    return(Xn)",python,"def quantile_norm(X):
    """"""Normalize the columns of X to each have the same distribution.

    Given an expression matrix (microarray data, read counts, etc) of M genes
    by N samples, quantile normalization ensures all samples have the same
    spread of data (by construction).

    The data across each row are averaged to obtain an average column. Each
    column quantile is replaced with the corresponding quantile of the average
    column.

    Parameters
    ----------
    X : 2D array of float, shape (M, N)
        The input data, with M rows (genes/features) and N columns (samples).

    Returns
    -------
    Xn : 2D array of float, shape (M, N)
        The normalized data.
    """"""
    # compute the quantiles
    quantiles = np.mean(np.sort(X, axis=0), axis=1)

    # compute the column-wise ranks. Each observation is replaced with its
    # rank in that column: the smallest observation is replaced by 1, the
    # second-smallest by 2, ..., and the largest by M, the number of rows.
    ranks = np.apply_along_axis(stats.rankdata, 0, X)

    # convert ranks to integer indices from 0 to M-1
    rank_indices = ranks.astype(int) - 1

    # index the quantiles for each rank with the ranks matrix
    Xn = quantiles[rank_indices]

    return(Xn)","['def' 'quantile_norm' '(' 'X' ')' ':' '# compute the quantiles'
 'quantiles' '=' 'np' '.' 'mean' '(' 'np' '.' 'sort' '(' 'X' ',' 'axis'
 '=' '0' ')' ',' 'axis' '=' '1' ')'
 '# compute the column-wise ranks. Each observation is replaced with its'
 '# rank in that column: the smallest observation is replaced by 1, the'
 '# second-smallest by 2, ..., and the largest by M, the number of rows.'
 'ranks' '=' 'np' '.' 'apply_along_axis' '(' 'stats' '.' 'rankdata' ','
 '0' ',' 'X' ')' '# convert ranks to integer indices from 0 to M-1'
 'rank_indices' '=' 'ranks' '.' 'astype' '(' 'int' ')' '-' '1'
 '# index the quantiles for each rank with the ranks matrix' 'Xn' '='
 'quantiles' '[' 'rank_indices' ']' 'return' '(' 'Xn' ')']","Normalize the columns of X to each have the same distribution.

    Given an expression matrix (microarray data, read counts, etc) of M genes
    by N samples, quantile normalization ensures all samples have the same
    spread of data (by construction).

    The data across each row are averaged to obtain an average column. Each
    column quantile is replaced with the corresponding quantile of the average
    column.

    Parameters
    ----------
    X : 2D array of float, shape (M, N)
        The input data, with M rows (genes/features) and N columns (samples).

    Returns
    -------
    Xn : 2D array of float, shape (M, N)
        The normalized data.","['Normalize' 'the' 'columns' 'of' 'X' 'to' 'each' 'have' 'the' 'same'
 'distribution' '.']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/stat/norm.py#L4-L39
rraadd88/rohan,rohan/dandage/stat/corr.py,corrdfs,"def corrdfs(df1,df2,method):
    """"""
    df1 in columns
    df2 in rows    
    """"""
    dcorr=pd.DataFrame(columns=df1.columns,index=df2.columns)
    dpval=pd.DataFrame(columns=df1.columns,index=df2.columns)
    for c1 in df1:
        for c2 in df2:
            if method=='spearman':
                dcorr.loc[c2,c1],dpval.loc[c2,c1]=spearmanr(df1[c1],df2[c2],
                                                        nan_policy='omit'
                                                       )
            elif method=='pearson':
                dcorr.loc[c2,c1],dpval.loc[c2,c1]=pearsonr(df1[c1],df2[c2],
#                                                         nan_policy='omit'
                                                       )
                
    if not df1.columns.name is None:
        dcorr.columns.name=df1.columns.name
        dpval.columns.name=df1.columns.name
    if not df2.columns.name is None:
        dcorr.index.name=df2.columns.name
        dpval.index.name=df2.columns.name
    return dcorr,dpval",python,"def corrdfs(df1,df2,method):
    """"""
    df1 in columns
    df2 in rows    
    """"""
    dcorr=pd.DataFrame(columns=df1.columns,index=df2.columns)
    dpval=pd.DataFrame(columns=df1.columns,index=df2.columns)
    for c1 in df1:
        for c2 in df2:
            if method=='spearman':
                dcorr.loc[c2,c1],dpval.loc[c2,c1]=spearmanr(df1[c1],df2[c2],
                                                        nan_policy='omit'
                                                       )
            elif method=='pearson':
                dcorr.loc[c2,c1],dpval.loc[c2,c1]=pearsonr(df1[c1],df2[c2],
#                                                         nan_policy='omit'
                                                       )
                
    if not df1.columns.name is None:
        dcorr.columns.name=df1.columns.name
        dpval.columns.name=df1.columns.name
    if not df2.columns.name is None:
        dcorr.index.name=df2.columns.name
        dpval.index.name=df2.columns.name
    return dcorr,dpval","['def' 'corrdfs' '(' 'df1' ',' 'df2' ',' 'method' ')' ':' 'dcorr' '=' 'pd'
 '.' 'DataFrame' '(' 'columns' '=' 'df1' '.' 'columns' ',' 'index' '='
 'df2' '.' 'columns' ')' 'dpval' '=' 'pd' '.' 'DataFrame' '(' 'columns'
 '=' 'df1' '.' 'columns' ',' 'index' '=' 'df2' '.' 'columns' ')' 'for'
 'c1' 'in' 'df1' ':' 'for' 'c2' 'in' 'df2' ':' 'if' 'method' '=='
 ""'spearman'"" ':' 'dcorr' '.' 'loc' '[' 'c2' ',' 'c1' ']' ',' 'dpval' '.'
 'loc' '[' 'c2' ',' 'c1' ']' '=' 'spearmanr' '(' 'df1' '[' 'c1' ']' ','
 'df2' '[' 'c2' ']' ',' 'nan_policy' '=' ""'omit'"" ')' 'elif' 'method' '=='
 ""'pearson'"" ':' 'dcorr' '.' 'loc' '[' 'c2' ',' 'c1' ']' ',' 'dpval' '.'
 'loc' '[' 'c2' ',' 'c1' ']' '=' 'pearsonr' '(' 'df1' '[' 'c1' ']' ','
 'df2' '[' 'c2' ']' ','
 ""#                                                         nan_policy='omit'""
 ')' 'if' 'not' 'df1' '.' 'columns' '.' 'name' 'is' 'None' ':' 'dcorr' '.'
 'columns' '.' 'name' '=' 'df1' '.' 'columns' '.' 'name' 'dpval' '.'
 'columns' '.' 'name' '=' 'df1' '.' 'columns' '.' 'name' 'if' 'not' 'df2'
 '.' 'columns' '.' 'name' 'is' 'None' ':' 'dcorr' '.' 'index' '.' 'name'
 '=' 'df2' '.' 'columns' '.' 'name' 'dpval' '.' 'index' '.' 'name' '='
 'df2' '.' 'columns' '.' 'name' 'return' 'dcorr' ',' 'dpval']","df1 in columns
    df2 in rows",['df1' 'in' 'columns' 'df2' 'in' 'rows'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/stat/corr.py#L8-L32
rraadd88/rohan,rohan/dandage/io_seqs.py,gffatributes2ids,"def gffatributes2ids(s):
    """"""
    Deconvolutes ids from `attributes` column in GFF3 to seprate columns.
    :param s: attribute string.
    :returns: tuple of ids
    """"""
    Name,gene_id,transcript_id,protein_id,exon_id=np.nan,np.nan,np.nan,np.nan,np.nan
    if '=' in s:
        d=dict([i.split('=') for i in s.split(';')])
        if 'Parent' in d:
            d[d['Parent'].split(':')[0]+'_id']=d['Parent'].split(':')[1]
        Name,gene_id,transcript_id,protein_id,exon_id=np.nan,np.nan,np.nan,np.nan,np.nan
        if 'Name' in d:    
            Name=d['Name']
        if 'gene_id' in d:    
            gene_id=d['gene_id']
        if 'transcript_id' in d:    
            transcript_id=d['transcript_id']
        if 'protein_id' in d:    
            protein_id=d['protein_id']
        if 'exon_id' in d:    
            exon_id=d['exon_id']
    return Name,gene_id,transcript_id,protein_id,exon_id",python,"def gffatributes2ids(s):
    """"""
    Deconvolutes ids from `attributes` column in GFF3 to seprate columns.
    :param s: attribute string.
    :returns: tuple of ids
    """"""
    Name,gene_id,transcript_id,protein_id,exon_id=np.nan,np.nan,np.nan,np.nan,np.nan
    if '=' in s:
        d=dict([i.split('=') for i in s.split(';')])
        if 'Parent' in d:
            d[d['Parent'].split(':')[0]+'_id']=d['Parent'].split(':')[1]
        Name,gene_id,transcript_id,protein_id,exon_id=np.nan,np.nan,np.nan,np.nan,np.nan
        if 'Name' in d:    
            Name=d['Name']
        if 'gene_id' in d:    
            gene_id=d['gene_id']
        if 'transcript_id' in d:    
            transcript_id=d['transcript_id']
        if 'protein_id' in d:    
            protein_id=d['protein_id']
        if 'exon_id' in d:    
            exon_id=d['exon_id']
    return Name,gene_id,transcript_id,protein_id,exon_id","['def' 'gffatributes2ids' '(' 's' ')' ':' 'Name' ',' 'gene_id' ','
 'transcript_id' ',' 'protein_id' ',' 'exon_id' '=' 'np' '.' 'nan' ','
 'np' '.' 'nan' ',' 'np' '.' 'nan' ',' 'np' '.' 'nan' ',' 'np' '.' 'nan'
 'if' ""'='"" 'in' 's' ':' 'd' '=' 'dict' '(' '[' 'i' '.' 'split' '(' ""'='""
 ')' 'for' 'i' 'in' 's' '.' 'split' '(' ""';'"" ')' ']' ')' 'if' ""'Parent'""
 'in' 'd' ':' 'd' '[' 'd' '[' ""'Parent'"" ']' '.' 'split' '(' ""':'"" ')' '['
 '0' ']' '+' ""'_id'"" ']' '=' 'd' '[' ""'Parent'"" ']' '.' 'split' '(' ""':'""
 ')' '[' '1' ']' 'Name' ',' 'gene_id' ',' 'transcript_id' ',' 'protein_id'
 ',' 'exon_id' '=' 'np' '.' 'nan' ',' 'np' '.' 'nan' ',' 'np' '.' 'nan'
 ',' 'np' '.' 'nan' ',' 'np' '.' 'nan' 'if' ""'Name'"" 'in' 'd' ':' 'Name'
 '=' 'd' '[' ""'Name'"" ']' 'if' ""'gene_id'"" 'in' 'd' ':' 'gene_id' '=' 'd'
 '[' ""'gene_id'"" ']' 'if' ""'transcript_id'"" 'in' 'd' ':' 'transcript_id'
 '=' 'd' '[' ""'transcript_id'"" ']' 'if' ""'protein_id'"" 'in' 'd' ':'
 'protein_id' '=' 'd' '[' ""'protein_id'"" ']' 'if' ""'exon_id'"" 'in' 'd' ':'
 'exon_id' '=' 'd' '[' ""'exon_id'"" ']' 'return' 'Name' ',' 'gene_id' ','
 'transcript_id' ',' 'protein_id' ',' 'exon_id']","Deconvolutes ids from `attributes` column in GFF3 to seprate columns.
    :param s: attribute string.
    :returns: tuple of ids","['Deconvolutes' 'ids' 'from' 'attributes' 'column' 'in' 'GFF3' 'to'
 'seprate' 'columns' '.' ':' 'param' 's' ':' 'attribute' 'string' '.' ':'
 'returns' ':' 'tuple' 'of' 'ids']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_seqs.py#L107-L129
rraadd88/rohan,rohan/dandage/io_seqs.py,hamming_distance,"def hamming_distance(s1, s2):
    """"""Return the Hamming distance between equal-length sequences""""""
#     print(s1,s2)
    if len(s1) != len(s2):
        raise ValueError(""Undefined for sequences of unequal length"")
    return sum(el1 != el2 for el1, el2 in zip(s1.upper(), s2.upper()))",python,"def hamming_distance(s1, s2):
    """"""Return the Hamming distance between equal-length sequences""""""
#     print(s1,s2)
    if len(s1) != len(s2):
        raise ValueError(""Undefined for sequences of unequal length"")
    return sum(el1 != el2 for el1, el2 in zip(s1.upper(), s2.upper()))","['def' 'hamming_distance' '(' 's1' ',' 's2' ')' ':' '#     print(s1,s2)'
 'if' 'len' '(' 's1' ')' '!=' 'len' '(' 's2' ')' ':' 'raise' 'ValueError'
 '(' '""Undefined for sequences of unequal length""' ')' 'return' 'sum' '('
 'el1' '!=' 'el2' 'for' 'el1' ',' 'el2' 'in' 'zip' '(' 's1' '.' 'upper'
 '(' ')' ',' 's2' '.' 'upper' '(' ')' ')' ')']",Return the Hamming distance between equal-length sequences,"['Return' 'the' 'Hamming' 'distance' 'between' 'equal' '-' 'length'
 'sequences']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_seqs.py#L131-L136
rraadd88/rohan,rohan/dandage/io_seqs.py,align,"def align(s1,s2,test=False,seqfmt='dna',
         psm=None,pmm=None,pgo=None,pge=None,
         matrix=None,
         outscore=False):
    """"""
    Creates pairwise local alignment between seqeunces.
    Get the visualization and alignment scores.
    :param s1: seqeunce 1
    :param s2: seqeunce 2    
    
    REF: http://biopython.org/DIST/docs/api/Bio.pairwise2-module.html
    The match parameters are:

    CODE  DESCRIPTION
    x     No parameters. Identical characters have score of 1, otherwise 0.
    m     A match score is the score of identical chars, otherwise mismatch
          score.
    d     A dictionary returns the score of any pair of characters.
    c     A callback function returns scores.
    The gap penalty parameters are:

    CODE  DESCRIPTION
    x     No gap penalties.
    s     Same open and extend gap penalties for both sequences.
    d     The sequences have different open and extend gap penalties.
    c     A callback function returns the gap penalties.  
    --
    DNA: 
    localms: psm=2,pmm=0.5,pgo=-3,pge=-1):
    Protein:
    http://resources.qiagenbioinformatics.com/manuals/clcgenomicsworkbench/650/Use_scoring_matrices.html
    """"""
    import operator
    from Bio import pairwise2
    if seqfmt=='dna':
        if any([p is None for p in [psm,pmm,pgo,pge]]):
            alignments = pairwise2.align.localxx(s1.upper(),s2.upper())
        else:
            alignments = pairwise2.align.localms(s1.upper(),s2.upper(),psm,pmm,pgo,pge)
    elif seqfmt=='protein':
        from Bio.pairwise2 import format_alignment
        from Bio.SubsMat import MatrixInfo
        if matrix is None:
            matrix = MatrixInfo.blosum62
        alignments =pairwise2.align.globaldx(s1, s2, matrix)
#         print(format_alignment(*a))        
    if test:
        print(alignments)
    alignsymb=np.nan
    score=np.nan
    sorted_alignments = sorted(alignments, key=operator.itemgetter(2))
    for a in alignments:
        alignstr=pairwise2.format_alignment(*a)
        alignsymb=alignstr.split('\n')[1]
        score=a[2]
        if test:
            print(alignstr)
        break
    if not outscore:
        return alignsymb.replace(' ','-'),score
    else:
        return score",python,"def align(s1,s2,test=False,seqfmt='dna',
         psm=None,pmm=None,pgo=None,pge=None,
         matrix=None,
         outscore=False):
    """"""
    Creates pairwise local alignment between seqeunces.
    Get the visualization and alignment scores.
    :param s1: seqeunce 1
    :param s2: seqeunce 2    
    
    REF: http://biopython.org/DIST/docs/api/Bio.pairwise2-module.html
    The match parameters are:

    CODE  DESCRIPTION
    x     No parameters. Identical characters have score of 1, otherwise 0.
    m     A match score is the score of identical chars, otherwise mismatch
          score.
    d     A dictionary returns the score of any pair of characters.
    c     A callback function returns scores.
    The gap penalty parameters are:

    CODE  DESCRIPTION
    x     No gap penalties.
    s     Same open and extend gap penalties for both sequences.
    d     The sequences have different open and extend gap penalties.
    c     A callback function returns the gap penalties.  
    --
    DNA: 
    localms: psm=2,pmm=0.5,pgo=-3,pge=-1):
    Protein:
    http://resources.qiagenbioinformatics.com/manuals/clcgenomicsworkbench/650/Use_scoring_matrices.html
    """"""
    import operator
    from Bio import pairwise2
    if seqfmt=='dna':
        if any([p is None for p in [psm,pmm,pgo,pge]]):
            alignments = pairwise2.align.localxx(s1.upper(),s2.upper())
        else:
            alignments = pairwise2.align.localms(s1.upper(),s2.upper(),psm,pmm,pgo,pge)
    elif seqfmt=='protein':
        from Bio.pairwise2 import format_alignment
        from Bio.SubsMat import MatrixInfo
        if matrix is None:
            matrix = MatrixInfo.blosum62
        alignments =pairwise2.align.globaldx(s1, s2, matrix)
#         print(format_alignment(*a))        
    if test:
        print(alignments)
    alignsymb=np.nan
    score=np.nan
    sorted_alignments = sorted(alignments, key=operator.itemgetter(2))
    for a in alignments:
        alignstr=pairwise2.format_alignment(*a)
        alignsymb=alignstr.split('\n')[1]
        score=a[2]
        if test:
            print(alignstr)
        break
    if not outscore:
        return alignsymb.replace(' ','-'),score
    else:
        return score","['def' 'align' '(' 's1' ',' 's2' ',' 'test' '=' 'False' ',' 'seqfmt' '='
 ""'dna'"" ',' 'psm' '=' 'None' ',' 'pmm' '=' 'None' ',' 'pgo' '=' 'None'
 ',' 'pge' '=' 'None' ',' 'matrix' '=' 'None' ',' 'outscore' '=' 'False'
 ')' ':' 'import' 'operator' 'from' 'Bio' 'import' 'pairwise2' 'if'
 'seqfmt' '==' ""'dna'"" ':' 'if' 'any' '(' '[' 'p' 'is' 'None' 'for' 'p'
 'in' '[' 'psm' ',' 'pmm' ',' 'pgo' ',' 'pge' ']' ']' ')' ':' 'alignments'
 '=' 'pairwise2' '.' 'align' '.' 'localxx' '(' 's1' '.' 'upper' '(' ')'
 ',' 's2' '.' 'upper' '(' ')' ')' 'else' ':' 'alignments' '=' 'pairwise2'
 '.' 'align' '.' 'localms' '(' 's1' '.' 'upper' '(' ')' ',' 's2' '.'
 'upper' '(' ')' ',' 'psm' ',' 'pmm' ',' 'pgo' ',' 'pge' ')' 'elif'
 'seqfmt' '==' ""'protein'"" ':' 'from' 'Bio' '.' 'pairwise2' 'import'
 'format_alignment' 'from' 'Bio' '.' 'SubsMat' 'import' 'MatrixInfo' 'if'
 'matrix' 'is' 'None' ':' 'matrix' '=' 'MatrixInfo' '.' 'blosum62'
 'alignments' '=' 'pairwise2' '.' 'align' '.' 'globaldx' '(' 's1' ',' 's2'
 ',' 'matrix' ')' '#         print(format_alignment(*a))        ' 'if'
 'test' ':' 'print' '(' 'alignments' ')' 'alignsymb' '=' 'np' '.' 'nan'
 'score' '=' 'np' '.' 'nan' 'sorted_alignments' '=' 'sorted' '('
 'alignments' ',' 'key' '=' 'operator' '.' 'itemgetter' '(' '2' ')' ')'
 'for' 'a' 'in' 'alignments' ':' 'alignstr' '=' 'pairwise2' '.'
 'format_alignment' '(' '*' 'a' ')' 'alignsymb' '=' 'alignstr' '.' 'split'
 '(' ""'\\n'"" ')' '[' '1' ']' 'score' '=' 'a' '[' '2' ']' 'if' 'test' ':'
 'print' '(' 'alignstr' ')' 'break' 'if' 'not' 'outscore' ':' 'return'
 'alignsymb' '.' 'replace' '(' ""' '"" ',' ""'-'"" ')' ',' 'score' 'else' ':'
 'return' 'score']","Creates pairwise local alignment between seqeunces.
    Get the visualization and alignment scores.
    :param s1: seqeunce 1
    :param s2: seqeunce 2    
    
    REF: http://biopython.org/DIST/docs/api/Bio.pairwise2-module.html
    The match parameters are:

    CODE  DESCRIPTION
    x     No parameters. Identical characters have score of 1, otherwise 0.
    m     A match score is the score of identical chars, otherwise mismatch
          score.
    d     A dictionary returns the score of any pair of characters.
    c     A callback function returns scores.
    The gap penalty parameters are:

    CODE  DESCRIPTION
    x     No gap penalties.
    s     Same open and extend gap penalties for both sequences.
    d     The sequences have different open and extend gap penalties.
    c     A callback function returns the gap penalties.  
    --
    DNA: 
    localms: psm=2,pmm=0.5,pgo=-3,pge=-1):
    Protein:
    http://resources.qiagenbioinformatics.com/manuals/clcgenomicsworkbench/650/Use_scoring_matrices.html","['Creates' 'pairwise' 'local' 'alignment' 'between' 'seqeunces' '.' 'Get'
 'the' 'visualization' 'and' 'alignment' 'scores' '.' ':' 'param' 's1' ':'
 'seqeunce' '1' ':' 'param' 's2' ':' 'seqeunce' '2' 'REF' ':' 'http' ':'
 '//' 'biopython' '.' 'org' '/' 'DIST' '/' 'docs' '/' 'api' '/' 'Bio' '.'
 'pairwise2' '-' 'module' '.' 'html' 'The' 'match' 'parameters' 'are' ':']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_seqs.py#L138-L199
rraadd88/rohan,rohan/dandage/io_seqs.py,translate,"def translate(dnaseq,host='human',fmtout=str,tax_id=None):
    """"""
    Translates a DNA seqeunce
    :param dnaseq: DNA sequence
    :param host: host organism
    :param fmtout: format of output sequence
    """"""
    if isinstance(dnaseq,str): 
        dnaseq=Seq.Seq(dnaseq,Alphabet.generic_dna)
    if tax_id is None:
        tax_id=1 # stanndard codon table. ref http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc25
    prtseq=dnaseq.translate(table=tax_id)
    if fmtout is str:
        return str(prtseq)
    else:
        return prtseq",python,"def translate(dnaseq,host='human',fmtout=str,tax_id=None):
    """"""
    Translates a DNA seqeunce
    :param dnaseq: DNA sequence
    :param host: host organism
    :param fmtout: format of output sequence
    """"""
    if isinstance(dnaseq,str): 
        dnaseq=Seq.Seq(dnaseq,Alphabet.generic_dna)
    if tax_id is None:
        tax_id=1 # stanndard codon table. ref http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc25
    prtseq=dnaseq.translate(table=tax_id)
    if fmtout is str:
        return str(prtseq)
    else:
        return prtseq","['def' 'translate' '(' 'dnaseq' ',' 'host' '=' ""'human'"" ',' 'fmtout' '='
 'str' ',' 'tax_id' '=' 'None' ')' ':' 'if' 'isinstance' '(' 'dnaseq' ','
 'str' ')' ':' 'dnaseq' '=' 'Seq' '.' 'Seq' '(' 'dnaseq' ',' 'Alphabet'
 '.' 'generic_dna' ')' 'if' 'tax_id' 'is' 'None' ':' 'tax_id' '=' '1'
 '# stanndard codon table. ref http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc25'
 'prtseq' '=' 'dnaseq' '.' 'translate' '(' 'table' '=' 'tax_id' ')' 'if'
 'fmtout' 'is' 'str' ':' 'return' 'str' '(' 'prtseq' ')' 'else' ':'
 'return' 'prtseq']","Translates a DNA seqeunce
    :param dnaseq: DNA sequence
    :param host: host organism
    :param fmtout: format of output sequence","['Translates' 'a' 'DNA' 'seqeunce' ':' 'param' 'dnaseq' ':' 'DNA'
 'sequence' ':' 'param' 'host' ':' 'host' 'organism' ':' 'param' 'fmtout'
 ':' 'format' 'of' 'output' 'sequence']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_seqs.py#L201-L216
rraadd88/rohan,rohan/dandage/align/__init__.py,get_genomes,"def get_genomes(cfg):
    """"""
    Installs genomes
    
    :param cfg: configuration dict
    """"""
    if cfg['host']=='homo_sapiens':
        contigs=['1',
                 '2',
                 '3',
                 '4',
                 '5',
                 '6',
                 '7',
                 '8',
                 '9',
                 '10',
                 '11',
                 '12',
                 '13',
                 '14',
                 '15',
                 '16',
                 '17',
                 '18',
                 '19',
                 '20',
                 '21',
                 '22',
                 'X','Y']
    else:
        runbashcmd(f""pyensembl install --reference-name {cfg['genomeassembly']} --release {cfg['genomerelease']} --species {cfg['host']}"")

        import pyensembl
        ensembl = pyensembl.EnsemblRelease(species=pyensembl.species.Species.register(
                                            latin_name=cfg['host'],
                                            synonyms=[cfg['host']],
                                            reference_assemblies={
                                                cfg['genomeassembly']: (cfg['genomerelease'], cfg['genomerelease']),
                                            }),release=cfg['genomerelease'])
        contig_mito=['MTDNA','MITO','MT']
        contigs=[c for c in ensembl.contigs() if ((not '.' in c) and (len(c)<5) and (c not in contig_mito))]    
    if len(contigs)==0:
        contigs=[c for c in ensembl.contigs()]
        # logging.error([c for c in ensembl.contigs()])
        # logging.error('no contigs identified by pyensembl; aborting')
        # sys.exit(0)
    logging.info(f""{len(contigs)} contigs/chromosomes in the genome"")
    logging.info(contigs)
    # raw genome next
    if 'human' in cfg['host'].lower():
        cfg['host']='homo_sapiens'
    if 'yeast' in cfg['host'].lower():
        cfg['host']='saccharomyces_cerevisiae'
    host_=""_"".join(s for s in cfg['host'].split('_')).capitalize()
    if 'GRCh37' in cfg['genomeassembly']:    
        ensembl_fastad=f""pub/grch37/update/fasta/{cfg['host']}/dna/""
    else:
        ensembl_fastad=f""pub/release-{cfg['genomerelease']}/fasta/{cfg['host']}/dna/""        
    genome_fastad=f""{cfg['genomed']}/{ensembl_fastad}""
    cfg['genomep']=f'{genome_fastad}/genome.fa'.format()
    if not exists(cfg['genomep']):
        logging.error(f""not found: {cfg['genomep']}"")
        ifdlref = input(f""Genome file are not there at {genome_fastad}.\n Download?[Y/n]: "")
        if ifdlref=='Y':
        # #FIXME download contigs and cat and get index, sizes
            for contig in contigs:
                fn=f""{cfg['host'].capitalize()}.{cfg['genomeassembly']}.dna_sm.chromosome.{contig}.fa.gz""
                fp=f'{ensembl_fastad}/{fn}'
                if not exists(f""{cfg['genomed']}{fp.replace('.gz','')}""):
                    if not exists(f""{cfg['genomed']}{fp}""):
                        logging.info(f'downloading {fp}')
                        cmd=f""wget -q -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg['genomed']}""
                        try:
                            runbashcmd(cmd,test=cfg['test'])
                        except:
                            fn=f""{cfg['host'].capitalize()}.{cfg['genomeassembly']}.dna_sm.toplevel.fa.gz""
                            fp='{}/{}'.format(ensembl_fastad,fn)                        
                            if not exists(fp):
                                cmd=f""wget -q -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg['genomed']}""
                                # print(cmd)
                                runbashcmd(cmd,test=cfg['test'])
                                break
                #                 break
            # make the fa ready
            if not exists(cfg['genomep']):
                cmd=f""gunzip {genome_fastad}*.fa.gz;cat {genome_fastad}/*.fa > {genome_fastad}/genome.fa;""
                runbashcmd(cmd,test=cfg['test'])
        else:
            logging.error('abort')
            sys.exit(1)
    if not exists(cfg['genomep']+'.bwt'):
        cmd=f""{cfg['bwa']} index {cfg['genomep']}""
        runbashcmd(cmd,test=cfg['test'])
    else:        
        logging.info('bwa index is present')
    if not exists(cfg['genomep']+'.fai'):
        cmd=f""{cfg['samtools']} faidx {cfg['genomep']}""
        runbashcmd(cmd,test=cfg['test'])
    else:
        logging.info('samtools index is present')
    if not exists(cfg['genomep']+'.sizes'):
        cmd=f""cut -f1,2 {cfg['genomep']}.fai > {cfg['genomep']}.sizes""            
        runbashcmd(cmd,test=cfg['test'])
    else:
        logging.info('sizes of contigs are present')

    if 'GRCh37' in cfg['genomeassembly']:    
#     ftp://ftp.ensembl.org/pub/grch37/update/gff3/homo_sapiens/Homo_sapiens.GRCh37.87.gff3.gz
        ensembl_gff3d=f""pub/grch37/update/gff3/{cfg['host']}/""    
    else:
        ensembl_gff3d=f""pub/release-{cfg['genomerelease']}/gff3/{cfg['host']}/""    
    
    genome_gff3d=f""{cfg['genomed']}/{ensembl_gff3d}""
    cfg['genomegffp']=f'{genome_gff3d}/genome.gff3'
    if not exists(cfg['genomegffp']):
        logging.error('not found: {}'.format(cfg['genomegffp']))
        ifdlref = input(""Download genome annotations at {}?[Y/n]: "".format(genome_gff3d))
        if ifdlref=='Y':
        # #FIXME download contigs and cat and get index, sizes
            fn=f""{cfg['host'].capitalize()}.{cfg['genomeassembly']}.{cfg['genomerelease']}.gff3.gz""
            fp=f""{ensembl_gff3d}/{fn}""
            if not exists(fp):
                cmd=f""wget -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg['genomed']}""
                runbashcmd(cmd,test=cfg['test'])
            # move to genome.gff3
                cmd=f""cp {genome_gff3d}/{fn} {cfg['genomegffp']}""
                runbashcmd(cmd,test=cfg['test'])

        else:
            logging.error('abort')
            sys.exit(1)
    logging.info('genomes are installed!')
    return cfg",python,"def get_genomes(cfg):
    """"""
    Installs genomes
    
    :param cfg: configuration dict
    """"""
    if cfg['host']=='homo_sapiens':
        contigs=['1',
                 '2',
                 '3',
                 '4',
                 '5',
                 '6',
                 '7',
                 '8',
                 '9',
                 '10',
                 '11',
                 '12',
                 '13',
                 '14',
                 '15',
                 '16',
                 '17',
                 '18',
                 '19',
                 '20',
                 '21',
                 '22',
                 'X','Y']
    else:
        runbashcmd(f""pyensembl install --reference-name {cfg['genomeassembly']} --release {cfg['genomerelease']} --species {cfg['host']}"")

        import pyensembl
        ensembl = pyensembl.EnsemblRelease(species=pyensembl.species.Species.register(
                                            latin_name=cfg['host'],
                                            synonyms=[cfg['host']],
                                            reference_assemblies={
                                                cfg['genomeassembly']: (cfg['genomerelease'], cfg['genomerelease']),
                                            }),release=cfg['genomerelease'])
        contig_mito=['MTDNA','MITO','MT']
        contigs=[c for c in ensembl.contigs() if ((not '.' in c) and (len(c)<5) and (c not in contig_mito))]    
    if len(contigs)==0:
        contigs=[c for c in ensembl.contigs()]
        # logging.error([c for c in ensembl.contigs()])
        # logging.error('no contigs identified by pyensembl; aborting')
        # sys.exit(0)
    logging.info(f""{len(contigs)} contigs/chromosomes in the genome"")
    logging.info(contigs)
    # raw genome next
    if 'human' in cfg['host'].lower():
        cfg['host']='homo_sapiens'
    if 'yeast' in cfg['host'].lower():
        cfg['host']='saccharomyces_cerevisiae'
    host_=""_"".join(s for s in cfg['host'].split('_')).capitalize()
    if 'GRCh37' in cfg['genomeassembly']:    
        ensembl_fastad=f""pub/grch37/update/fasta/{cfg['host']}/dna/""
    else:
        ensembl_fastad=f""pub/release-{cfg['genomerelease']}/fasta/{cfg['host']}/dna/""        
    genome_fastad=f""{cfg['genomed']}/{ensembl_fastad}""
    cfg['genomep']=f'{genome_fastad}/genome.fa'.format()
    if not exists(cfg['genomep']):
        logging.error(f""not found: {cfg['genomep']}"")
        ifdlref = input(f""Genome file are not there at {genome_fastad}.\n Download?[Y/n]: "")
        if ifdlref=='Y':
        # #FIXME download contigs and cat and get index, sizes
            for contig in contigs:
                fn=f""{cfg['host'].capitalize()}.{cfg['genomeassembly']}.dna_sm.chromosome.{contig}.fa.gz""
                fp=f'{ensembl_fastad}/{fn}'
                if not exists(f""{cfg['genomed']}{fp.replace('.gz','')}""):
                    if not exists(f""{cfg['genomed']}{fp}""):
                        logging.info(f'downloading {fp}')
                        cmd=f""wget -q -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg['genomed']}""
                        try:
                            runbashcmd(cmd,test=cfg['test'])
                        except:
                            fn=f""{cfg['host'].capitalize()}.{cfg['genomeassembly']}.dna_sm.toplevel.fa.gz""
                            fp='{}/{}'.format(ensembl_fastad,fn)                        
                            if not exists(fp):
                                cmd=f""wget -q -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg['genomed']}""
                                # print(cmd)
                                runbashcmd(cmd,test=cfg['test'])
                                break
                #                 break
            # make the fa ready
            if not exists(cfg['genomep']):
                cmd=f""gunzip {genome_fastad}*.fa.gz;cat {genome_fastad}/*.fa > {genome_fastad}/genome.fa;""
                runbashcmd(cmd,test=cfg['test'])
        else:
            logging.error('abort')
            sys.exit(1)
    if not exists(cfg['genomep']+'.bwt'):
        cmd=f""{cfg['bwa']} index {cfg['genomep']}""
        runbashcmd(cmd,test=cfg['test'])
    else:        
        logging.info('bwa index is present')
    if not exists(cfg['genomep']+'.fai'):
        cmd=f""{cfg['samtools']} faidx {cfg['genomep']}""
        runbashcmd(cmd,test=cfg['test'])
    else:
        logging.info('samtools index is present')
    if not exists(cfg['genomep']+'.sizes'):
        cmd=f""cut -f1,2 {cfg['genomep']}.fai > {cfg['genomep']}.sizes""            
        runbashcmd(cmd,test=cfg['test'])
    else:
        logging.info('sizes of contigs are present')

    if 'GRCh37' in cfg['genomeassembly']:    
#     ftp://ftp.ensembl.org/pub/grch37/update/gff3/homo_sapiens/Homo_sapiens.GRCh37.87.gff3.gz
        ensembl_gff3d=f""pub/grch37/update/gff3/{cfg['host']}/""    
    else:
        ensembl_gff3d=f""pub/release-{cfg['genomerelease']}/gff3/{cfg['host']}/""    
    
    genome_gff3d=f""{cfg['genomed']}/{ensembl_gff3d}""
    cfg['genomegffp']=f'{genome_gff3d}/genome.gff3'
    if not exists(cfg['genomegffp']):
        logging.error('not found: {}'.format(cfg['genomegffp']))
        ifdlref = input(""Download genome annotations at {}?[Y/n]: "".format(genome_gff3d))
        if ifdlref=='Y':
        # #FIXME download contigs and cat and get index, sizes
            fn=f""{cfg['host'].capitalize()}.{cfg['genomeassembly']}.{cfg['genomerelease']}.gff3.gz""
            fp=f""{ensembl_gff3d}/{fn}""
            if not exists(fp):
                cmd=f""wget -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg['genomed']}""
                runbashcmd(cmd,test=cfg['test'])
            # move to genome.gff3
                cmd=f""cp {genome_gff3d}/{fn} {cfg['genomegffp']}""
                runbashcmd(cmd,test=cfg['test'])

        else:
            logging.error('abort')
            sys.exit(1)
    logging.info('genomes are installed!')
    return cfg","['def' 'get_genomes' '(' 'cfg' ')' ':' 'if' 'cfg' '[' ""'host'"" ']' '=='
 ""'homo_sapiens'"" ':' 'contigs' '=' '[' ""'1'"" ',' ""'2'"" ',' ""'3'"" ','
 ""'4'"" ',' ""'5'"" ',' ""'6'"" ',' ""'7'"" ',' ""'8'"" ',' ""'9'"" ',' ""'10'"" ','
 ""'11'"" ',' ""'12'"" ',' ""'13'"" ',' ""'14'"" ',' ""'15'"" ',' ""'16'"" ',' ""'17'""
 ',' ""'18'"" ',' ""'19'"" ',' ""'20'"" ',' ""'21'"" ',' ""'22'"" ',' ""'X'"" ','
 ""'Y'"" ']' 'else' ':' 'runbashcmd' '('
 'f""pyensembl install --reference-name {cfg[\'genomeassembly\']} --release {cfg[\'genomerelease\']} --species {cfg[\'host\']}""'
 ')' 'import' 'pyensembl' 'ensembl' '=' 'pyensembl' '.' 'EnsemblRelease'
 '(' 'species' '=' 'pyensembl' '.' 'species' '.' 'Species' '.' 'register'
 '(' 'latin_name' '=' 'cfg' '[' ""'host'"" ']' ',' 'synonyms' '=' '[' 'cfg'
 '[' ""'host'"" ']' ']' ',' 'reference_assemblies' '=' '{' 'cfg' '['
 ""'genomeassembly'"" ']' ':' '(' 'cfg' '[' ""'genomerelease'"" ']' ',' 'cfg'
 '[' ""'genomerelease'"" ']' ')' ',' '}' ')' ',' 'release' '=' 'cfg' '['
 ""'genomerelease'"" ']' ')' 'contig_mito' '=' '[' ""'MTDNA'"" ',' ""'MITO'""
 ',' ""'MT'"" ']' 'contigs' '=' '[' 'c' 'for' 'c' 'in' 'ensembl' '.'
 'contigs' '(' ')' 'if' '(' '(' 'not' ""'.'"" 'in' 'c' ')' 'and' '(' 'len'
 '(' 'c' ')' '<' '5' ')' 'and' '(' 'c' 'not' 'in' 'contig_mito' ')' ')'
 ']' 'if' 'len' '(' 'contigs' ')' '==' '0' ':' 'contigs' '=' '[' 'c' 'for'
 'c' 'in' 'ensembl' '.' 'contigs' '(' ')' ']'
 '# logging.error([c for c in ensembl.contigs()])'
 ""# logging.error('no contigs identified by pyensembl; aborting')""
 '# sys.exit(0)' 'logging' '.' 'info' '('
 'f""{len(contigs)} contigs/chromosomes in the genome""' ')' 'logging' '.'
 'info' '(' 'contigs' ')' '# raw genome next' 'if' ""'human'"" 'in' 'cfg'
 '[' ""'host'"" ']' '.' 'lower' '(' ')' ':' 'cfg' '[' ""'host'"" ']' '='
 ""'homo_sapiens'"" 'if' ""'yeast'"" 'in' 'cfg' '[' ""'host'"" ']' '.' 'lower'
 '(' ')' ':' 'cfg' '[' ""'host'"" ']' '=' ""'saccharomyces_cerevisiae'""
 'host_' '=' '""_""' '.' 'join' '(' 's' 'for' 's' 'in' 'cfg' '[' ""'host'""
 ']' '.' 'split' '(' ""'_'"" ')' ')' '.' 'capitalize' '(' ')' 'if'
 ""'GRCh37'"" 'in' 'cfg' '[' ""'genomeassembly'"" ']' ':' 'ensembl_fastad' '='
 'f""pub/grch37/update/fasta/{cfg[\'host\']}/dna/""' 'else' ':'
 'ensembl_fastad' '='
 'f""pub/release-{cfg[\'genomerelease\']}/fasta/{cfg[\'host\']}/dna/""'
 'genome_fastad' '=' 'f""{cfg[\'genomed\']}/{ensembl_fastad}""' 'cfg' '['
 ""'genomep'"" ']' '=' ""f'{genome_fastad}/genome.fa'"" '.' 'format' '(' ')'
 'if' 'not' 'exists' '(' 'cfg' '[' ""'genomep'"" ']' ')' ':' 'logging' '.'
 'error' '(' 'f""not found: {cfg[\'genomep\']}""' ')' 'ifdlref' '=' 'input'
 '('
 'f""Genome file are not there at {genome_fastad}.\\n Download?[Y/n]: ""'
 ')' 'if' 'ifdlref' '==' ""'Y'"" ':'
 '# #FIXME download contigs and cat and get index, sizes' 'for' 'contig'
 'in' 'contigs' ':' 'fn' '='
 'f""{cfg[\'host\'].capitalize()}.{cfg[\'genomeassembly\']}.dna_sm.chromosome.{contig}.fa.gz""'
 'fp' '=' ""f'{ensembl_fastad}/{fn}'"" 'if' 'not' 'exists' '('
 'f""{cfg[\'genomed\']}{fp.replace(\'.gz\',\'\')}""' ')' ':' 'if' 'not'
 'exists' '(' 'f""{cfg[\'genomed\']}{fp}""' ')' ':' 'logging' '.' 'info' '('
 ""f'downloading {fp}'"" ')' 'cmd' '='
 'f""wget -q -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg[\'genomed\']}""'
 'try' ':' 'runbashcmd' '(' 'cmd' ',' 'test' '=' 'cfg' '[' ""'test'"" ']'
 ')' 'except' ':' 'fn' '='
 'f""{cfg[\'host\'].capitalize()}.{cfg[\'genomeassembly\']}.dna_sm.toplevel.fa.gz""'
 'fp' '=' ""'{}/{}'"" '.' 'format' '(' 'ensembl_fastad' ',' 'fn' ')' 'if'
 'not' 'exists' '(' 'fp' ')' ':' 'cmd' '='
 'f""wget -q -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg[\'genomed\']}""'
 '# print(cmd)' 'runbashcmd' '(' 'cmd' ',' 'test' '=' 'cfg' '[' ""'test'""
 ']' ')' 'break' '#                 break' '# make the fa ready' 'if'
 'not' 'exists' '(' 'cfg' '[' ""'genomep'"" ']' ')' ':' 'cmd' '='
 'f""gunzip {genome_fastad}*.fa.gz;cat {genome_fastad}/*.fa > {genome_fastad}/genome.fa;""'
 'runbashcmd' '(' 'cmd' ',' 'test' '=' 'cfg' '[' ""'test'"" ']' ')' 'else'
 ':' 'logging' '.' 'error' '(' ""'abort'"" ')' 'sys' '.' 'exit' '(' '1' ')'
 'if' 'not' 'exists' '(' 'cfg' '[' ""'genomep'"" ']' '+' ""'.bwt'"" ')' ':'
 'cmd' '=' 'f""{cfg[\'bwa\']} index {cfg[\'genomep\']}""' 'runbashcmd' '('
 'cmd' ',' 'test' '=' 'cfg' '[' ""'test'"" ']' ')' 'else' ':' 'logging' '.'
 'info' '(' ""'bwa index is present'"" ')' 'if' 'not' 'exists' '(' 'cfg' '['
 ""'genomep'"" ']' '+' ""'.fai'"" ')' ':' 'cmd' '='
 'f""{cfg[\'samtools\']} faidx {cfg[\'genomep\']}""' 'runbashcmd' '(' 'cmd'
 ',' 'test' '=' 'cfg' '[' ""'test'"" ']' ')' 'else' ':' 'logging' '.' 'info'
 '(' ""'samtools index is present'"" ')' 'if' 'not' 'exists' '(' 'cfg' '['
 ""'genomep'"" ']' '+' ""'.sizes'"" ')' ':' 'cmd' '='
 'f""cut -f1,2 {cfg[\'genomep\']}.fai > {cfg[\'genomep\']}.sizes""'
 'runbashcmd' '(' 'cmd' ',' 'test' '=' 'cfg' '[' ""'test'"" ']' ')' 'else'
 ':' 'logging' '.' 'info' '(' ""'sizes of contigs are present'"" ')' 'if'
 ""'GRCh37'"" 'in' 'cfg' '[' ""'genomeassembly'"" ']' ':'
 '#     ftp://ftp.ensembl.org/pub/grch37/update/gff3/homo_sapiens/Homo_sapiens.GRCh37.87.gff3.gz'
 'ensembl_gff3d' '=' 'f""pub/grch37/update/gff3/{cfg[\'host\']}/""' 'else'
 ':' 'ensembl_gff3d' '='
 'f""pub/release-{cfg[\'genomerelease\']}/gff3/{cfg[\'host\']}/""'
 'genome_gff3d' '=' 'f""{cfg[\'genomed\']}/{ensembl_gff3d}""' 'cfg' '['
 ""'genomegffp'"" ']' '=' ""f'{genome_gff3d}/genome.gff3'"" 'if' 'not'
 'exists' '(' 'cfg' '[' ""'genomegffp'"" ']' ')' ':' 'logging' '.' 'error'
 '(' ""'not found: {}'"" '.' 'format' '(' 'cfg' '[' ""'genomegffp'"" ']' ')'
 ')' 'ifdlref' '=' 'input' '('
 '""Download genome annotations at {}?[Y/n]: ""' '.' 'format' '('
 'genome_gff3d' ')' ')' 'if' 'ifdlref' '==' ""'Y'"" ':'
 '# #FIXME download contigs and cat and get index, sizes' 'fn' '='
 'f""{cfg[\'host\'].capitalize()}.{cfg[\'genomeassembly\']}.{cfg[\'genomerelease\']}.gff3.gz""'
 'fp' '=' 'f""{ensembl_gff3d}/{fn}""' 'if' 'not' 'exists' '(' 'fp' ')' ':'
 'cmd' '='
 'f""wget -x -nH ftp://ftp.ensembl.org/{fp} -P {cfg[\'genomed\']}""'
 'runbashcmd' '(' 'cmd' ',' 'test' '=' 'cfg' '[' ""'test'"" ']' ')'
 '# move to genome.gff3' 'cmd' '='
 'f""cp {genome_gff3d}/{fn} {cfg[\'genomegffp\']}""' 'runbashcmd' '(' 'cmd'
 ',' 'test' '=' 'cfg' '[' ""'test'"" ']' ')' 'else' ':' 'logging' '.'
 'error' '(' ""'abort'"" ')' 'sys' '.' 'exit' '(' '1' ')' 'logging' '.'
 'info' '(' ""'genomes are installed!'"" ')' 'return' 'cfg']","Installs genomes
    
    :param cfg: configuration dict",['Installs' 'genomes' ':' 'param' 'cfg' ':' 'configuration' 'dict'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/__init__.py#L28-L161
rraadd88/rohan,rohan/dandage/io_dfs.py,delunnamedcol,"def delunnamedcol(df):
    """"""
    Deletes all the unnamed columns

    :param df: pandas dataframe
    """"""
    cols_del=[c for c in df.columns if 'Unnamed' in c]
    return df.drop(cols_del,axis=1)",python,"def delunnamedcol(df):
    """"""
    Deletes all the unnamed columns

    :param df: pandas dataframe
    """"""
    cols_del=[c for c in df.columns if 'Unnamed' in c]
    return df.drop(cols_del,axis=1)","['def' 'delunnamedcol' '(' 'df' ')' ':' 'cols_del' '=' '[' 'c' 'for' 'c'
 'in' 'df' '.' 'columns' 'if' ""'Unnamed'"" 'in' 'c' ']' 'return' 'df' '.'
 'drop' '(' 'cols_del' ',' 'axis' '=' '1' ')']","Deletes all the unnamed columns

    :param df: pandas dataframe",['Deletes' 'all' 'the' 'unnamed' 'columns'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L43-L50
rraadd88/rohan,rohan/dandage/io_dfs.py,concat_cols,"def concat_cols(df1,df2,idx_col,df1_cols,df2_cols,
                df1_suffix,df2_suffix,wc_cols=[],suffix_all=False):
    """"""
    Concatenates two pandas tables 

    :param df1: dataframe 1
    :param df2: dataframe 2
    :param idx_col: column name which will be used as a common index 
    """"""

    df1=df1.set_index(idx_col)
    df2=df2.set_index(idx_col)    
    if not len(wc_cols)==0:
        for wc in wc_cols:
            df1_cols=df1_cols+[c for c in df1.columns if wc in c]
            df2_cols=df2_cols+[c for c in df2.columns if wc in c]
    combo=pd.concat([df1.loc[:,df1_cols],df2.loc[:,df2_cols]],axis=1)
    # find common columns and rename them
    # print df1_cols
    # print df2_cols    
    if suffix_all:
        df1_cols=[""%s%s"" % (c,df1_suffix) for c in df1_cols]
        df2_cols=[""%s%s"" % (c,df2_suffix) for c in df2_cols]
        # df1_cols[df1_cols.index(col)]=""%s%s"" % (col,df1_suffix)
        # df2_cols[df2_cols.index(col)]=""%s%s"" % (col,df2_suffix)
    else:
        common_cols=[col for col in df1_cols if col in df2_cols]
        for col in common_cols:
            df1_cols[df1_cols.index(col)]=""%s%s"" % (col,df1_suffix)
            df2_cols[df2_cols.index(col)]=""%s%s"" % (col,df2_suffix)
    combo.columns=df1_cols+df2_cols
    combo.index.name=idx_col
    return combo",python,"def concat_cols(df1,df2,idx_col,df1_cols,df2_cols,
                df1_suffix,df2_suffix,wc_cols=[],suffix_all=False):
    """"""
    Concatenates two pandas tables 

    :param df1: dataframe 1
    :param df2: dataframe 2
    :param idx_col: column name which will be used as a common index 
    """"""

    df1=df1.set_index(idx_col)
    df2=df2.set_index(idx_col)    
    if not len(wc_cols)==0:
        for wc in wc_cols:
            df1_cols=df1_cols+[c for c in df1.columns if wc in c]
            df2_cols=df2_cols+[c for c in df2.columns if wc in c]
    combo=pd.concat([df1.loc[:,df1_cols],df2.loc[:,df2_cols]],axis=1)
    # find common columns and rename them
    # print df1_cols
    # print df2_cols    
    if suffix_all:
        df1_cols=[""%s%s"" % (c,df1_suffix) for c in df1_cols]
        df2_cols=[""%s%s"" % (c,df2_suffix) for c in df2_cols]
        # df1_cols[df1_cols.index(col)]=""%s%s"" % (col,df1_suffix)
        # df2_cols[df2_cols.index(col)]=""%s%s"" % (col,df2_suffix)
    else:
        common_cols=[col for col in df1_cols if col in df2_cols]
        for col in common_cols:
            df1_cols[df1_cols.index(col)]=""%s%s"" % (col,df1_suffix)
            df2_cols[df2_cols.index(col)]=""%s%s"" % (col,df2_suffix)
    combo.columns=df1_cols+df2_cols
    combo.index.name=idx_col
    return combo","['def' 'concat_cols' '(' 'df1' ',' 'df2' ',' 'idx_col' ',' 'df1_cols' ','
 'df2_cols' ',' 'df1_suffix' ',' 'df2_suffix' ',' 'wc_cols' '=' '[' ']'
 ',' 'suffix_all' '=' 'False' ')' ':' 'df1' '=' 'df1' '.' 'set_index' '('
 'idx_col' ')' 'df2' '=' 'df2' '.' 'set_index' '(' 'idx_col' ')' 'if'
 'not' 'len' '(' 'wc_cols' ')' '==' '0' ':' 'for' 'wc' 'in' 'wc_cols' ':'
 'df1_cols' '=' 'df1_cols' '+' '[' 'c' 'for' 'c' 'in' 'df1' '.' 'columns'
 'if' 'wc' 'in' 'c' ']' 'df2_cols' '=' 'df2_cols' '+' '[' 'c' 'for' 'c'
 'in' 'df2' '.' 'columns' 'if' 'wc' 'in' 'c' ']' 'combo' '=' 'pd' '.'
 'concat' '(' '[' 'df1' '.' 'loc' '[' ':' ',' 'df1_cols' ']' ',' 'df2' '.'
 'loc' '[' ':' ',' 'df2_cols' ']' ']' ',' 'axis' '=' '1' ')'
 '# find common columns and rename them' '# print df1_cols'
 '# print df2_cols    ' 'if' 'suffix_all' ':' 'df1_cols' '=' '[' '""%s%s""'
 '%' '(' 'c' ',' 'df1_suffix' ')' 'for' 'c' 'in' 'df1_cols' ']' 'df2_cols'
 '=' '[' '""%s%s""' '%' '(' 'c' ',' 'df2_suffix' ')' 'for' 'c' 'in'
 'df2_cols' ']'
 '# df1_cols[df1_cols.index(col)]=""%s%s"" % (col,df1_suffix)'
 '# df2_cols[df2_cols.index(col)]=""%s%s"" % (col,df2_suffix)' 'else' ':'
 'common_cols' '=' '[' 'col' 'for' 'col' 'in' 'df1_cols' 'if' 'col' 'in'
 'df2_cols' ']' 'for' 'col' 'in' 'common_cols' ':' 'df1_cols' '['
 'df1_cols' '.' 'index' '(' 'col' ')' ']' '=' '""%s%s""' '%' '(' 'col' ','
 'df1_suffix' ')' 'df2_cols' '[' 'df2_cols' '.' 'index' '(' 'col' ')' ']'
 '=' '""%s%s""' '%' '(' 'col' ',' 'df2_suffix' ')' 'combo' '.' 'columns' '='
 'df1_cols' '+' 'df2_cols' 'combo' '.' 'index' '.' 'name' '=' 'idx_col'
 'return' 'combo']","Concatenates two pandas tables 

    :param df1: dataframe 1
    :param df2: dataframe 2
    :param idx_col: column name which will be used as a common index",['Concatenates' 'two' 'pandas' 'tables'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L174-L206
rraadd88/rohan,rohan/dandage/io_dfs.py,get_colmin,"def get_colmin(data):
    """"""
    Get rowwise column names with minimum values

    :param data: pandas dataframe
    """"""
    data=data.T
    colmins=[]
    for col in data:
        colmins.append(data[col].idxmin())
    return colmins",python,"def get_colmin(data):
    """"""
    Get rowwise column names with minimum values

    :param data: pandas dataframe
    """"""
    data=data.T
    colmins=[]
    for col in data:
        colmins.append(data[col].idxmin())
    return colmins","['def' 'get_colmin' '(' 'data' ')' ':' 'data' '=' 'data' '.' 'T' 'colmins'
 '=' '[' ']' 'for' 'col' 'in' 'data' ':' 'colmins' '.' 'append' '(' 'data'
 '[' 'col' ']' '.' 'idxmin' '(' ')' ')' 'return' 'colmins']","Get rowwise column names with minimum values

    :param data: pandas dataframe",['Get' 'rowwise' 'column' 'names' 'with' 'minimum' 'values'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L208-L218
rraadd88/rohan,rohan/dandage/io_dfs.py,fhs2data_combo_appended,"def fhs2data_combo_appended(fhs, cols=None,labels=None,labels_coln='labels',sep=',',
                           error_bad_lines=True):
    """"""
    to be deprecated
    Collates data from multiple csv files vertically

    :param fhs: list of paths to csv files
    :param cols: list of column names to concatenate
    """"""    
    if labels is None:
        labels=[basename(fh) for fh in fhs]
    if len(fhs)>0:
        data_all=pd.DataFrame(columns=cols)
        for fhi,fh in enumerate(fhs):
            label=labels[fhi]
            try:
                data=pd.read_csv(fh,sep=sep,error_bad_lines=error_bad_lines)
            except:
                raise ValueError(f""something wrong with file pd.read_csv({fh},sep={sep})"")
            if len(data)!=0:
                data.loc[:,labels_coln]=label
                if not cols is None:
                    data=data.loc[:,cols]
                data_all=data_all.append(data,sort=True)
        return del_Unnamed(data_all)",python,"def fhs2data_combo_appended(fhs, cols=None,labels=None,labels_coln='labels',sep=',',
                           error_bad_lines=True):
    """"""
    to be deprecated
    Collates data from multiple csv files vertically

    :param fhs: list of paths to csv files
    :param cols: list of column names to concatenate
    """"""    
    if labels is None:
        labels=[basename(fh) for fh in fhs]
    if len(fhs)>0:
        data_all=pd.DataFrame(columns=cols)
        for fhi,fh in enumerate(fhs):
            label=labels[fhi]
            try:
                data=pd.read_csv(fh,sep=sep,error_bad_lines=error_bad_lines)
            except:
                raise ValueError(f""something wrong with file pd.read_csv({fh},sep={sep})"")
            if len(data)!=0:
                data.loc[:,labels_coln]=label
                if not cols is None:
                    data=data.loc[:,cols]
                data_all=data_all.append(data,sort=True)
        return del_Unnamed(data_all)","['def' 'fhs2data_combo_appended' '(' 'fhs' ',' 'cols' '=' 'None' ','
 'labels' '=' 'None' ',' 'labels_coln' '=' ""'labels'"" ',' 'sep' '=' ""','""
 ',' 'error_bad_lines' '=' 'True' ')' ':' 'if' 'labels' 'is' 'None' ':'
 'labels' '=' '[' 'basename' '(' 'fh' ')' 'for' 'fh' 'in' 'fhs' ']' 'if'
 'len' '(' 'fhs' ')' '>' '0' ':' 'data_all' '=' 'pd' '.' 'DataFrame' '('
 'columns' '=' 'cols' ')' 'for' 'fhi' ',' 'fh' 'in' 'enumerate' '(' 'fhs'
 ')' ':' 'label' '=' 'labels' '[' 'fhi' ']' 'try' ':' 'data' '=' 'pd' '.'
 'read_csv' '(' 'fh' ',' 'sep' '=' 'sep' ',' 'error_bad_lines' '='
 'error_bad_lines' ')' 'except' ':' 'raise' 'ValueError' '('
 'f""something wrong with file pd.read_csv({fh},sep={sep})""' ')' 'if' 'len'
 '(' 'data' ')' '!=' '0' ':' 'data' '.' 'loc' '[' ':' ',' 'labels_coln'
 ']' '=' 'label' 'if' 'not' 'cols' 'is' 'None' ':' 'data' '=' 'data' '.'
 'loc' '[' ':' ',' 'cols' ']' 'data_all' '=' 'data_all' '.' 'append' '('
 'data' ',' 'sort' '=' 'True' ')' 'return' 'del_Unnamed' '(' 'data_all'
 ')']","to be deprecated
    Collates data from multiple csv files vertically

    :param fhs: list of paths to csv files
    :param cols: list of column names to concatenate","['to' 'be' 'deprecated' 'Collates' 'data' 'from' 'multiple' 'csv' 'files'
 'vertically']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L248-L272
rraadd88/rohan,rohan/dandage/io_dfs.py,rename_cols,"def rename_cols(df,names,renames=None,prefix=None,suffix=None):
    """"""
    rename columns of a pandas table

    :param df: pandas dataframe
    :param names: list of new column names
    """"""
    if not prefix is None:
        renames=[ ""%s%s"" % (prefix,s) for s in names]
    if not suffix is None:    
        renames=[ ""%s%s"" % (s,suffix) for s in names]
    if not renames is None:
        for i,name in enumerate(names):
#             names=[renames[i] if s==names[i] else s for s in names]    
            rename=renames[i]    
            df.loc[:,rename]=df.loc[:,name]
        df=df.drop(names,axis=1)
        return df",python,"def rename_cols(df,names,renames=None,prefix=None,suffix=None):
    """"""
    rename columns of a pandas table

    :param df: pandas dataframe
    :param names: list of new column names
    """"""
    if not prefix is None:
        renames=[ ""%s%s"" % (prefix,s) for s in names]
    if not suffix is None:    
        renames=[ ""%s%s"" % (s,suffix) for s in names]
    if not renames is None:
        for i,name in enumerate(names):
#             names=[renames[i] if s==names[i] else s for s in names]    
            rename=renames[i]    
            df.loc[:,rename]=df.loc[:,name]
        df=df.drop(names,axis=1)
        return df","['def' 'rename_cols' '(' 'df' ',' 'names' ',' 'renames' '=' 'None' ','
 'prefix' '=' 'None' ',' 'suffix' '=' 'None' ')' ':' 'if' 'not' 'prefix'
 'is' 'None' ':' 'renames' '=' '[' '""%s%s""' '%' '(' 'prefix' ',' 's' ')'
 'for' 's' 'in' 'names' ']' 'if' 'not' 'suffix' 'is' 'None' ':' 'renames'
 '=' '[' '""%s%s""' '%' '(' 's' ',' 'suffix' ')' 'for' 's' 'in' 'names' ']'
 'if' 'not' 'renames' 'is' 'None' ':' 'for' 'i' ',' 'name' 'in'
 'enumerate' '(' 'names' ')' ':'
 '#             names=[renames[i] if s==names[i] else s for s in names]    '
 'rename' '=' 'renames' '[' 'i' ']' 'df' '.' 'loc' '[' ':' ',' 'rename'
 ']' '=' 'df' '.' 'loc' '[' ':' ',' 'name' ']' 'df' '=' 'df' '.' 'drop'
 '(' 'names' ',' 'axis' '=' '1' ')' 'return' 'df']","rename columns of a pandas table

    :param df: pandas dataframe
    :param names: list of new column names",['rename' 'columns' 'of' 'a' 'pandas' 'table'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L275-L292
rraadd88/rohan,rohan/dandage/io_dfs.py,reorderbydf,"def reorderbydf(df2,df1):
    """"""
    Reorder rows of a dataframe by other dataframe

    :param df2: input dataframe
    :param df1: template dataframe 
    """"""
    df3=pd.DataFrame()
    for idx,row in df1.iterrows():
        df3=df3.append(df2.loc[idx,:])
    return df3",python,"def reorderbydf(df2,df1):
    """"""
    Reorder rows of a dataframe by other dataframe

    :param df2: input dataframe
    :param df1: template dataframe 
    """"""
    df3=pd.DataFrame()
    for idx,row in df1.iterrows():
        df3=df3.append(df2.loc[idx,:])
    return df3","['def' 'reorderbydf' '(' 'df2' ',' 'df1' ')' ':' 'df3' '=' 'pd' '.'
 'DataFrame' '(' ')' 'for' 'idx' ',' 'row' 'in' 'df1' '.' 'iterrows' '('
 ')' ':' 'df3' '=' 'df3' '.' 'append' '(' 'df2' '.' 'loc' '[' 'idx' ','
 ':' ']' ')' 'return' 'df3']","Reorder rows of a dataframe by other dataframe

    :param df2: input dataframe
    :param df1: template dataframe",['Reorder' 'rows' 'of' 'a' 'dataframe' 'by' 'other' 'dataframe'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L295-L305
rraadd88/rohan,rohan/dandage/io_dfs.py,dmap2lin,"def dmap2lin(df,idxn='index',coln='column',colvalue_name='value'):
    """"""
    dmap: ids in index and columns 
    idxn,coln of dmap -> 1st and 2nd col
    """"""
#     df.columns.name=coln
    df.index.name=idxn
    return df.reset_index().melt(id_vars=[df.index.name],
                             var_name=coln,value_name=colvalue_name)",python,"def dmap2lin(df,idxn='index',coln='column',colvalue_name='value'):
    """"""
    dmap: ids in index and columns 
    idxn,coln of dmap -> 1st and 2nd col
    """"""
#     df.columns.name=coln
    df.index.name=idxn
    return df.reset_index().melt(id_vars=[df.index.name],
                             var_name=coln,value_name=colvalue_name)","['def' 'dmap2lin' '(' 'df' ',' 'idxn' '=' ""'index'"" ',' 'coln' '='
 ""'column'"" ',' 'colvalue_name' '=' ""'value'"" ')' ':'
 '#     df.columns.name=coln' 'df' '.' 'index' '.' 'name' '=' 'idxn'
 'return' 'df' '.' 'reset_index' '(' ')' '.' 'melt' '(' 'id_vars' '=' '['
 'df' '.' 'index' '.' 'name' ']' ',' 'var_name' '=' 'coln' ','
 'value_name' '=' 'colvalue_name' ')']","dmap: ids in index and columns 
    idxn,coln of dmap -> 1st and 2nd col","['dmap' ':' 'ids' 'in' 'index' 'and' 'columns' 'idxn' 'coln' 'of' 'dmap'
 '-' '>' '1st' 'and' '2nd' 'col']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L314-L322
rraadd88/rohan,rohan/dandage/io_dfs.py,df2unstack,"def df2unstack(df,coln='columns',idxn='index',col='value'):
    """"""
    will be deprecated
    """"""
    return dmap2lin(df,idxn=idxn,coln=coln,colvalue_name=col)",python,"def df2unstack(df,coln='columns',idxn='index',col='value'):
    """"""
    will be deprecated
    """"""
    return dmap2lin(df,idxn=idxn,coln=coln,colvalue_name=col)","['def' 'df2unstack' '(' 'df' ',' 'coln' '=' ""'columns'"" ',' 'idxn' '='
 ""'index'"" ',' 'col' '=' ""'value'"" ')' ':' 'return' 'dmap2lin' '(' 'df'
 ',' 'idxn' '=' 'idxn' ',' 'coln' '=' 'coln' ',' 'colvalue_name' '=' 'col'
 ')']",will be deprecated,['will' 'be' 'deprecated'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L325-L329
rraadd88/rohan,rohan/dandage/io_dfs.py,df2chucks,"def df2chucks(din,chunksize,outd,fn,return_fmt='\t',force=False):
    """"""
    :param return_fmt: '\t': tab-sep file, lly, '.', 'list': returns a list
    """"""
    from os.path import exists#,splitext,dirname,splitext,basename,realpath
    from os import makedirs
    din.index=range(0,len(din),1)

    chunkrange=list(np.arange(0,len(din),chunksize))
    chunkrange=list(zip([c+1 if ci!=0 else 0 for ci,c in enumerate(chunkrange)],chunkrange[1:]+[len(din)-1]))

    chunk2range={}
    for ri,r in enumerate(chunkrange):    
        chunk2range[ri+1]=r

    if not exists(outd):
        makedirs(outd)
    chunks=[]
    chunkps=[]
    for chunk in chunk2range:
        chunkp='{}/{}_chunk{:08d}.tsv'.format(outd,fn,chunk)
        rnge=chunk2range[chunk]
        din_=din.loc[rnge[0]:rnge[1],:]
        if not exists(chunkp) or force:
            if return_fmt=='list':
                chunks.append(din_)
            else:
                din_.to_csv(chunkp,sep=return_fmt)
            del din_
        chunkps.append(chunkp)
    if return_fmt=='list':
        return chunks
    else:
        return chunkps",python,"def df2chucks(din,chunksize,outd,fn,return_fmt='\t',force=False):
    """"""
    :param return_fmt: '\t': tab-sep file, lly, '.', 'list': returns a list
    """"""
    from os.path import exists#,splitext,dirname,splitext,basename,realpath
    from os import makedirs
    din.index=range(0,len(din),1)

    chunkrange=list(np.arange(0,len(din),chunksize))
    chunkrange=list(zip([c+1 if ci!=0 else 0 for ci,c in enumerate(chunkrange)],chunkrange[1:]+[len(din)-1]))

    chunk2range={}
    for ri,r in enumerate(chunkrange):    
        chunk2range[ri+1]=r

    if not exists(outd):
        makedirs(outd)
    chunks=[]
    chunkps=[]
    for chunk in chunk2range:
        chunkp='{}/{}_chunk{:08d}.tsv'.format(outd,fn,chunk)
        rnge=chunk2range[chunk]
        din_=din.loc[rnge[0]:rnge[1],:]
        if not exists(chunkp) or force:
            if return_fmt=='list':
                chunks.append(din_)
            else:
                din_.to_csv(chunkp,sep=return_fmt)
            del din_
        chunkps.append(chunkp)
    if return_fmt=='list':
        return chunks
    else:
        return chunkps","['def' 'df2chucks' '(' 'din' ',' 'chunksize' ',' 'outd' ',' 'fn' ','
 'return_fmt' '=' ""'\\t'"" ',' 'force' '=' 'False' ')' ':' 'from' 'os' '.'
 'path' 'import' 'exists' '#,splitext,dirname,splitext,basename,realpath'
 'from' 'os' 'import' 'makedirs' 'din' '.' 'index' '=' 'range' '(' '0' ','
 'len' '(' 'din' ')' ',' '1' ')' 'chunkrange' '=' 'list' '(' 'np' '.'
 'arange' '(' '0' ',' 'len' '(' 'din' ')' ',' 'chunksize' ')' ')'
 'chunkrange' '=' 'list' '(' 'zip' '(' '[' 'c' '+' '1' 'if' 'ci' '!=' '0'
 'else' '0' 'for' 'ci' ',' 'c' 'in' 'enumerate' '(' 'chunkrange' ')' ']'
 ',' 'chunkrange' '[' '1' ':' ']' '+' '[' 'len' '(' 'din' ')' '-' '1' ']'
 ')' ')' 'chunk2range' '=' '{' '}' 'for' 'ri' ',' 'r' 'in' 'enumerate' '('
 'chunkrange' ')' ':' 'chunk2range' '[' 'ri' '+' '1' ']' '=' 'r' 'if'
 'not' 'exists' '(' 'outd' ')' ':' 'makedirs' '(' 'outd' ')' 'chunks' '='
 '[' ']' 'chunkps' '=' '[' ']' 'for' 'chunk' 'in' 'chunk2range' ':'
 'chunkp' '=' ""'{}/{}_chunk{:08d}.tsv'"" '.' 'format' '(' 'outd' ',' 'fn'
 ',' 'chunk' ')' 'rnge' '=' 'chunk2range' '[' 'chunk' ']' 'din_' '=' 'din'
 '.' 'loc' '[' 'rnge' '[' '0' ']' ':' 'rnge' '[' '1' ']' ',' ':' ']' 'if'
 'not' 'exists' '(' 'chunkp' ')' 'or' 'force' ':' 'if' 'return_fmt' '=='
 ""'list'"" ':' 'chunks' '.' 'append' '(' 'din_' ')' 'else' ':' 'din_' '.'
 'to_csv' '(' 'chunkp' ',' 'sep' '=' 'return_fmt' ')' 'del' 'din_'
 'chunkps' '.' 'append' '(' 'chunkp' ')' 'if' 'return_fmt' '==' ""'list'""
 ':' 'return' 'chunks' 'else' ':' 'return' 'chunkps']",":param return_fmt: '\t': tab-sep file, lly, '.', 'list': returns a list","[':' 'param' 'return_fmt' ':' '\\' 't' ':' 'tab' '-' 'sep' 'file' 'lly'
 '.' 'list' ':' 'returns' 'a' 'list']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L366-L399
rraadd88/rohan,rohan/dandage/io_dfs.py,dflin2dfbinarymap,"def dflin2dfbinarymap(dflin,col1,col2,params_df2submap={'aggfunc':'sum','binary':True,'binaryby':'nan'},test=False):
    """"""
    if not binary:
        dropna the df by value [col index and value] column
    """"""
    # get the submap ready
    df_map=df2submap(df=dflin,
              col=col1,idx=col2,**params_df2submap)
    if test:           
        logging.debug(df_map.unstack().unique())
    # make columns and index symmetric
    df_map_symm=dfmap2symmcolidx(df_map)
    df_map_symm=df_map_symm.fillna(False)
    if test:           
        logging.debug(df_map_symm.unstack().unique())
    df_map_symm=(df_map_symm+df_map_symm.T)/2
    if test:           
        logging.debug(df_map_symm.unstack().unique())
    df_map_symm=df_map_symm!=0
    if test:           
        logging.debug(df_map_symm.unstack().unique())
    return df_map_symm",python,"def dflin2dfbinarymap(dflin,col1,col2,params_df2submap={'aggfunc':'sum','binary':True,'binaryby':'nan'},test=False):
    """"""
    if not binary:
        dropna the df by value [col index and value] column
    """"""
    # get the submap ready
    df_map=df2submap(df=dflin,
              col=col1,idx=col2,**params_df2submap)
    if test:           
        logging.debug(df_map.unstack().unique())
    # make columns and index symmetric
    df_map_symm=dfmap2symmcolidx(df_map)
    df_map_symm=df_map_symm.fillna(False)
    if test:           
        logging.debug(df_map_symm.unstack().unique())
    df_map_symm=(df_map_symm+df_map_symm.T)/2
    if test:           
        logging.debug(df_map_symm.unstack().unique())
    df_map_symm=df_map_symm!=0
    if test:           
        logging.debug(df_map_symm.unstack().unique())
    return df_map_symm","['def' 'dflin2dfbinarymap' '(' 'dflin' ',' 'col1' ',' 'col2' ','
 'params_df2submap' '=' '{' ""'aggfunc'"" ':' ""'sum'"" ',' ""'binary'"" ':'
 'True' ',' ""'binaryby'"" ':' ""'nan'"" '}' ',' 'test' '=' 'False' ')' ':'
 '# get the submap ready' 'df_map' '=' 'df2submap' '(' 'df' '=' 'dflin'
 ',' 'col' '=' 'col1' ',' 'idx' '=' 'col2' ',' '*' '*' 'params_df2submap'
 ')' 'if' 'test' ':' 'logging' '.' 'debug' '(' 'df_map' '.' 'unstack' '('
 ')' '.' 'unique' '(' ')' ')' '# make columns and index symmetric'
 'df_map_symm' '=' 'dfmap2symmcolidx' '(' 'df_map' ')' 'df_map_symm' '='
 'df_map_symm' '.' 'fillna' '(' 'False' ')' 'if' 'test' ':' 'logging' '.'
 'debug' '(' 'df_map_symm' '.' 'unstack' '(' ')' '.' 'unique' '(' ')' ')'
 'df_map_symm' '=' '(' 'df_map_symm' '+' 'df_map_symm' '.' 'T' ')' '/' '2'
 'if' 'test' ':' 'logging' '.' 'debug' '(' 'df_map_symm' '.' 'unstack' '('
 ')' '.' 'unique' '(' ')' ')' 'df_map_symm' '=' 'df_map_symm' '!=' '0'
 'if' 'test' ':' 'logging' '.' 'debug' '(' 'df_map_symm' '.' 'unstack' '('
 ')' '.' 'unique' '(' ')' ')' 'return' 'df_map_symm']","if not binary:
        dropna the df by value [col index and value] column","['if' 'not' 'binary' ':' 'dropna' 'the' 'df' 'by' 'value' '[' 'col'
 'index' 'and' 'value' ']' 'column']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L411-L432
rraadd88/rohan,rohan/dandage/io_dfs.py,get_offdiag_vals,"def get_offdiag_vals(dcorr):
    """"""
    for lin dcorr i guess
    """"""
    del_indexes=[]
    for spc1 in np.unique(dcorr.index.get_level_values(0)):
        for spc2 in np.unique(dcorr.index.get_level_values(0)):
            if (not (spc1,spc2) in del_indexes) and (not (spc2,spc1) in del_indexes):
                del_indexes.append((spc1,spc2))
    #         break
    for spc1 in np.unique(dcorr.index.get_level_values(0)):
        for spc2 in np.unique(dcorr.index.get_level_values(0)):
            if spc1==spc2:
                del_indexes.append((spc1,spc2))
    
    return dcorr.drop(del_indexes)",python,"def get_offdiag_vals(dcorr):
    """"""
    for lin dcorr i guess
    """"""
    del_indexes=[]
    for spc1 in np.unique(dcorr.index.get_level_values(0)):
        for spc2 in np.unique(dcorr.index.get_level_values(0)):
            if (not (spc1,spc2) in del_indexes) and (not (spc2,spc1) in del_indexes):
                del_indexes.append((spc1,spc2))
    #         break
    for spc1 in np.unique(dcorr.index.get_level_values(0)):
        for spc2 in np.unique(dcorr.index.get_level_values(0)):
            if spc1==spc2:
                del_indexes.append((spc1,spc2))
    
    return dcorr.drop(del_indexes)","['def' 'get_offdiag_vals' '(' 'dcorr' ')' ':' 'del_indexes' '=' '[' ']'
 'for' 'spc1' 'in' 'np' '.' 'unique' '(' 'dcorr' '.' 'index' '.'
 'get_level_values' '(' '0' ')' ')' ':' 'for' 'spc2' 'in' 'np' '.'
 'unique' '(' 'dcorr' '.' 'index' '.' 'get_level_values' '(' '0' ')' ')'
 ':' 'if' '(' 'not' '(' 'spc1' ',' 'spc2' ')' 'in' 'del_indexes' ')' 'and'
 '(' 'not' '(' 'spc2' ',' 'spc1' ')' 'in' 'del_indexes' ')' ':'
 'del_indexes' '.' 'append' '(' '(' 'spc1' ',' 'spc2' ')' ')'
 '#         break' 'for' 'spc1' 'in' 'np' '.' 'unique' '(' 'dcorr' '.'
 'index' '.' 'get_level_values' '(' '0' ')' ')' ':' 'for' 'spc2' 'in' 'np'
 '.' 'unique' '(' 'dcorr' '.' 'index' '.' 'get_level_values' '(' '0' ')'
 ')' ':' 'if' 'spc1' '==' 'spc2' ':' 'del_indexes' '.' 'append' '(' '('
 'spc1' ',' 'spc2' ')' ')' 'return' 'dcorr' '.' 'drop' '(' 'del_indexes'
 ')']",for lin dcorr i guess,['for' 'lin' 'dcorr' 'i' 'guess'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L490-L505
rraadd88/rohan,rohan/dandage/io_dfs.py,dfsyn2appended,"def dfsyn2appended(df,colsyn,colsynfmt=None,colsynstrsep=';'):
    """"""
    for merging dfs with names with df with synonymns
    param colsyn: col containing tuples of synonymns 
    """"""
    colsynappended=colsyn+' appended'
    df.index=range(len(df))
    if colsynfmt=='str':
        df.loc[:,colsyn]=df.loc[:,colsyn].apply(lambda x : x.split(colsynstrsep))
    #make duplicated row for each synonymn
    dfsynappended=df[colsyn].apply(pd.Series).unstack().reset_index().drop('level_0',axis=1).set_index('level_1')
    dfsynappended.columns=[colsynappended]
    dfsynappended=dfsynappended.dropna()
    return dfsynappended.join(df,how='left')",python,"def dfsyn2appended(df,colsyn,colsynfmt=None,colsynstrsep=';'):
    """"""
    for merging dfs with names with df with synonymns
    param colsyn: col containing tuples of synonymns 
    """"""
    colsynappended=colsyn+' appended'
    df.index=range(len(df))
    if colsynfmt=='str':
        df.loc[:,colsyn]=df.loc[:,colsyn].apply(lambda x : x.split(colsynstrsep))
    #make duplicated row for each synonymn
    dfsynappended=df[colsyn].apply(pd.Series).unstack().reset_index().drop('level_0',axis=1).set_index('level_1')
    dfsynappended.columns=[colsynappended]
    dfsynappended=dfsynappended.dropna()
    return dfsynappended.join(df,how='left')","['def' 'dfsyn2appended' '(' 'df' ',' 'colsyn' ',' 'colsynfmt' '=' 'None'
 ',' 'colsynstrsep' '=' ""';'"" ')' ':' 'colsynappended' '=' 'colsyn' '+'
 ""' appended'"" 'df' '.' 'index' '=' 'range' '(' 'len' '(' 'df' ')' ')'
 'if' 'colsynfmt' '==' ""'str'"" ':' 'df' '.' 'loc' '[' ':' ',' 'colsyn' ']'
 '=' 'df' '.' 'loc' '[' ':' ',' 'colsyn' ']' '.' 'apply' '(' 'lambda' 'x'
 ':' 'x' '.' 'split' '(' 'colsynstrsep' ')' ')'
 '#make duplicated row for each synonymn' 'dfsynappended' '=' 'df' '['
 'colsyn' ']' '.' 'apply' '(' 'pd' '.' 'Series' ')' '.' 'unstack' '(' ')'
 '.' 'reset_index' '(' ')' '.' 'drop' '(' ""'level_0'"" ',' 'axis' '=' '1'
 ')' '.' 'set_index' '(' ""'level_1'"" ')' 'dfsynappended' '.' 'columns' '='
 '[' 'colsynappended' ']' 'dfsynappended' '=' 'dfsynappended' '.' 'dropna'
 '(' ')' 'return' 'dfsynappended' '.' 'join' '(' 'df' ',' 'how' '='
 ""'left'"" ')']","for merging dfs with names with df with synonymns
    param colsyn: col containing tuples of synonymns","['for' 'merging' 'dfs' 'with' 'names' 'with' 'df' 'with' 'synonymns'
 'param' 'colsyn' ':' 'col' 'containing' 'tuples' 'of' 'synonymns']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L638-L651
rraadd88/rohan,rohan/dandage/io_dfs.py,drop_duplicates_agg,"def drop_duplicates_agg(df,colsgroupby,cols2aggf,test=False):
    """"""
    colsgroupby: unique names ~index
    cols2aggf: rest of the cols `unique_dropna_str` for categories
    """"""
    if test:
        print(df.shape)
        print(df.drop_duplicates(subset=colsgroupby).shape)
    #ddup aggregated
    dfdupagg=df.loc[(df.duplicated(subset=colsgroupby,keep=False)),:].groupby(colsgroupby).agg(cols2aggf)
    #drop duplicates all
    df_=df.drop_duplicates(subset=colsgroupby,keep=False)
    if test:
        print(df_.shape)
    #append ddup aggregated
    dfout=df_.append(dfdupagg,sort=True)
    if test:
        print(dfout.shape)
    return dfout",python,"def drop_duplicates_agg(df,colsgroupby,cols2aggf,test=False):
    """"""
    colsgroupby: unique names ~index
    cols2aggf: rest of the cols `unique_dropna_str` for categories
    """"""
    if test:
        print(df.shape)
        print(df.drop_duplicates(subset=colsgroupby).shape)
    #ddup aggregated
    dfdupagg=df.loc[(df.duplicated(subset=colsgroupby,keep=False)),:].groupby(colsgroupby).agg(cols2aggf)
    #drop duplicates all
    df_=df.drop_duplicates(subset=colsgroupby,keep=False)
    if test:
        print(df_.shape)
    #append ddup aggregated
    dfout=df_.append(dfdupagg,sort=True)
    if test:
        print(dfout.shape)
    return dfout","['def' 'drop_duplicates_agg' '(' 'df' ',' 'colsgroupby' ',' 'cols2aggf'
 ',' 'test' '=' 'False' ')' ':' 'if' 'test' ':' 'print' '(' 'df' '.'
 'shape' ')' 'print' '(' 'df' '.' 'drop_duplicates' '(' 'subset' '='
 'colsgroupby' ')' '.' 'shape' ')' '#ddup aggregated' 'dfdupagg' '=' 'df'
 '.' 'loc' '[' '(' 'df' '.' 'duplicated' '(' 'subset' '=' 'colsgroupby'
 ',' 'keep' '=' 'False' ')' ')' ',' ':' ']' '.' 'groupby' '('
 'colsgroupby' ')' '.' 'agg' '(' 'cols2aggf' ')' '#drop duplicates all'
 'df_' '=' 'df' '.' 'drop_duplicates' '(' 'subset' '=' 'colsgroupby' ','
 'keep' '=' 'False' ')' 'if' 'test' ':' 'print' '(' 'df_' '.' 'shape' ')'
 '#append ddup aggregated' 'dfout' '=' 'df_' '.' 'append' '(' 'dfdupagg'
 ',' 'sort' '=' 'True' ')' 'if' 'test' ':' 'print' '(' 'dfout' '.' 'shape'
 ')' 'return' 'dfout']","colsgroupby: unique names ~index
    cols2aggf: rest of the cols `unique_dropna_str` for categories","['colsgroupby' ':' 'unique' 'names' '~index' 'cols2aggf' ':' 'rest' 'of'
 'the' 'cols' 'unique_dropna_str' 'for' 'categories']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L654-L672
rraadd88/rohan,rohan/dandage/io_dfs.py,get_intersectionsbysubsets,"def get_intersectionsbysubsets(df,cols_fracby2vals,cols_subset,col_ids):
    """"""
    cols_fracby:
    cols_subset:
    """"""
    for col_fracby in cols_fracby2vals:
        val=cols_fracby2vals[col_fracby]
        ids=df.loc[(df[col_fracby]==val),col_ids].dropna().unique()
        for col_subset in cols_subset:
            for subset in dropna(df[col_subset].unique()):
                ids_subset=df.loc[(df[col_subset]==subset),col_ids].dropna().unique()
                df.loc[(df[col_subset]==subset),f'P {col_fracby} {col_subset}']=len(set(ids_subset).intersection(ids))/len(ids_subset)
    return df",python,"def get_intersectionsbysubsets(df,cols_fracby2vals,cols_subset,col_ids):
    """"""
    cols_fracby:
    cols_subset:
    """"""
    for col_fracby in cols_fracby2vals:
        val=cols_fracby2vals[col_fracby]
        ids=df.loc[(df[col_fracby]==val),col_ids].dropna().unique()
        for col_subset in cols_subset:
            for subset in dropna(df[col_subset].unique()):
                ids_subset=df.loc[(df[col_subset]==subset),col_ids].dropna().unique()
                df.loc[(df[col_subset]==subset),f'P {col_fracby} {col_subset}']=len(set(ids_subset).intersection(ids))/len(ids_subset)
    return df","['def' 'get_intersectionsbysubsets' '(' 'df' ',' 'cols_fracby2vals' ','
 'cols_subset' ',' 'col_ids' ')' ':' 'for' 'col_fracby' 'in'
 'cols_fracby2vals' ':' 'val' '=' 'cols_fracby2vals' '[' 'col_fracby' ']'
 'ids' '=' 'df' '.' 'loc' '[' '(' 'df' '[' 'col_fracby' ']' '==' 'val' ')'
 ',' 'col_ids' ']' '.' 'dropna' '(' ')' '.' 'unique' '(' ')' 'for'
 'col_subset' 'in' 'cols_subset' ':' 'for' 'subset' 'in' 'dropna' '(' 'df'
 '[' 'col_subset' ']' '.' 'unique' '(' ')' ')' ':' 'ids_subset' '=' 'df'
 '.' 'loc' '[' '(' 'df' '[' 'col_subset' ']' '==' 'subset' ')' ','
 'col_ids' ']' '.' 'dropna' '(' ')' '.' 'unique' '(' ')' 'df' '.' 'loc'
 '[' '(' 'df' '[' 'col_subset' ']' '==' 'subset' ')' ','
 ""f'P {col_fracby} {col_subset}'"" ']' '=' 'len' '(' 'set' '(' 'ids_subset'
 ')' '.' 'intersection' '(' 'ids' ')' ')' '/' 'len' '(' 'ids_subset' ')'
 'return' 'df']","cols_fracby:
    cols_subset:",['cols_fracby' ':' 'cols_subset' ':'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_dfs.py#L687-L699
rraadd88/rohan,rohan/dandage/db/biogrid.py,get_dbiogrid_intmap,"def get_dbiogrid_intmap(taxid,dbiogridp,dbiogrid_intmap_symmp,dbiogrid_intlinp,dbiogrid_intmapp,
                        logf=None,
                        experimental_system_type='physical',
                        genefmt='name',
                        force=False,test=False,
                        keep_exp_syss=[],del_exp_syss=[],
                        filldiagonal_withna=False):
    """"""
    taxid=559292
    del_exp_syss=[""Co-purification"", ""Co-fractionation"", ""Proximity Label-MS"", 
                                  ""Affinity Capture-RNA"", ""Protein-peptide""]
    """"""    
    gene_fmt2colns={'biogrid':{'id':'Systematic Name',
                 'name':'Official Symbol'},}
    if (not exists(dbiogrid_intmap_symmp)) or force:
        if not exists(dbiogrid_intmapp) or force:
            if not exists(dbiogrid_intlinp) or force:
                if dbiogridp.endswith('tab2.txt'):
                    print(f""converting to parquet"")
                    dbiogrid=pd.read_csv(dbiogridp,
                       sep='\t',low_memory=False)
                    dbiogridp=dbiogridp+"".pqt""
                    to_table_pqt(dbiogrid,dbiogridp)
                    print(f""use {dbiogridp}"")
                elif dbiogridp.endswith('tab2.txt.pqt'):
                    dbiogrid=read_table_pqt(dbiogridp)
                # filter biogrid
                # taxonomic id of Scer
                dbiogrid=dbiogrid.loc[((dbiogrid['Experimental System Type']==experimental_system_type) \
                           & (dbiogrid['Organism Interactor A']==taxid) \
                           & (dbiogrid['Organism Interactor B']==taxid)),:]

                print('All physical Experimental Systems.')
                if test:
                    dlog=pd.DataFrame(dbiogrid['Experimental System'].value_counts())
                    print(dlog)
                    if not logf is None:
                        to_table(dlog,f'{logf}.all_int.tsv')                    
                if len(keep_exp_syss)!=0:
                    dbiogrid=dbiogrid.loc[((dbiogrid['Experimental System'].isin(keep_exp_syss))),:]
                elif len(del_exp_syss)!=0:
                    dbiogrid=dbiogrid.loc[((~dbiogrid['Experimental System'].isin(del_exp_syss))),:]
                elif len(del_exp_syss)!=0 and len(keep_exp_syss)!=0:
                    print('Error: either specify keep_exp_syss or del_exp_syss')
                    return False
                if test:
                    print('Experimental Systems used.')
                    dlog=pd.DataFrame(dbiogrid['Experimental System'].value_counts())
                    if not logf is None:
                        to_table(dlog,f'{logf}.kept_int.tsv')
                    print(dlog)
                to_table_pqt(dbiogrid,dbiogrid_intlinp)
            else:
                dbiogrid=read_table_pqt(dbiogrid_intlinp)
            # this cell makes a symmetric interaction map
            dbiogrid['count']=1
            if test:
                print(dbiogrid['count'].sum())
            # mean of counts of intearations
            ## higher is more confident interaction captured in multiple assays
            dbiogrid_grid=dbiogrid.pivot_table(values='count',
                                             index=f""{gene_fmt2colns['biogrid'][genefmt]} Interactor A"",
                                            columns=f""{gene_fmt2colns['biogrid'][genefmt]} Interactor B"",
                                            aggfunc='sum',)

            # make it symmetric
            if test:
                print('shape of non-symm intmap: ',dbiogrid_grid.shape)
            to_table_pqt(dbiogrid_grid,dbiogrid_intmapp)
        else:         
#             dbiogrid_grid=pd.read_table(dbiogrid_intmapp)
            dbiogrid_grid=read_table_pqt(dbiogrid_intmapp+'.pqt')
        dbiogrid_grid=set_index(dbiogrid_grid,f""{gene_fmt2colns['biogrid'][genefmt]} Interactor A"")
        geneids=set(dbiogrid_grid.index).union(set(dbiogrid_grid.columns))
        if test:
            print('total number of genes',len(geneids))
        # missing rows to nan
        dbiogrid_intmap_symm=pd.DataFrame(columns=geneids,index=geneids)

        dbiogrid_intmap_symm.loc[dbiogrid_grid.index,:]=dbiogrid_grid.loc[dbiogrid_grid.index,:]
        dbiogrid_intmap_symm.loc[:,dbiogrid_grid.columns]=dbiogrid_grid.loc[:,dbiogrid_grid.columns]
        if test:
            print(dbiogrid_intmap_symm.shape)
        dbiogrid_intmap_symm=dbiogrid_intmap_symm.fillna(0)
        dbiogrid_intmap_symm=(dbiogrid_intmap_symm+dbiogrid_intmap_symm.T)/2
        dbiogrid_intmap_symm.index.name='Interactor A'
        dbiogrid_intmap_symm.columns.name='Interactor B'
#         if test:
#             dbiogrid_intmap_symm=dbiogrid_intmap_symm.iloc[:5,:5]
        if filldiagonal_withna:
            dbiogrid_intmap_symm=filldiagonal(dbiogrid_intmap_symm)
        to_table_pqt(dbiogrid_intmap_symm,dbiogrid_intmap_symmp)
        print('file saved at: ',dbiogrid_intmap_symmp)
        dbiogrid_intmap_symm_lin=get_degrees(dbiogrid_intmap_symm)
        dbiogrid_intmap_symm_lin.index.name=f'gene {genefmt}'
        to_table(dbiogrid_intmap_symm_lin,f'{dbiogrid_intmap_symmp}.degrees.tsv')
    else:
        dbiogrid_intmap_symm=read_table_pqt(dbiogrid_intmap_symmp)
    return dbiogrid_intmap_symm",python,"def get_dbiogrid_intmap(taxid,dbiogridp,dbiogrid_intmap_symmp,dbiogrid_intlinp,dbiogrid_intmapp,
                        logf=None,
                        experimental_system_type='physical',
                        genefmt='name',
                        force=False,test=False,
                        keep_exp_syss=[],del_exp_syss=[],
                        filldiagonal_withna=False):
    """"""
    taxid=559292
    del_exp_syss=[""Co-purification"", ""Co-fractionation"", ""Proximity Label-MS"", 
                                  ""Affinity Capture-RNA"", ""Protein-peptide""]
    """"""    
    gene_fmt2colns={'biogrid':{'id':'Systematic Name',
                 'name':'Official Symbol'},}
    if (not exists(dbiogrid_intmap_symmp)) or force:
        if not exists(dbiogrid_intmapp) or force:
            if not exists(dbiogrid_intlinp) or force:
                if dbiogridp.endswith('tab2.txt'):
                    print(f""converting to parquet"")
                    dbiogrid=pd.read_csv(dbiogridp,
                       sep='\t',low_memory=False)
                    dbiogridp=dbiogridp+"".pqt""
                    to_table_pqt(dbiogrid,dbiogridp)
                    print(f""use {dbiogridp}"")
                elif dbiogridp.endswith('tab2.txt.pqt'):
                    dbiogrid=read_table_pqt(dbiogridp)
                # filter biogrid
                # taxonomic id of Scer
                dbiogrid=dbiogrid.loc[((dbiogrid['Experimental System Type']==experimental_system_type) \
                           & (dbiogrid['Organism Interactor A']==taxid) \
                           & (dbiogrid['Organism Interactor B']==taxid)),:]

                print('All physical Experimental Systems.')
                if test:
                    dlog=pd.DataFrame(dbiogrid['Experimental System'].value_counts())
                    print(dlog)
                    if not logf is None:
                        to_table(dlog,f'{logf}.all_int.tsv')                    
                if len(keep_exp_syss)!=0:
                    dbiogrid=dbiogrid.loc[((dbiogrid['Experimental System'].isin(keep_exp_syss))),:]
                elif len(del_exp_syss)!=0:
                    dbiogrid=dbiogrid.loc[((~dbiogrid['Experimental System'].isin(del_exp_syss))),:]
                elif len(del_exp_syss)!=0 and len(keep_exp_syss)!=0:
                    print('Error: either specify keep_exp_syss or del_exp_syss')
                    return False
                if test:
                    print('Experimental Systems used.')
                    dlog=pd.DataFrame(dbiogrid['Experimental System'].value_counts())
                    if not logf is None:
                        to_table(dlog,f'{logf}.kept_int.tsv')
                    print(dlog)
                to_table_pqt(dbiogrid,dbiogrid_intlinp)
            else:
                dbiogrid=read_table_pqt(dbiogrid_intlinp)
            # this cell makes a symmetric interaction map
            dbiogrid['count']=1
            if test:
                print(dbiogrid['count'].sum())
            # mean of counts of intearations
            ## higher is more confident interaction captured in multiple assays
            dbiogrid_grid=dbiogrid.pivot_table(values='count',
                                             index=f""{gene_fmt2colns['biogrid'][genefmt]} Interactor A"",
                                            columns=f""{gene_fmt2colns['biogrid'][genefmt]} Interactor B"",
                                            aggfunc='sum',)

            # make it symmetric
            if test:
                print('shape of non-symm intmap: ',dbiogrid_grid.shape)
            to_table_pqt(dbiogrid_grid,dbiogrid_intmapp)
        else:         
#             dbiogrid_grid=pd.read_table(dbiogrid_intmapp)
            dbiogrid_grid=read_table_pqt(dbiogrid_intmapp+'.pqt')
        dbiogrid_grid=set_index(dbiogrid_grid,f""{gene_fmt2colns['biogrid'][genefmt]} Interactor A"")
        geneids=set(dbiogrid_grid.index).union(set(dbiogrid_grid.columns))
        if test:
            print('total number of genes',len(geneids))
        # missing rows to nan
        dbiogrid_intmap_symm=pd.DataFrame(columns=geneids,index=geneids)

        dbiogrid_intmap_symm.loc[dbiogrid_grid.index,:]=dbiogrid_grid.loc[dbiogrid_grid.index,:]
        dbiogrid_intmap_symm.loc[:,dbiogrid_grid.columns]=dbiogrid_grid.loc[:,dbiogrid_grid.columns]
        if test:
            print(dbiogrid_intmap_symm.shape)
        dbiogrid_intmap_symm=dbiogrid_intmap_symm.fillna(0)
        dbiogrid_intmap_symm=(dbiogrid_intmap_symm+dbiogrid_intmap_symm.T)/2
        dbiogrid_intmap_symm.index.name='Interactor A'
        dbiogrid_intmap_symm.columns.name='Interactor B'
#         if test:
#             dbiogrid_intmap_symm=dbiogrid_intmap_symm.iloc[:5,:5]
        if filldiagonal_withna:
            dbiogrid_intmap_symm=filldiagonal(dbiogrid_intmap_symm)
        to_table_pqt(dbiogrid_intmap_symm,dbiogrid_intmap_symmp)
        print('file saved at: ',dbiogrid_intmap_symmp)
        dbiogrid_intmap_symm_lin=get_degrees(dbiogrid_intmap_symm)
        dbiogrid_intmap_symm_lin.index.name=f'gene {genefmt}'
        to_table(dbiogrid_intmap_symm_lin,f'{dbiogrid_intmap_symmp}.degrees.tsv')
    else:
        dbiogrid_intmap_symm=read_table_pqt(dbiogrid_intmap_symmp)
    return dbiogrid_intmap_symm","['def' 'get_dbiogrid_intmap' '(' 'taxid' ',' 'dbiogridp' ','
 'dbiogrid_intmap_symmp' ',' 'dbiogrid_intlinp' ',' 'dbiogrid_intmapp' ','
 'logf' '=' 'None' ',' 'experimental_system_type' '=' ""'physical'"" ','
 'genefmt' '=' ""'name'"" ',' 'force' '=' 'False' ',' 'test' '=' 'False' ','
 'keep_exp_syss' '=' '[' ']' ',' 'del_exp_syss' '=' '[' ']' ','
 'filldiagonal_withna' '=' 'False' ')' ':' 'gene_fmt2colns' '=' '{'
 ""'biogrid'"" ':' '{' ""'id'"" ':' ""'Systematic Name'"" ',' ""'name'"" ':'
 ""'Official Symbol'"" '}' ',' '}' 'if' '(' 'not' 'exists' '('
 'dbiogrid_intmap_symmp' ')' ')' 'or' 'force' ':' 'if' 'not' 'exists' '('
 'dbiogrid_intmapp' ')' 'or' 'force' ':' 'if' 'not' 'exists' '('
 'dbiogrid_intlinp' ')' 'or' 'force' ':' 'if' 'dbiogridp' '.' 'endswith'
 '(' ""'tab2.txt'"" ')' ':' 'print' '(' 'f""converting to parquet""' ')'
 'dbiogrid' '=' 'pd' '.' 'read_csv' '(' 'dbiogridp' ',' 'sep' '=' ""'\\t'""
 ',' 'low_memory' '=' 'False' ')' 'dbiogridp' '=' 'dbiogridp' '+' '"".pqt""'
 'to_table_pqt' '(' 'dbiogrid' ',' 'dbiogridp' ')' 'print' '('
 'f""use {dbiogridp}""' ')' 'elif' 'dbiogridp' '.' 'endswith' '('
 ""'tab2.txt.pqt'"" ')' ':' 'dbiogrid' '=' 'read_table_pqt' '(' 'dbiogridp'
 ')' '# filter biogrid' '# taxonomic id of Scer' 'dbiogrid' '=' 'dbiogrid'
 '.' 'loc' '[' '(' '(' 'dbiogrid' '[' ""'Experimental System Type'"" ']'
 '==' 'experimental_system_type' ')' '&' '(' 'dbiogrid' '['
 ""'Organism Interactor A'"" ']' '==' 'taxid' ')' '&' '(' 'dbiogrid' '['
 ""'Organism Interactor B'"" ']' '==' 'taxid' ')' ')' ',' ':' ']' 'print'
 '(' ""'All physical Experimental Systems.'"" ')' 'if' 'test' ':' 'dlog' '='
 'pd' '.' 'DataFrame' '(' 'dbiogrid' '[' ""'Experimental System'"" ']' '.'
 'value_counts' '(' ')' ')' 'print' '(' 'dlog' ')' 'if' 'not' 'logf' 'is'
 'None' ':' 'to_table' '(' 'dlog' ',' ""f'{logf}.all_int.tsv'"" ')' 'if'
 'len' '(' 'keep_exp_syss' ')' '!=' '0' ':' 'dbiogrid' '=' 'dbiogrid' '.'
 'loc' '[' '(' '(' 'dbiogrid' '[' ""'Experimental System'"" ']' '.' 'isin'
 '(' 'keep_exp_syss' ')' ')' ')' ',' ':' ']' 'elif' 'len' '('
 'del_exp_syss' ')' '!=' '0' ':' 'dbiogrid' '=' 'dbiogrid' '.' 'loc' '['
 '(' '(' '~' 'dbiogrid' '[' ""'Experimental System'"" ']' '.' 'isin' '('
 'del_exp_syss' ')' ')' ')' ',' ':' ']' 'elif' 'len' '(' 'del_exp_syss'
 ')' '!=' '0' 'and' 'len' '(' 'keep_exp_syss' ')' '!=' '0' ':' 'print' '('
 ""'Error: either specify keep_exp_syss or del_exp_syss'"" ')' 'return'
 'False' 'if' 'test' ':' 'print' '(' ""'Experimental Systems used.'"" ')'
 'dlog' '=' 'pd' '.' 'DataFrame' '(' 'dbiogrid' '['
 ""'Experimental System'"" ']' '.' 'value_counts' '(' ')' ')' 'if' 'not'
 'logf' 'is' 'None' ':' 'to_table' '(' 'dlog' ',' ""f'{logf}.kept_int.tsv'""
 ')' 'print' '(' 'dlog' ')' 'to_table_pqt' '(' 'dbiogrid' ','
 'dbiogrid_intlinp' ')' 'else' ':' 'dbiogrid' '=' 'read_table_pqt' '('
 'dbiogrid_intlinp' ')' '# this cell makes a symmetric interaction map'
 'dbiogrid' '[' ""'count'"" ']' '=' '1' 'if' 'test' ':' 'print' '('
 'dbiogrid' '[' ""'count'"" ']' '.' 'sum' '(' ')' ')'
 '# mean of counts of intearations'
 '## higher is more confident interaction captured in multiple assays'
 'dbiogrid_grid' '=' 'dbiogrid' '.' 'pivot_table' '(' 'values' '='
 ""'count'"" ',' 'index' '='
 'f""{gene_fmt2colns[\'biogrid\'][genefmt]} Interactor A""' ',' 'columns'
 '=' 'f""{gene_fmt2colns[\'biogrid\'][genefmt]} Interactor B""' ','
 'aggfunc' '=' ""'sum'"" ',' ')' '# make it symmetric' 'if' 'test' ':'
 'print' '(' ""'shape of non-symm intmap: '"" ',' 'dbiogrid_grid' '.'
 'shape' ')' 'to_table_pqt' '(' 'dbiogrid_grid' ',' 'dbiogrid_intmapp' ')'
 'else' ':' '#             dbiogrid_grid=pd.read_table(dbiogrid_intmapp)'
 'dbiogrid_grid' '=' 'read_table_pqt' '(' 'dbiogrid_intmapp' '+' ""'.pqt'""
 ')' 'dbiogrid_grid' '=' 'set_index' '(' 'dbiogrid_grid' ','
 'f""{gene_fmt2colns[\'biogrid\'][genefmt]} Interactor A""' ')' 'geneids'
 '=' 'set' '(' 'dbiogrid_grid' '.' 'index' ')' '.' 'union' '(' 'set' '('
 'dbiogrid_grid' '.' 'columns' ')' ')' 'if' 'test' ':' 'print' '('
 ""'total number of genes'"" ',' 'len' '(' 'geneids' ')' ')'
 '# missing rows to nan' 'dbiogrid_intmap_symm' '=' 'pd' '.' 'DataFrame'
 '(' 'columns' '=' 'geneids' ',' 'index' '=' 'geneids' ')'
 'dbiogrid_intmap_symm' '.' 'loc' '[' 'dbiogrid_grid' '.' 'index' ',' ':'
 ']' '=' 'dbiogrid_grid' '.' 'loc' '[' 'dbiogrid_grid' '.' 'index' ',' ':'
 ']' 'dbiogrid_intmap_symm' '.' 'loc' '[' ':' ',' 'dbiogrid_grid' '.'
 'columns' ']' '=' 'dbiogrid_grid' '.' 'loc' '[' ':' ',' 'dbiogrid_grid'
 '.' 'columns' ']' 'if' 'test' ':' 'print' '(' 'dbiogrid_intmap_symm' '.'
 'shape' ')' 'dbiogrid_intmap_symm' '=' 'dbiogrid_intmap_symm' '.'
 'fillna' '(' '0' ')' 'dbiogrid_intmap_symm' '=' '('
 'dbiogrid_intmap_symm' '+' 'dbiogrid_intmap_symm' '.' 'T' ')' '/' '2'
 'dbiogrid_intmap_symm' '.' 'index' '.' 'name' '=' ""'Interactor A'""
 'dbiogrid_intmap_symm' '.' 'columns' '.' 'name' '=' ""'Interactor B'""
 '#         if test:'
 '#             dbiogrid_intmap_symm=dbiogrid_intmap_symm.iloc[:5,:5]'
 'if' 'filldiagonal_withna' ':' 'dbiogrid_intmap_symm' '=' 'filldiagonal'
 '(' 'dbiogrid_intmap_symm' ')' 'to_table_pqt' '(' 'dbiogrid_intmap_symm'
 ',' 'dbiogrid_intmap_symmp' ')' 'print' '(' ""'file saved at: '"" ','
 'dbiogrid_intmap_symmp' ')' 'dbiogrid_intmap_symm_lin' '=' 'get_degrees'
 '(' 'dbiogrid_intmap_symm' ')' 'dbiogrid_intmap_symm_lin' '.' 'index' '.'
 'name' '=' ""f'gene {genefmt}'"" 'to_table' '(' 'dbiogrid_intmap_symm_lin'
 ',' ""f'{dbiogrid_intmap_symmp}.degrees.tsv'"" ')' 'else' ':'
 'dbiogrid_intmap_symm' '=' 'read_table_pqt' '(' 'dbiogrid_intmap_symmp'
 ')' 'return' 'dbiogrid_intmap_symm']","taxid=559292
    del_exp_syss=[""Co-purification"", ""Co-fractionation"", ""Proximity Label-MS"", 
                                  ""Affinity Capture-RNA"", ""Protein-peptide""]","['taxid' '=' '559292' 'del_exp_syss' '=' '[' 'Co' '-' 'purification' 'Co'
 '-' 'fractionation' 'Proximity' 'Label' '-' 'MS' 'Affinity' 'Capture' '-'
 'RNA' 'Protein' '-' 'peptide' ']']",train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/db/biogrid.py#L15-L113
rraadd88/rohan,rohan/dandage/io_text.py,corpus2clusters,"def corpus2clusters(corpus, index,params_clustermap={'vmin':0,'vmax':1,'figsize':[6,6]}):
    """"""
    corpus: list of strings
    """"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    vect = TfidfVectorizer()
    tfidf = vect.fit_transform(corpus)

    df=pd.DataFrame((tfidf * tfidf.T).A,
                index=index,
                columns=index,
                )
    clustergrid=sns.clustermap(df,**params_clustermap
    #                                method='complete', metric='canberra',
                                  )
    dclusters=get_clusters(clustergrid,axis=0,criterion='maxclust',clusters_fraction=0.25)
    return dclusters",python,"def corpus2clusters(corpus, index,params_clustermap={'vmin':0,'vmax':1,'figsize':[6,6]}):
    """"""
    corpus: list of strings
    """"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    vect = TfidfVectorizer()
    tfidf = vect.fit_transform(corpus)

    df=pd.DataFrame((tfidf * tfidf.T).A,
                index=index,
                columns=index,
                )
    clustergrid=sns.clustermap(df,**params_clustermap
    #                                method='complete', metric='canberra',
                                  )
    dclusters=get_clusters(clustergrid,axis=0,criterion='maxclust',clusters_fraction=0.25)
    return dclusters","['def' 'corpus2clusters' '(' 'corpus' ',' 'index' ',' 'params_clustermap'
 '=' '{' ""'vmin'"" ':' '0' ',' ""'vmax'"" ':' '1' ',' ""'figsize'"" ':' '[' '6'
 ',' '6' ']' '}' ')' ':' 'from' 'sklearn' '.' 'feature_extraction' '.'
 'text' 'import' 'TfidfVectorizer' 'vect' '=' 'TfidfVectorizer' '(' ')'
 'tfidf' '=' 'vect' '.' 'fit_transform' '(' 'corpus' ')' 'df' '=' 'pd' '.'
 'DataFrame' '(' '(' 'tfidf' '*' 'tfidf' '.' 'T' ')' '.' 'A' ',' 'index'
 '=' 'index' ',' 'columns' '=' 'index' ',' ')' 'clustergrid' '=' 'sns' '.'
 'clustermap' '(' 'df' ',' '*' '*' 'params_clustermap'
 ""#                                method='complete', metric='canberra',""
 ')' 'dclusters' '=' 'get_clusters' '(' 'clustergrid' ',' 'axis' '=' '0'
 ',' 'criterion' '=' ""'maxclust'"" ',' 'clusters_fraction' '=' '0.25' ')'
 'return' 'dclusters']",corpus: list of strings,['corpus' ':' 'list' 'of' 'strings'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/io_text.py#L3-L19
rraadd88/rohan,rohan/dandage/__init__.py,get_deps,"def get_deps(cfg=None,deps=[]):
    """"""
    Installs conda dependencies.

    :param cfg: configuration dict
    """"""
    if not cfg is None:
        if not 'deps' in cfg:
            cfg['deps']=deps
        else:
            deps=cfg['deps']
    if not len(deps)==0:
        for dep in deps:
            if not dep in cfg:
                runbashcmd(f'conda install {dep}',test=cfg['test'])
                cfg[dep]=dep
    logging.info(f""{len(deps)} deps installed."")
    return cfg",python,"def get_deps(cfg=None,deps=[]):
    """"""
    Installs conda dependencies.

    :param cfg: configuration dict
    """"""
    if not cfg is None:
        if not 'deps' in cfg:
            cfg['deps']=deps
        else:
            deps=cfg['deps']
    if not len(deps)==0:
        for dep in deps:
            if not dep in cfg:
                runbashcmd(f'conda install {dep}',test=cfg['test'])
                cfg[dep]=dep
    logging.info(f""{len(deps)} deps installed."")
    return cfg","['def' 'get_deps' '(' 'cfg' '=' 'None' ',' 'deps' '=' '[' ']' ')' ':' 'if'
 'not' 'cfg' 'is' 'None' ':' 'if' 'not' ""'deps'"" 'in' 'cfg' ':' 'cfg' '['
 ""'deps'"" ']' '=' 'deps' 'else' ':' 'deps' '=' 'cfg' '[' ""'deps'"" ']' 'if'
 'not' 'len' '(' 'deps' ')' '==' '0' ':' 'for' 'dep' 'in' 'deps' ':' 'if'
 'not' 'dep' 'in' 'cfg' ':' 'runbashcmd' '(' ""f'conda install {dep}'"" ','
 'test' '=' 'cfg' '[' ""'test'"" ']' ')' 'cfg' '[' 'dep' ']' '=' 'dep'
 'logging' '.' 'info' '(' 'f""{len(deps)} deps installed.""' ')' 'return'
 'cfg']","Installs conda dependencies.

    :param cfg: configuration dict",['Installs' 'conda' 'dependencies' '.'],train,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/__init__.py#L7-L24
