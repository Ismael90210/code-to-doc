{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-08T19:05:13.122976Z",
     "start_time": "2025-05-08T19:05:13.114213Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from prompt_techniques import PromptTechniques\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from rouge import Rouge\n",
    "from transformers import logging as transformers_logging"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:42:41.582422Z",
     "start_time": "2025-05-08T19:42:38.717261Z"
    }
   },
   "cell_type": "code",
   "source": "!pip uninstall transformers -y",
   "id": "4d52804fd547e4f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T20:08:27.769407Z",
     "start_time": "2025-05-08T20:07:51.414044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install tokenizers==0.13.3 --no-deps  \n",
    "!pip install transformers==4.31.0"
   ],
   "id": "55a343cb060dd1d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.13.3\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [62 lines of output]\n",
      "  C:\\Users\\adenk\\AppData\\Local\\Temp\\pip-build-env-g0bwij9z\\overlay\\Lib\\site-packages\\setuptools\\dist.py:761: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: Apache Software License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Obtaining dependency information for transformers==4.31.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers==4.31.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.31.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers==4.31.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers==4.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers==4.31.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers==4.31.0) (2025.1.31)\n",
      "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [62 lines of output]\n",
      "  C:\\Users\\adenk\\AppData\\Local\\Temp\\pip-build-env-w5mixx1w\\overlay\\Lib\\site-packages\\setuptools\\dist.py:761: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: Apache Software License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:23:50.173761Z",
     "start_time": "2025-05-08T19:22:47.348231Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install --force-reinstall accelerate>=0.26.0",
   "id": "c8aa2b24f5d7a812",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\adenk\\\\PycharmProjects\\\\code-to-doc\\\\.venv\\\\Lib\\\\site-packages\\\\~afetensors\\\\_safetensors_rust.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:16:31.638731Z",
     "start_time": "2025-05-08T19:14:48.775271Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install torch transformers[torch] accelerate>=0.26.0",
   "id": "dd2c38064e866684",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:43:53.166003Z",
     "start_time": "2025-05-08T19:43:50.348754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import __version__ as transformers_version\n",
    "from accelerate import __version__ as accelerate_version\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers_version}\")\n",
    "print(f\"Accelerate: {accelerate_version}\")"
   ],
   "id": "bc0e551393fc3c5a",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m transformers_version\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01maccelerate\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m accelerate_version\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPyTorch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch.__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'transformers'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T16:53:50.958415Z",
     "start_time": "2025-05-08T16:53:50.949141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration Cell - Set these variables before running\n",
    "DATA_DIR = '../dataset'  # Directory containing your datasets\n",
    "OUTPUT_DIR = '../output'  # Directory to save results\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ],
   "id": "89d17c2481ed96cf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T16:53:52.993126Z",
     "start_time": "2025-05-08T16:53:52.959907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# %%\n",
    "# Helper functions\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load dataset from CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_results(df, filename):\n",
    "    \"\"\"Save results to CSV\"\"\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# %%\n",
    "# Model and Technique Selection\n",
    "model_options = ['llama3.2:1b', 'qwen2.5-coder:0.5b', 'deepseek-r1:1.5b']\n",
    "technique_options = {\n",
    "    '1': 'Zero-shot',\n",
    "    '2': 'Few-shot', \n",
    "    '3': 'Chain-of-thought',\n",
    "    '4': 'Structured',\n",
    "    '5': 'One-shot'\n",
    "}\n",
    "\n",
    "# Create interactive widgets\n",
    "# Reverse mapping for display names to keys\n",
    "technique_keys = {v: k for k, v in technique_options.items()}\n",
    "\n",
    "# Create interactive widgets\n",
    "model_dropdown = widgets.Dropdown(options=model_options, description='Model:')\n",
    "technique_dropdown = widgets.Dropdown(options=list(technique_options.values()), description='Technique:')\n",
    "dataset_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]\n",
    "dataset_dropdown = widgets.Dropdown(options=dataset_files, description='Dataset:')\n",
    "\n",
    "display(model_dropdown, technique_dropdown, dataset_dropdown)\n",
    "\n"
   ],
   "id": "a279ea60f82754e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropdown(description='Model:', options=('llama3.2:1b', 'qwen2.5-coder:0.5b', 'deepseek-r1:1.5b'), value='llama…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d60cdfc2a84429793a364ebf7160486"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dropdown(description='Technique:', options=('Zero-shot', 'Few-shot', 'Chain-of-thought', 'Structured', 'One-sh…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e5fc1e9057a40ab9b6c56cc18874bf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dropdown(description='Dataset:', options=('ctuning_ck.csv', 'data.csv', 'HHammond_PrettyPandas.csv', 'michaelp…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63bca0c2514743988838447c93b501b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:34:54.791032Z",
     "start_time": "2025-05-08T17:31:40.462634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main Processing Cell\n",
    "def run_generator(model, technique_name, dataset_file):\n",
    "    \"\"\"Run the documentation generator with selected parameters\"\"\"\n",
    "    # Convert technique name back to key\n",
    "    technique = technique_keys[technique_name]\n",
    "    \n",
    "    # Load dataset\n",
    "    df = load_dataset(os.path.join(DATA_DIR, dataset_file))\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Initialize prompt techniques\n",
    "    pt = PromptTechniques(model)\n",
    "    \n",
    "    # Prepare examples (from your main.py)\n",
    "    examples = [\n",
    "        {\"input\": \"def add(a, b): return a + b\", \"output\": \"Parameters: a first number, b first number Returns: sum of two numbers\"}, \n",
    "        {\"input\": \"def greet(name): print(f\\\"Hello, {name}\\\")\", \"output\": \"Greets a user by name.\"},\n",
    "        # ... (include all your examples from main.py)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each function in the dataset\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating docs\"):\n",
    "        func = row['func_code_string']\n",
    "        docstring = row.get('func_documentation_string', '')\n",
    "        \n",
    "        # Apply selected technique\n",
    "        generated = None\n",
    "        try:\n",
    "            if technique == '1':\n",
    "                generated = pt.zero_shot_prompting(func, 1)\n",
    "            elif technique == '2':\n",
    "                generated = pt.few_shot_prompting(func, examples, 1)\n",
    "            elif technique == '3':\n",
    "                generated = pt.chain_of_thought_prompting(func, 1)\n",
    "            elif technique == '4':\n",
    "                generated = pt.structured_prompting(func, 1)\n",
    "            elif technique == '5':\n",
    "                generated = pt.one_shot_prompting(func, examples, 1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating docs for {row.get('func_name', '')}: {str(e)}\")\n",
    "            generated = None\n",
    "        \n",
    "        # Store results with proper key handling\n",
    "        result = {\n",
    "            'filename': row.get('repository_name', ''),\n",
    "            'function_name': row.get('func_name', ''),\n",
    "            'input_code': func,\n",
    "            'model': model,\n",
    "            'technique': technique_name,\n",
    "            'original_doc': docstring,\n",
    "            'prompt_used': pt.prompt,\n",
    "            'generated_doc': ''  # Default empty string\n",
    "        }\n",
    "        \n",
    "        if generated:\n",
    "            if technique in ['1', '4']:  # Zero-shot and Structured use 'output'\n",
    "                result['generated_doc'] = generated[0].get('output', '')\n",
    "            elif technique in ['2', '3', '5']:  # Few-shot, Chain-of-thought, One-shot use 'model_output'\n",
    "                result['generated_doc'] = generated[0].get('model_output', '')\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# %%\n",
    "# Execute with selected parameters\n",
    "selected_model = model_dropdown.value\n",
    "selected_technique_name = technique_dropdown.value  # Get the display name\n",
    "selected_dataset = dataset_dropdown.value\n",
    "\n",
    "results_df = run_generator(selected_model, selected_technique_name, selected_dataset)\n"
   ],
   "id": "81093d4172dc71a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. Shape: (5, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating docs: 100%|██████████| 5/5 [03:14<00:00, 38.86s/it]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display \n",
    "if results_df is not None:\n",
    "    # Display sample results\n",
    "    display(Markdown(\"### Sample Generated Documentation\"))\n",
    "    sample = results_df.sample(min(3, len(results_df)))\n",
    "    for _, row in sample.iterrows():\n",
    "        display(Markdown(f\"**Function:** {row['function_name']}\"))\n",
    "        display(Markdown(f\"**Original Code:**\\n```python\\n{row['input_code']}\\n```\"))\n",
    "        display(Markdown(f\"**Generated Docstring:**\\n```\\n{row['generated_doc']}\\n```\"))\n",
    "        display(Markdown(\"---\"))\n",
    "    \n",
    "    # Show full DataFrame\n",
    "    display(Markdown(\"### Full Results DataFrame\"))\n",
    "    display(results_df.head())"
   ],
   "id": "10b23cbf8fca2f60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:35:02.963970Z",
     "start_time": "2025-05-08T17:35:02.954093Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.head()",
   "id": "3f055d979e61ff0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             filename       function_name  \\\n",
       "0  tadeck/onetimepass  _is_possible_token   \n",
       "1  tadeck/onetimepass            get_hotp   \n",
       "2  tadeck/onetimepass            get_totp   \n",
       "3  tadeck/onetimepass          valid_hotp   \n",
       "4  tadeck/onetimepass          valid_totp   \n",
       "\n",
       "                                          input_code             model  \\\n",
       "0  def _is_possible_token(token, token_length=6):...  deepseek-r1:1.5b   \n",
       "1  def get_hotp(\\r\\n        secret,\\r\\n        in...  deepseek-r1:1.5b   \n",
       "2  def get_totp(\\r\\n        secret,\\r\\n        as...  deepseek-r1:1.5b   \n",
       "3  def valid_hotp(\\r\\n        token,\\r\\n        s...  deepseek-r1:1.5b   \n",
       "4  def valid_totp(\\r\\n        token,\\r\\n        s...  deepseek-r1:1.5b   \n",
       "\n",
       "  technique                                       original_doc  \\\n",
       "0  One-shot  Determines if given value is acceptable as a t...   \n",
       "1  One-shot  Get HMAC-based one-time password on the basis ...   \n",
       "2  One-shot  Get time-based one-time password on the basis ...   \n",
       "3  One-shot  Check if given token is valid for given secret...   \n",
       "4  One-shot  Check if given token is valid time-based one-t...   \n",
       "\n",
       "                                         prompt_used  \\\n",
       "0  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "1  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "2  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "3  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "4  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "\n",
       "                                       generated_doc  \n",
       "0  <think>\\nAlright, so I'm looking at this probl...  \n",
       "1  <think>\\nAlright, I need to write a Python doc...  \n",
       "2  <think>\\nAlright, let's tackle this problem st...  \n",
       "3  <think>\\nAlright, I need to write a docstring ...  \n",
       "4  <think>\\nAlright, let's tackle this problem st...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>function_name</th>\n",
       "      <th>input_code</th>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th>original_doc</th>\n",
       "      <th>prompt_used</th>\n",
       "      <th>generated_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>_is_possible_token</td>\n",
       "      <td>def _is_possible_token(token, token_length=6):...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, so I'm looking at this probl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_hotp</td>\n",
       "      <td>def get_hotp(\\r\\n        secret,\\r\\n        in...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, I need to write a Python doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_totp</td>\n",
       "      <td>def get_totp(\\r\\n        secret,\\r\\n        as...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, let's tackle this problem st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_hotp</td>\n",
       "      <td>def valid_hotp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, I need to write a docstring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_totp</td>\n",
       "      <td>def valid_totp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, let's tackle this problem st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "llama",
   "id": "a562650da8cc023d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:28:42.772971Z",
     "start_time": "2025-05-08T04:28:42.766092Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/llama_zero.csv', index=False)",
   "id": "3647e44f94fd1dc6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:52:52.068844Z",
     "start_time": "2025-05-08T04:52:52.060497Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/llama_few.csv', index=False)",
   "id": "1d7a70eda2997108",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T05:00:52.771805Z",
     "start_time": "2025-05-08T05:00:52.763846Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/llama_chain.csv', index=False)",
   "id": "60e9f88dda7d3153",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T05:08:35.165024Z",
     "start_time": "2025-05-08T05:08:35.158081Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/llama_structured.csv', index=False)",
   "id": "adcacfa86744b2b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T05:12:15.157950Z",
     "start_time": "2025-05-08T05:12:15.151679Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/llama_oneshot.csv', index=False)",
   "id": "886bdd7baebb9e63",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "qwen",
   "id": "efb8ee3872186205"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T16:57:14.170884Z",
     "start_time": "2025-05-08T16:57:14.139619Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/qwen_zero.csv', index = False)",
   "id": "71c0358540262e95",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:01:17.625503Z",
     "start_time": "2025-05-08T17:01:17.613545Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/qwen_few.csv', index = False)",
   "id": "e014ab62970f4658",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:03:37.067651Z",
     "start_time": "2025-05-08T17:03:37.056180Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/qwen_chain.csv', index = False)",
   "id": "32a8088809b46067",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:06:02.771632Z",
     "start_time": "2025-05-08T17:06:02.761165Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/qwen_structured.csv', index = False)",
   "id": "b36f76f8572829ad",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:09:45.576091Z",
     "start_time": "2025-05-08T17:09:45.568213Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/qwen_oneshot.csv', index = False)",
   "id": "1e3f153c7d5da913",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "deepseek",
   "id": "5961ae89f7b28ec5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:15:50.386195Z",
     "start_time": "2025-05-08T17:15:50.375840Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/deep_zero.csv', index = False)",
   "id": "f267406f18f5975c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:22:40.051852Z",
     "start_time": "2025-05-08T17:22:40.040534Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/deep_few.csv', index = False)",
   "id": "605994d69d93837f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:27:36.509184Z",
     "start_time": "2025-05-08T17:27:36.503060Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/deep_chain.csv', index = False)",
   "id": "418b1478464fb55c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:31:31.316745Z",
     "start_time": "2025-05-08T17:31:31.309640Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/deep_structured.csv', index = False)",
   "id": "386a4bc8b9ba34f3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:35:09.176338Z",
     "start_time": "2025-05-08T17:35:09.169433Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.to_csv('../output/deep_oneshot.csv', index = False)",
   "id": "a0e86ea64631d3ae",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b0091f4287ceadd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:51:10.117360Z",
     "start_time": "2025-05-08T17:51:10.106800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_docstring(text):\n",
    "    \"\"\"\n",
    "    Extract just the docstring content from model output, handling different formats.\n",
    "    Removes code fences, language markers, and other non-docstring content.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Pattern for Google-style docstrings (triple quotes with optional whitespace)\n",
    "    docstring_pattern = r'\\\"\\\"\\\"(.*?)\\\"\\\"\\\"|\\'\\'\\'(.*?)\\'\\'\\''\n",
    "    \n",
    "    # Try to find docstring pattern first\n",
    "    matches = re.findall(docstring_pattern, text, re.DOTALL)\n",
    "    if matches:\n",
    "        # Join all matches (handles cases with both single and double quotes)\n",
    "        combined = \" \".join([\" \".join(match) for match in matches])\n",
    "        return combined.strip()\n",
    "    \n",
    "    # If no docstring pattern found, look for markdown code blocks\n",
    "    code_block_pattern = r'```(?:python)?\\n(.*?)\\n```'\n",
    "    code_matches = re.findall(code_block_pattern, text, re.DOTALL)\n",
    "    if code_matches:\n",
    "        return code_matches[0].strip()\n",
    "    \n",
    "    # Fallback - return first 3 lines if no clear pattern found\n",
    "    return \"\\n\".join(text.split(\"\\n\")[:3]).strip()\n",
    "\n",
    "def process_output_file(file_path):\n",
    "    \"\"\"Process a single output file to extract clean docstrings\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create new column for cleaned docstrings\n",
    "        df['clean_docstring'] = df['generated_doc'].apply(extract_docstring)\n",
    "        \n",
    "        # Save back to same file with additional column\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Processed and saved: {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None"
   ],
   "id": "5360823680ad0a7a",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:51:31.093168Z",
     "start_time": "2025-05-08T17:51:31.038600Z"
    }
   },
   "cell_type": "code",
   "source": "process_output_file('../output/llama_zero.csv')",
   "id": "2a5cdb93ac0b8b0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: ../output/llama_zero.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             filename       function_name  \\\n",
       "0  tadeck/onetimepass  _is_possible_token   \n",
       "1  tadeck/onetimepass            get_hotp   \n",
       "2  tadeck/onetimepass            get_totp   \n",
       "3  tadeck/onetimepass          valid_hotp   \n",
       "4  tadeck/onetimepass          valid_totp   \n",
       "\n",
       "                                          input_code        model  technique  \\\n",
       "0  def _is_possible_token(token, token_length=6):...  llama3.2:1b  Zero-shot   \n",
       "1  def get_hotp(\\r\\n        secret,\\r\\n        in...  llama3.2:1b  Zero-shot   \n",
       "2  def get_totp(\\r\\n        secret,\\r\\n        as...  llama3.2:1b  Zero-shot   \n",
       "3  def valid_hotp(\\r\\n        token,\\r\\n        s...  llama3.2:1b  Zero-shot   \n",
       "4  def valid_totp(\\r\\n        token,\\r\\n        s...  llama3.2:1b  Zero-shot   \n",
       "\n",
       "                                       generated_doc  \\\n",
       "0  ```\\ndef _is_possible_token(\\n    token: int |...   \n",
       "1  ```\\ndef get_hotp(\\n    secret: str or bytes,\\...   \n",
       "2  ```\\ndef get_totp(\\n    secret,\\n    as_string...   \n",
       "3  ```\\ndef valid_hotp(\\n    token: int or str,  ...   \n",
       "4  ```\\n\"\"\"\\nChecks if a given time-based one-tim...   \n",
       "\n",
       "                                        original_doc  \\\n",
       "0  Determines if given value is acceptable as a t...   \n",
       "1  Get HMAC-based one-time password on the basis ...   \n",
       "2  Get time-based one-time password on the basis ...   \n",
       "3  Check if given token is valid for given secret...   \n",
       "4  Check if given token is valid time-based one-t...   \n",
       "\n",
       "                                         prompt_used  \\\n",
       "0  As a senior Python engineer, write a professio...   \n",
       "1  As a senior Python engineer, write a professio...   \n",
       "2  As a senior Python engineer, write a professio...   \n",
       "3  As a senior Python engineer, write a professio...   \n",
       "4  As a senior Python engineer, write a professio...   \n",
       "\n",
       "                                     clean_docstring  \n",
       "0  Determines if given value is acceptable as a t...  \n",
       "1  Get HMAC-based one-time password on the basis ...  \n",
       "2  Generate a time-based one-time password (TOTP)...  \n",
       "3  Check if given token is valid for given secret...  \n",
       "4  Checks if a given time-based one-time password...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>function_name</th>\n",
       "      <th>input_code</th>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th>generated_doc</th>\n",
       "      <th>original_doc</th>\n",
       "      <th>prompt_used</th>\n",
       "      <th>clean_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>_is_possible_token</td>\n",
       "      <td>def _is_possible_token(token, token_length=6):...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>Zero-shot</td>\n",
       "      <td>```\\ndef _is_possible_token(\\n    token: int |...</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "      <td>As a senior Python engineer, write a professio...</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_hotp</td>\n",
       "      <td>def get_hotp(\\r\\n        secret,\\r\\n        in...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>Zero-shot</td>\n",
       "      <td>```\\ndef get_hotp(\\n    secret: str or bytes,\\...</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "      <td>As a senior Python engineer, write a professio...</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_totp</td>\n",
       "      <td>def get_totp(\\r\\n        secret,\\r\\n        as...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>Zero-shot</td>\n",
       "      <td>```\\ndef get_totp(\\n    secret,\\n    as_string...</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "      <td>As a senior Python engineer, write a professio...</td>\n",
       "      <td>Generate a time-based one-time password (TOTP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_hotp</td>\n",
       "      <td>def valid_hotp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>Zero-shot</td>\n",
       "      <td>```\\ndef valid_hotp(\\n    token: int or str,  ...</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "      <td>As a senior Python engineer, write a professio...</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_totp</td>\n",
       "      <td>def valid_totp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>Zero-shot</td>\n",
       "      <td>```\\n\"\"\"\\nChecks if a given time-based one-tim...</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "      <td>As a senior Python engineer, write a professio...</td>\n",
       "      <td>Checks if a given time-based one-time password...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:58:23.681267Z",
     "start_time": "2025-05-08T17:58:23.544102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "process_output_file('../output/llama_few.csv')\n",
    "process_output_file('../output/llama_chain.csv')\n",
    "process_output_file('../output/llama_structured.csv')\n",
    "process_output_file('../output/llama_oneshot.csv')"
   ],
   "id": "409a2b7b05be1fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: ../output/llama_few.csv\n",
      "Processed and saved: ../output/llama_chain.csv\n",
      "Processed and saved: ../output/llama_structured.csv\n",
      "Processed and saved: ../output/llama_oneshot.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             filename       function_name  \\\n",
       "0  tadeck/onetimepass  _is_possible_token   \n",
       "1  tadeck/onetimepass            get_hotp   \n",
       "2  tadeck/onetimepass            get_totp   \n",
       "3  tadeck/onetimepass          valid_hotp   \n",
       "4  tadeck/onetimepass          valid_totp   \n",
       "\n",
       "                                          input_code        model technique  \\\n",
       "0  def _is_possible_token(token, token_length=6):...  llama3.2:1b  One-shot   \n",
       "1  def get_hotp(\\r\\n        secret,\\r\\n        in...  llama3.2:1b  One-shot   \n",
       "2  def get_totp(\\r\\n        secret,\\r\\n        as...  llama3.2:1b  One-shot   \n",
       "3  def valid_hotp(\\r\\n        token,\\r\\n        s...  llama3.2:1b  One-shot   \n",
       "4  def valid_totp(\\r\\n        token,\\r\\n        s...  llama3.2:1b  One-shot   \n",
       "\n",
       "                                        original_doc  \\\n",
       "0  Determines if given value is acceptable as a t...   \n",
       "1  Get HMAC-based one-time password on the basis ...   \n",
       "2  Get time-based one-time password on the basis ...   \n",
       "3  Check if given token is valid for given secret...   \n",
       "4  Check if given token is valid time-based one-t...   \n",
       "\n",
       "                                         prompt_used  \\\n",
       "0  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "1  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "2  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "3  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "4  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "\n",
       "                                       generated_doc  \\\n",
       "0  Here is a possible docstring for the `_is_poss...   \n",
       "1  Here is a possible docstring for the `get_hotp...   \n",
       "2  Here is a possible docstring for the `get_totp...   \n",
       "3  Here is a possible docstring for the `valid_ho...   \n",
       "4  Here is a docstring for the `valid_totp` funct...   \n",
       "\n",
       "                                     clean_docstring  \n",
       "0  Determines if given value is acceptable as a t...  \n",
       "1  Get HMAC-based one-time password on the basis ...  \n",
       "2  Generate time-based one-time password (TOTP) b...  \n",
       "3  Check if given token is valid for given secret...  \n",
       "4  Checks if given TOTP (Time-Based One-Time Pass...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>function_name</th>\n",
       "      <th>input_code</th>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th>original_doc</th>\n",
       "      <th>prompt_used</th>\n",
       "      <th>generated_doc</th>\n",
       "      <th>clean_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>_is_possible_token</td>\n",
       "      <td>def _is_possible_token(token, token_length=6):...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>Here is a possible docstring for the `_is_poss...</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_hotp</td>\n",
       "      <td>def get_hotp(\\r\\n        secret,\\r\\n        in...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>Here is a possible docstring for the `get_hotp...</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_totp</td>\n",
       "      <td>def get_totp(\\r\\n        secret,\\r\\n        as...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>Here is a possible docstring for the `get_totp...</td>\n",
       "      <td>Generate time-based one-time password (TOTP) b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_hotp</td>\n",
       "      <td>def valid_hotp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>Here is a possible docstring for the `valid_ho...</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_totp</td>\n",
       "      <td>def valid_totp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>Here is a docstring for the `valid_totp` funct...</td>\n",
       "      <td>Checks if given TOTP (Time-Based One-Time Pass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a72a82dc7e4eab11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:59:15.817387Z",
     "start_time": "2025-05-08T17:59:15.757518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "process_output_file('../output/qwen_zero.csv')\n",
    "process_output_file('../output/qwen_few.csv')\n",
    "process_output_file('../output/qwen_chain.csv')\n",
    "process_output_file('../output/qwen_structured.csv')\n",
    "process_output_file('../output/qwen_oneshot.csv')"
   ],
   "id": "4b39bdf879e80f41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: ../output/qwen_zero.csv\n",
      "Processed and saved: ../output/qwen_few.csv\n",
      "Processed and saved: ../output/qwen_chain.csv\n",
      "Processed and saved: ../output/qwen_structured.csv\n",
      "Processed and saved: ../output/qwen_oneshot.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             filename       function_name  \\\n",
       "0  tadeck/onetimepass  _is_possible_token   \n",
       "1  tadeck/onetimepass            get_hotp   \n",
       "2  tadeck/onetimepass            get_totp   \n",
       "3  tadeck/onetimepass          valid_hotp   \n",
       "4  tadeck/onetimepass          valid_totp   \n",
       "\n",
       "                                          input_code               model  \\\n",
       "0  def _is_possible_token(token, token_length=6):...  qwen2.5-coder:0.5b   \n",
       "1  def get_hotp(\\r\\n        secret,\\r\\n        in...  qwen2.5-coder:0.5b   \n",
       "2  def get_totp(\\r\\n        secret,\\r\\n        as...  qwen2.5-coder:0.5b   \n",
       "3  def valid_hotp(\\r\\n        token,\\r\\n        s...  qwen2.5-coder:0.5b   \n",
       "4  def valid_totp(\\r\\n        token,\\r\\n        s...  qwen2.5-coder:0.5b   \n",
       "\n",
       "  technique                                       original_doc  \\\n",
       "0  One-shot  Determines if given value is acceptable as a t...   \n",
       "1  One-shot  Get HMAC-based one-time password on the basis ...   \n",
       "2  One-shot  Get time-based one-time password on the basis ...   \n",
       "3  One-shot  Check if given token is valid for given secret...   \n",
       "4  One-shot  Check if given token is valid time-based one-t...   \n",
       "\n",
       "                                         prompt_used  \\\n",
       "0  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "1  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "2  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "3  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "4  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "\n",
       "                                       generated_doc  \\\n",
       "0  ```python\\ndef _is_possible_token(token, token...   \n",
       "1  Here's a complete docstring for the `get_hotp`...   \n",
       "2  The following is a docstring for the given fun...   \n",
       "3  Here is the docstring for the `valid_hotp` fun...   \n",
       "4  ```python\\ndef valid_totp(\\n        token,\\n  ...   \n",
       "\n",
       "                                     clean_docstring  \n",
       "0  Determines if given value is acceptable as a t...  \n",
       "1  Get HMAC-based one-time password on the basis ...  \n",
       "2  Get time-based one-time password on the basis ...  \n",
       "3  Check if given token is valid for given secret...  \n",
       "4  Check if given token is valid time-based one-t...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>function_name</th>\n",
       "      <th>input_code</th>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th>original_doc</th>\n",
       "      <th>prompt_used</th>\n",
       "      <th>generated_doc</th>\n",
       "      <th>clean_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>_is_possible_token</td>\n",
       "      <td>def _is_possible_token(token, token_length=6):...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>```python\\ndef _is_possible_token(token, token...</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_hotp</td>\n",
       "      <td>def get_hotp(\\r\\n        secret,\\r\\n        in...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>Here's a complete docstring for the `get_hotp`...</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_totp</td>\n",
       "      <td>def get_totp(\\r\\n        secret,\\r\\n        as...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>The following is a docstring for the given fun...</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_hotp</td>\n",
       "      <td>def valid_hotp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>Here is the docstring for the `valid_hotp` fun...</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_totp</td>\n",
       "      <td>def valid_totp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>```python\\ndef valid_totp(\\n        token,\\n  ...</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:59:21.411027Z",
     "start_time": "2025-05-08T17:59:21.349914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "process_output_file('../output/deep_zero.csv')\n",
    "process_output_file('../output/deep_few.csv')\n",
    "process_output_file('../output/deep_chain.csv')\n",
    "process_output_file('../output/deep_structured.csv')\n",
    "process_output_file('../output/deep_oneshot.csv')"
   ],
   "id": "3a6c8e008a6d543e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: ../output/deep_zero.csv\n",
      "Processed and saved: ../output/deep_few.csv\n",
      "Processed and saved: ../output/deep_chain.csv\n",
      "Processed and saved: ../output/deep_structured.csv\n",
      "Processed and saved: ../output/deep_oneshot.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             filename       function_name  \\\n",
       "0  tadeck/onetimepass  _is_possible_token   \n",
       "1  tadeck/onetimepass            get_hotp   \n",
       "2  tadeck/onetimepass            get_totp   \n",
       "3  tadeck/onetimepass          valid_hotp   \n",
       "4  tadeck/onetimepass          valid_totp   \n",
       "\n",
       "                                          input_code             model  \\\n",
       "0  def _is_possible_token(token, token_length=6):...  deepseek-r1:1.5b   \n",
       "1  def get_hotp(\\r\\n        secret,\\r\\n        in...  deepseek-r1:1.5b   \n",
       "2  def get_totp(\\r\\n        secret,\\r\\n        as...  deepseek-r1:1.5b   \n",
       "3  def valid_hotp(\\r\\n        token,\\r\\n        s...  deepseek-r1:1.5b   \n",
       "4  def valid_totp(\\r\\n        token,\\r\\n        s...  deepseek-r1:1.5b   \n",
       "\n",
       "  technique                                       original_doc  \\\n",
       "0  One-shot  Determines if given value is acceptable as a t...   \n",
       "1  One-shot  Get HMAC-based one-time password on the basis ...   \n",
       "2  One-shot  Get time-based one-time password on the basis ...   \n",
       "3  One-shot  Check if given token is valid for given secret...   \n",
       "4  One-shot  Check if given token is valid time-based one-t...   \n",
       "\n",
       "                                         prompt_used  \\\n",
       "0  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "1  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "2  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "3  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "4  Here is one example:\\nFunction:\\ndef add(a, b)...   \n",
       "\n",
       "                                       generated_doc  \\\n",
       "0  <think>\\nAlright, so I'm looking at this probl...   \n",
       "1  <think>\\nAlright, I need to write a Python doc...   \n",
       "2  <think>\\nAlright, let's tackle this problem st...   \n",
       "3  <think>\\nAlright, I need to write a docstring ...   \n",
       "4  <think>\\nAlright, let's tackle this problem st...   \n",
       "\n",
       "                                     clean_docstring  \n",
       "0  Determines if given value is acceptable as a t...  \n",
       "1  >>> get_hotp(b'MFRGGZDFMZTWQ2LK', intervals_no...  \n",
       "2  Get time-based one-time password (OTTP) on the...  \n",
       "3  >>> secret = b'MFRGGZDFMZTWQ2LK'\\n>>> valid_ho...  \n",
       "4  >>> secret = b'MFRGGZDFMZTWQ2LK'\\n>>> token = ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>function_name</th>\n",
       "      <th>input_code</th>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th>original_doc</th>\n",
       "      <th>prompt_used</th>\n",
       "      <th>generated_doc</th>\n",
       "      <th>clean_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>_is_possible_token</td>\n",
       "      <td>def _is_possible_token(token, token_length=6):...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, so I'm looking at this probl...</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_hotp</td>\n",
       "      <td>def get_hotp(\\r\\n        secret,\\r\\n        in...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, I need to write a Python doc...</td>\n",
       "      <td>&gt;&gt;&gt; get_hotp(b'MFRGGZDFMZTWQ2LK', intervals_no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_totp</td>\n",
       "      <td>def get_totp(\\r\\n        secret,\\r\\n        as...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, let's tackle this problem st...</td>\n",
       "      <td>Get time-based one-time password (OTTP) on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_hotp</td>\n",
       "      <td>def valid_hotp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, I need to write a docstring ...</td>\n",
       "      <td>&gt;&gt;&gt; secret = b'MFRGGZDFMZTWQ2LK'\\n&gt;&gt;&gt; valid_ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_totp</td>\n",
       "      <td>def valid_totp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "      <td>Here is one example:\\nFunction:\\ndef add(a, b)...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, let's tackle this problem st...</td>\n",
       "      <td>&gt;&gt;&gt; secret = b'MFRGGZDFMZTWQ2LK'\\n&gt;&gt;&gt; token = ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:01:01.118412Z",
     "start_time": "2025-05-08T18:01:01.096953Z"
    }
   },
   "cell_type": "code",
   "source": "test = pd.read_csv('../output/qwen_structured.csv')",
   "id": "f004a441e9c199c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             filename       function_name  \\\n",
       "0  tadeck/onetimepass  _is_possible_token   \n",
       "1  tadeck/onetimepass            get_hotp   \n",
       "2  tadeck/onetimepass            get_totp   \n",
       "3  tadeck/onetimepass          valid_hotp   \n",
       "4  tadeck/onetimepass          valid_totp   \n",
       "\n",
       "                                          input_code               model  \\\n",
       "0  def _is_possible_token(token, token_length=6):...  qwen2.5-coder:0.5b   \n",
       "1  def get_hotp(\\r\\n        secret,\\r\\n        in...  qwen2.5-coder:0.5b   \n",
       "2  def get_totp(\\r\\n        secret,\\r\\n        as...  qwen2.5-coder:0.5b   \n",
       "3  def valid_hotp(\\r\\n        token,\\r\\n        s...  qwen2.5-coder:0.5b   \n",
       "4  def valid_totp(\\r\\n        token,\\r\\n        s...  qwen2.5-coder:0.5b   \n",
       "\n",
       "    technique                                       original_doc  \\\n",
       "0  Structured  Determines if given value is acceptable as a t...   \n",
       "1  Structured  Get HMAC-based one-time password on the basis ...   \n",
       "2  Structured  Get time-based one-time password on the basis ...   \n",
       "3  Structured  Check if given token is valid for given secret...   \n",
       "4  Structured  Check if given token is valid time-based one-t...   \n",
       "\n",
       "                                         prompt_used  \\\n",
       "0  You are a senior Python developer and technica...   \n",
       "1  You are a senior Python developer and technica...   \n",
       "2  You are a senior Python developer and technica...   \n",
       "3  You are a senior Python developer and technica...   \n",
       "4  You are a senior Python developer and technica...   \n",
       "\n",
       "                                       generated_doc  \\\n",
       "0  ```python\\n\"\"\"Determines if given value is acc...   \n",
       "1  ```python\\ndef get_hotp(secret, intervals_no, ...   \n",
       "2  ```python\\ndef get_totp(secret, as_string=Fals...   \n",
       "3                                                ```   \n",
       "4  ```python\\n# Function to validate TOTP for a g...   \n",
       "\n",
       "                                     clean_docstring  \n",
       "0  Determines if given value is acceptable as a t...  \n",
       "1  Get HMAC-based one-time password on the basis ...  \n",
       "2  Get time-based one-time password on the basis ...  \n",
       "3                                                ```  \n",
       "4  Check if given token is valid time-based one-t...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>function_name</th>\n",
       "      <th>input_code</th>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th>original_doc</th>\n",
       "      <th>prompt_used</th>\n",
       "      <th>generated_doc</th>\n",
       "      <th>clean_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>_is_possible_token</td>\n",
       "      <td>def _is_possible_token(token, token_length=6):...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>Structured</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "      <td>You are a senior Python developer and technica...</td>\n",
       "      <td>```python\\n\"\"\"Determines if given value is acc...</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_hotp</td>\n",
       "      <td>def get_hotp(\\r\\n        secret,\\r\\n        in...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>Structured</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "      <td>You are a senior Python developer and technica...</td>\n",
       "      <td>```python\\ndef get_hotp(secret, intervals_no, ...</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>get_totp</td>\n",
       "      <td>def get_totp(\\r\\n        secret,\\r\\n        as...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>Structured</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "      <td>You are a senior Python developer and technica...</td>\n",
       "      <td>```python\\ndef get_totp(secret, as_string=Fals...</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_hotp</td>\n",
       "      <td>def valid_hotp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>Structured</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "      <td>You are a senior Python developer and technica...</td>\n",
       "      <td>```</td>\n",
       "      <td>```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tadeck/onetimepass</td>\n",
       "      <td>valid_totp</td>\n",
       "      <td>def valid_totp(\\r\\n        token,\\r\\n        s...</td>\n",
       "      <td>qwen2.5-coder:0.5b</td>\n",
       "      <td>Structured</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "      <td>You are a senior Python developer and technica...</td>\n",
       "      <td>```python\\n# Function to validate TOTP for a g...</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:03:03.107287Z",
     "start_time": "2025-05-08T18:03:03.079222Z"
    }
   },
   "cell_type": "code",
   "source": "test[['function_name', 'original_doc', 'clean_docstring']]",
   "id": "93bd2260199a6b33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        function_name                                       original_doc  \\\n",
       "0  _is_possible_token  Determines if given value is acceptable as a t...   \n",
       "1            get_hotp  Get HMAC-based one-time password on the basis ...   \n",
       "2            get_totp  Get time-based one-time password on the basis ...   \n",
       "3          valid_hotp  Check if given token is valid for given secret...   \n",
       "4          valid_totp  Check if given token is valid time-based one-t...   \n",
       "\n",
       "                                     clean_docstring  \n",
       "0  Determines if given value is acceptable as a t...  \n",
       "1  Get HMAC-based one-time password on the basis ...  \n",
       "2  Get time-based one-time password on the basis ...  \n",
       "3                                                ```  \n",
       "4  Check if given token is valid time-based one-t...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_name</th>\n",
       "      <th>original_doc</th>\n",
       "      <th>clean_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_is_possible_token</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "      <td>Determines if given value is acceptable as a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get_hotp</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "      <td>Get HMAC-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get_totp</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "      <td>Get time-based one-time password on the basis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>valid_hotp</td>\n",
       "      <td>Check if given token is valid for given secret...</td>\n",
       "      <td>```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>valid_totp</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "      <td>Check if given token is valid time-based one-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training y testing y tuning",
   "id": "d173830bbf019441"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f3256bd3134f4375"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:05:38.163120Z",
     "start_time": "2025-05-08T19:05:38.150858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2: Evaluation Function\n",
    "def evaluate_docstrings(df):\n",
    "    \"\"\"\n",
    "    Evaluate generated docstrings against original docstrings using multiple metrics.\n",
    "    Returns a dictionary of scores.\n",
    "    \"\"\"\n",
    "    # Load sentence transformer model for semantic similarity\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Filter out empty docstrings\n",
    "    valid_pairs = df[(df['clean_docstring'].str.len() > 0) & \n",
    "                    (df['original_doc'].str.len() > 0)].copy()\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        return {\"error\": \"No valid docstring pairs to evaluate\"}\n",
    "    \n",
    "    # Calculate exact match (strict comparison)\n",
    "    valid_pairs['exact_match'] = valid_pairs.apply(\n",
    "        lambda x: x['clean_docstring'].strip() == x['original_doc'].strip(), axis=1)\n",
    "    results['exact_match_accuracy'] = valid_pairs['exact_match'].mean()\n",
    "    \n",
    "    # Calculate semantic similarity using cosine similarity\n",
    "    original_embeddings = model.encode(valid_pairs['original_doc'].tolist())\n",
    "    generated_embeddings = model.encode(valid_pairs['clean_docstring'].tolist())\n",
    "    cosine_scores = util.cos_sim(original_embeddings, generated_embeddings)\n",
    "    results['mean_cosine_similarity'] = np.mean(np.diag(cosine_scores))\n",
    "    \n",
    "    # Calculate ROUGE scores (for text similarity)\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(\n",
    "        valid_pairs['clean_docstring'].tolist(),\n",
    "        valid_pairs['original_doc'].tolist(),\n",
    "        avg=True)\n",
    "    results.update({\n",
    "        'rouge-1': rouge_scores['rouge-1']['f'],\n",
    "        'rouge-2': rouge_scores['rouge-2']['f'],\n",
    "        'rouge-l': rouge_scores['rouge-l']['f']\n",
    "    })\n",
    "    \n",
    "    return results"
   ],
   "id": "465d792133eacfc6",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:05:41.037275Z",
     "start_time": "2025-05-08T19:05:41.026676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3: Training Data Preparation\n",
    "def prepare_training_data(df):\n",
    "    \"\"\"Prepare dataset for fine-tuning\"\"\"\n",
    "    # Filter out empty samples\n",
    "    df = df[(df['input_code'].str.len() > 0) & \n",
    "            (df['original_doc'].str.len() > 0)].copy()\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_pandas(df[['input_code', 'original_doc']])\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = [\"Generate docstring: \" + code for code in examples[\"input_code\"]]\n",
    "        targets = examples[\"original_doc\"]\n",
    "        model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "        \n",
    "        # Setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    return tokenized_dataset, tokenizer"
   ],
   "id": "8d8179d482ef2bdb",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:05:44.340988Z",
     "start_time": "2025-05-08T19:05:44.331276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Updated Cell 4: Model Training \n",
    "from accelerate import Accelerator\n",
    "\n",
    "def train_model(df, model_name=\"t5-small\", output_dir=\"../output\"):\n",
    "    \"\"\"Fine-tune a model on docstring generation\"\"\"\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Prepare data\n",
    "    tokenized_dataset, tokenizer = prepare_training_data(df)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True if accelerator.mixed_precision == \"fp16\" else False  # Auto-detect mixed precision\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Accelerate preparation\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    return trainer, tokenizer"
   ],
   "id": "1c857d006c5f4da4",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:05:47.814462Z",
     "start_time": "2025-05-08T19:05:47.806344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 5: Model Evaluation\n",
    "def evaluate_all_models(output_dir):\n",
    "    \"\"\"Evaluate all model outputs in the directory\"\"\"\n",
    "    results = []\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    for file in output_path.glob('*.csv'):\n",
    "        if file.is_file():\n",
    "            df = pd.read_csv(file)\n",
    "            model_name = file.stem.split('_')[0]\n",
    "            technique = '_'.join(file.stem.split('_')[1:-1])\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = evaluate_docstrings(df)\n",
    "            \n",
    "            # Add metadata\n",
    "            metrics.update({\n",
    "                'model': model_name,\n",
    "                'technique': technique,\n",
    "                'file': file.name,\n",
    "                'num_samples': len(df)\n",
    "            })\n",
    "            \n",
    "            results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "id": "f60402c3aada56c9",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:05:50.838619Z",
     "start_time": "2025-05-08T19:05:50.829943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 6: Generate Predictions\n",
    "def generate_predictions(trainer, tokenizer, df):\n",
    "    \"\"\"Generate predictions using trained model\"\"\"\n",
    "    # Prepare dataset\n",
    "    tokenized_dataset, _ = prepare_training_data(df)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = trainer.predict(tokenized_dataset)\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        predictions.predictions,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    # Add to DataFrame\n",
    "    result_df = df.copy()\n",
    "    result_df['model_prediction'] = decoded_preds[:len(result_df)]\n",
    "    \n",
    "    return result_df"
   ],
   "id": "439c63dcb6182403",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:07:00.826249Z",
     "start_time": "2025-05-08T19:06:59.836772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 7: Execute Pipeline (Final Version)\n",
    "import torch\n",
    "from transformers import __version__ as transformers_version\n",
    "from accelerate import __version__ as accelerate_version\n",
    "\n",
    "# Reduce verbosity of transformers logging\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "try:\n",
    "    # 1. First check the output directory\n",
    "    output_dir = Path('../output')\n",
    "    csv_files = list(output_dir.glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {output_dir.absolute()}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} files to process\")\n",
    "    \n",
    "    # 2. Evaluate all models\n",
    "    print(\"\\nEvaluating all models...\")\n",
    "    eval_results = evaluate_all_models('output/')\n",
    "    eval_results.to_csv('evaluation_results.csv', index=False)\n",
    "    display(eval_results)\n",
    "    \n",
    "    # 3. Combine all data for training\n",
    "    print(\"\\nLoading datasets for training...\")\n",
    "    all_data = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            if not df.empty:\n",
    "                print(f\"Loaded {file.name} ({len(df)} rows)\")\n",
    "                all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No valid data loaded - cannot proceed with training\")\n",
    "    \n",
    "    full_dataset = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nCombined dataset contains {len(full_dataset)} rows\")\n",
    "    \n",
    "    # 4. Train the model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    trainer, tokenizer = train_model(full_dataset)\n",
    "    print(\"Training completed successfully\")\n",
    "    \n",
    "    # 5. Generate and save predictions\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    final_predictions = generate_predictions(trainer, tokenizer, full_dataset)\n",
    "    final_predictions.to_csv('final_predictions.csv', index=False)\n",
    "    print(\"Predictions saved to final_predictions.csv\")\n",
    "    \n",
    "    # 6. Evaluate trained model performance\n",
    "    print(\"\\nEvaluating trained model...\")\n",
    "    trained_model_metrics = evaluate_docstrings(final_predictions)\n",
    "    print(\"\\nTrained Model Performance:\")\n",
    "    print(json.dumps(trained_model_metrics, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError in pipeline: {str(e)}\")\n",
    "    raise"
   ],
   "id": "e02e0dfdadd504c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 files to process\n",
      "\n",
      "Evaluating all models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets for training...\n",
      "Loaded deep_chain.csv (5 rows)\n",
      "Loaded deep_few.csv (5 rows)\n",
      "Loaded deep_oneshot.csv (5 rows)\n",
      "Loaded deep_structured.csv (5 rows)\n",
      "Loaded deep_zero.csv (5 rows)\n",
      "Loaded generated_docs_.csv (5 rows)\n",
      "Loaded llama_chain.csv (5 rows)\n",
      "Loaded llama_few.csv (5 rows)\n",
      "Loaded llama_oneshot.csv (5 rows)\n",
      "Loaded llama_structured.csv (5 rows)\n",
      "Loaded llama_zero.csv (5 rows)\n",
      "Loaded qwen_chain.csv (5 rows)\n",
      "Loaded qwen_few.csv (5 rows)\n",
      "Loaded qwen_oneshot.csv (5 rows)\n",
      "Loaded qwen_structured.csv (5 rows)\n",
      "Loaded qwen_zero.csv (5 rows)\n",
      "\n",
      "Combined dataset contains 80 rows\n",
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cc822e0e3a04829b4fbf424a0d50579"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adenk\\PycharmProjects\\code-to-doc\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in pipeline: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[95]\u001B[39m\u001B[32m, line 46\u001B[39m\n\u001B[32m     44\u001B[39m \u001B[38;5;66;03m# 4. Train the model\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mStarting model training...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m trainer, tokenizer = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTraining completed successfully\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     49\u001B[39m \u001B[38;5;66;03m# 5. Generate and save predictions\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[90]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(df, model_name, output_dir)\u001B[39m\n\u001B[32m     10\u001B[39m tokenized_dataset, tokenizer = prepare_training_data(df)\n\u001B[32m     11\u001B[39m model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m training_args = \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mepoch\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_total_limit\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreport_to\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mnone\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mepoch\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mepoch\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfp16\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmixed_precision\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfp16\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Auto-detect mixed precision\u001B[39;49;00m\n\u001B[32m     26\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m trainer = Trainer(\n\u001B[32m     29\u001B[39m     model=model,\n\u001B[32m     30\u001B[39m     args=training_args,\n\u001B[32m     31\u001B[39m     train_dataset=tokenized_dataset,\n\u001B[32m     32\u001B[39m     tokenizer=tokenizer,\n\u001B[32m     33\u001B[39m )\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# Accelerate preparation\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:132\u001B[39m, in \u001B[36m__init__\u001B[39m\u001B[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\code-to-doc\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1761\u001B[39m, in \u001B[36mTrainingArguments.__post_init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1759\u001B[39m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[32m   1760\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_available():\n\u001B[32m-> \u001B[39m\u001B[32m1761\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m   1763\u001B[39m \u001B[38;5;66;03m# Disable average tokens when using single device\u001B[39;00m\n\u001B[32m   1764\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.average_tokens_across_devices:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\code-to-doc\\.venv\\Lib\\site-packages\\transformers\\training_args.py:2297\u001B[39m, in \u001B[36mTrainingArguments.device\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2293\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2294\u001B[39m \u001B[33;03mThe device used by this process.\u001B[39;00m\n\u001B[32m   2295\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2296\u001B[39m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[33m\"\u001B[39m\u001B[33mtorch\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m-> \u001B[39m\u001B[32m2297\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_setup_devices\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\code-to-doc\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001B[39m, in \u001B[36mcached_property.__get__\u001B[39m\u001B[34m(self, obj, objtype)\u001B[39m\n\u001B[32m     65\u001B[39m cached = \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     cached = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\code-to-doc\\.venv\\Lib\\site-packages\\transformers\\training_args.py:2167\u001B[39m, in \u001B[36mTrainingArguments._setup_devices\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2165\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[32m   2166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[32m-> \u001B[39m\u001B[32m2167\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m   2168\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`: \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2169\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[33m'\u001B[39m\u001B[33maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2170\u001B[39m         )\n\u001B[32m   2171\u001B[39m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[32m   2172\u001B[39m accelerator_state_kwargs = {\u001B[33m\"\u001B[39m\u001B[33menabled\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33muse_configured_state\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[31mImportError\u001B[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:41:39.765004Z",
     "start_time": "2025-05-08T18:41:00.129752Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers sentence-transformers rouge-score pandas datasets torch",
   "id": "6490e198c114dffe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (4.1.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: torch in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Obtaining dependency information for absl-py from https://files.pythonhosted.org/packages/f6/d4/349f7f4bd5ea92dab34f5bb0fe31775ef6c311427a14d5a5b31ecb442341/absl_py-2.2.2-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nltk (from rouge-score)\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from torch) (80.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting click (from nltk->rouge-score)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\adenk\\pycharmprojects\\code-to-doc\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.6 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/135.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.6/135.6 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.5 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 98.2/98.2 kB 8.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25023 sha256=1913a24410263edebb6430b2a52915585e87440545266102999c1218d6f8c318\n",
      "  Stored in directory: c:\\users\\adenk\\appdata\\local\\pip\\cache\\wheels\\44\\af\\da\\5ffc433e2786f0b1a9c6f458d5fb8f611d8eb332387f18698f\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: click, absl-py, nltk, rouge-score\n",
      "Successfully installed absl-py-2.2.2 click-8.1.8 nltk-3.9.1 rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9abc372b07738de3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
