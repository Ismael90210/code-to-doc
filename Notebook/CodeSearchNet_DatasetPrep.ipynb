{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d77f64-0366-479c-b68c-a586d3d4f783",
   "metadata": {},
   "source": [
    "# Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cfa3267-08df-4800-b0b0-2ec16d775cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install unsloth\n",
    "# Get latest Unsloth\n",
    "# !pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246840c1-53d5-432c-97c6-41c4a7b41a49",
   "metadata": {},
   "source": [
    "# Load the dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f673f4-1e95-46cb-b274-281b696af120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a62ecd518d6473fa1c18ded59da270d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f13b646e-1434-48b7-aef5-b6aa8b672d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"espejelomar/code_search_net_python_10000_examples\")\n",
    "\n",
    "# Print available splits\n",
    "print(\"Available splits:\", list(dataset.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d7bc67d-01f4-4c1e-9501-05f495f64e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['Unnamed: 0', 'repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url']\n"
     ]
    }
   ],
   "source": [
    "# Access the train split\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Print column names\n",
    "column_names = train_dataset.column_names\n",
    "print(\"Column names:\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fcc370b-2ef7-44f7-b4d6-cfe035fac85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 0, 'repository_name': 'getsentry/libsourcemap', 'func_path_in_repository': 'libsourcemap/highlevel.py', 'func_name': 'View.get_original_function_name', 'whole_func_string': 'def get_original_function_name(self, line, col, minified_name,\\n                                   minified_source):\\n        \"\"\"Given a token location and a minified function name and the\\n        minified source file this returns the original function name if it\\n        can be found of the minified function in scope.\\n        \"\"\"\\n        # Silently ignore underflows\\n        if line < 0 or col < 0:\\n            return None\\n        minified_name = minified_name.encode(\\'utf-8\\')\\n        sout = _ffi.new(\\'const char **\\')\\n        try:\\n            slen = rustcall(_lib.lsm_view_get_original_function_name,\\n                            self._get_ptr(), line, col, minified_name,\\n                            minified_source, sout)\\n            if slen > 0:\\n                return _ffi.unpack(sout[0], slen).decode(\\'utf-8\\', \\'replace\\')\\n        except SourceMapError:\\n            # In some rare cases the library is/was known to panic.  We do\\n            # not want to report this upwards  (this happens on slicing\\n            # out of range on older rust versions in the rust-sourcemap\\n            # library)\\n            pass', 'language': 'python', 'func_code_string': 'def get_original_function_name(self, line, col, minified_name,\\n                                   minified_source):\\n        \"\"\"Given a token location and a minified function name and the\\n        minified source file this returns the original function name if it\\n        can be found of the minified function in scope.\\n        \"\"\"\\n        # Silently ignore underflows\\n        if line < 0 or col < 0:\\n            return None\\n        minified_name = minified_name.encode(\\'utf-8\\')\\n        sout = _ffi.new(\\'const char **\\')\\n        try:\\n            slen = rustcall(_lib.lsm_view_get_original_function_name,\\n                            self._get_ptr(), line, col, minified_name,\\n                            minified_source, sout)\\n            if slen > 0:\\n                return _ffi.unpack(sout[0], slen).decode(\\'utf-8\\', \\'replace\\')\\n        except SourceMapError:\\n            # In some rare cases the library is/was known to panic.  We do\\n            # not want to report this upwards  (this happens on slicing\\n            # out of range on older rust versions in the rust-sourcemap\\n            # library)\\n            pass', 'func_code_tokens': '[\\'def\\', \\'get_original_function_name\\', \\'(\\', \\'self\\', \\',\\', \\'line\\', \\',\\', \\'col\\', \\',\\', \\'minified_name\\', \\',\\', \\'minified_source\\', \\')\\', \\':\\', \\'# Silently ignore underflows\\', \\'if\\', \\'line\\', \\'<\\', \\'0\\', \\'or\\', \\'col\\', \\'<\\', \\'0\\', \\':\\', \\'return\\', \\'None\\', \\'minified_name\\', \\'=\\', \\'minified_name\\', \\'.\\', \\'encode\\', \\'(\\', \"\\'utf-8\\'\", \\')\\', \\'sout\\', \\'=\\', \\'_ffi\\', \\'.\\', \\'new\\', \\'(\\', \"\\'const char **\\'\", \\')\\', \\'try\\', \\':\\', \\'slen\\', \\'=\\', \\'rustcall\\', \\'(\\', \\'_lib\\', \\'.\\', \\'lsm_view_get_original_function_name\\', \\',\\', \\'self\\', \\'.\\', \\'_get_ptr\\', \\'(\\', \\')\\', \\',\\', \\'line\\', \\',\\', \\'col\\', \\',\\', \\'minified_name\\', \\',\\', \\'minified_source\\', \\',\\', \\'sout\\', \\')\\', \\'if\\', \\'slen\\', \\'>\\', \\'0\\', \\':\\', \\'return\\', \\'_ffi\\', \\'.\\', \\'unpack\\', \\'(\\', \\'sout\\', \\'[\\', \\'0\\', \\']\\', \\',\\', \\'slen\\', \\')\\', \\'.\\', \\'decode\\', \\'(\\', \"\\'utf-8\\'\", \\',\\', \"\\'replace\\'\", \\')\\', \\'except\\', \\'SourceMapError\\', \\':\\', \\'# In some rare cases the library is/was known to panic.  We do\\', \\'# not want to report this upwards  (this happens on slicing\\', \\'# out of range on older rust versions in the rust-sourcemap\\', \\'# library)\\', \\'pass\\']', 'func_documentation_string': 'Given a token location and a minified function name and the\\n        minified source file this returns the original function name if it\\n        can be found of the minified function in scope.', 'func_documentation_tokens': \"['Given', 'a', 'token', 'location', 'and', 'a', 'minified', 'function', 'name', 'and', 'the', 'minified', 'source', 'file', 'this', 'returns', 'the', 'original', 'function', 'name', 'if', 'it', 'can', 'be', 'found', 'of', 'the', 'minified', 'function', 'in', 'scope', '.']\", 'split_name': 'train', 'func_code_url': 'https://github.com/getsentry/libsourcemap/blob/94b5a34814fafee9dc23da8ec0ccca77f30e3370/libsourcemap/highlevel.py#L163-L185'}\n"
     ]
    }
   ],
   "source": [
    "# Access the first example\n",
    "first_example = train_dataset[0]\n",
    "print(first_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6286d6f2-03ba-4ae2-94b8-aa7999a9e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Unnamed: 0' column if present\n",
    "if 'Unnamed: 0' in train_dataset.column_names:\n",
    "    train_dataset = train_dataset.remove_columns(['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef0e8efa-d856-46cc-85c4-699b2f50c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in train dataset: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url']\n",
      "Number of examples in train dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "# Verify the columns and size of the train dataset\n",
    "\n",
    "print(\"Columns in train dataset:\", train_dataset.column_names)\n",
    "print(f\"Number of examples in train dataset: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aff90b5-7a6d-469d-a67e-6e1a57d4e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filtered dataset with only the first 1000 examples\n",
    "subset_size = 1000\n",
    "dataset = train_dataset.select(range(subset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a6c152-76ee-4c06-895d-04a38d1fb483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in filtered dataset: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url']\n",
      "Number of examples in filtered dataset: 1000\n"
     ]
    }
   ],
   "source": [
    "# Verify the columns and size of the filtered dataset\n",
    "\n",
    "print(\"Columns in filtered dataset:\", dataset.column_names)\n",
    "print(f\"Number of examples in filtered dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5be93b-876a-4183-929d-561b218f8f1a",
   "metadata": {},
   "source": [
    "# Convert dataset to ShareGPT format with proper variable substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a2a8a29-96f0-41e4-934c-c9b8992ca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sharegpt(dataset, merged_prompt, output_column_name, conversation_extension=1):\n",
    "    \"\"\"\n",
    "    Convert dataset to ShareGPT format with proper variable substitution\n",
    "\n",
    "    Args:\n",
    "        dataset: The source dataset\n",
    "        merged_prompt: Template string with {column_name} placeholders\n",
    "        output_column_name: Column to use as the output/completion\n",
    "        conversation_extension: Number of examples to combine into a single conversation\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "\n",
    "    for i in range(0, len(dataset), conversation_extension):\n",
    "        conversation = []\n",
    "\n",
    "        # Process each example in the current conversation window\n",
    "        for j in range(i, min(i + conversation_extension, len(dataset))):\n",
    "            example = dataset[j]\n",
    "\n",
    "            # Format the prompt by substituting variables\n",
    "            prompt = merged_prompt\n",
    "            for column in dataset.column_names:\n",
    "                if column in merged_prompt and column in example:\n",
    "                    placeholder = \"{\" + column + \"}\"\n",
    "                    prompt = prompt.replace(placeholder, str(example[column]))\n",
    "\n",
    "            # Add the human message\n",
    "            conversation.append({\n",
    "                \"from\": \"human\",\n",
    "                \"value\": prompt\n",
    "            })\n",
    "\n",
    "            # Add the assistant message\n",
    "            conversation.append({\n",
    "                \"from\": \"assistant\",\n",
    "                \"value\": example[output_column_name]\n",
    "            })\n",
    "\n",
    "        # Add the conversation to the formatted data\n",
    "        formatted_data.append({\"conversations\": conversation})\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "352bb0e9-c6f3-4f54-8c46-1109ca26c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code explanation\n",
    "code_explain_dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"Explain what this Python code does: {func_code_string}\",\n",
    "    output_column_name = \"func_documentation_string\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7e7feea-3153-4061-87ad-c0c90845a2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'Explain what this Python code does: def get_original_function_name(self, line, col, minified_name,\\n                                   minified_source):\\n        \"\"\"Given a token location and a minified function name and the\\n        minified source file this returns the original function name if it\\n        can be found of the minified function in scope.\\n        \"\"\"\\n        # Silently ignore underflows\\n        if line < 0 or col < 0:\\n            return None\\n        minified_name = minified_name.encode(\\'utf-8\\')\\n        sout = _ffi.new(\\'const char **\\')\\n        try:\\n            slen = rustcall(_lib.lsm_view_get_original_function_name,\\n                            self._get_ptr(), line, col, minified_name,\\n                            minified_source, sout)\\n            if slen > 0:\\n                return _ffi.unpack(sout[0], slen).decode(\\'utf-8\\', \\'replace\\')\\n        except SourceMapError:\\n            # In some rare cases the library is/was known to panic.  We do\\n            # not want to report this upwards  (this happens on slicing\\n            # out of range on older rust versions in the rust-sourcemap\\n            # library)\\n            pass'},\n",
       "  {'from': 'assistant',\n",
       "   'value': 'Given a token location and a minified function name and the\\n        minified source file this returns the original function name if it\\n        can be found of the minified function in scope.'}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_explain_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4ce00-1654-4efb-9efe-efab71d5483e",
   "metadata": {},
   "source": [
    "# Initialize the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7dfac54-5e80-4cfe-9887-64843e3456fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: We'll be using `/tmp/unsloth_compiled_cache` for temporary Unsloth patches.\n",
      "Standard import failed for UnslothBCOTrainer: No module named 'UnslothBCOTrainer'. Using tempfile instead!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA RTX A5000. Num GPUs = 1. Max memory: 23.573 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adb08dd8b094db6808b8a2c3da0b782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/457M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f68c00075094fb1855153bf3639af60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487b006f1fba4a82a409b2012aebec94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7014790ce33641439d600ce6e05cf60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d7ce867b9847a4bf7d50170c9edbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50487bde2ddb4dd29259e2a5d67babcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0b03212d894fc28031ff98f4b6aa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed35f7a8c3b54e068498a3268cfccf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama3.2-1b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cdbaa40-eca9-472a-9086-4b8b87896614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# First, convert your list to a Hugging Face Dataset\n",
    "code_explain_dataset_hf = Dataset.from_list(code_explain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d93312bb-83f2-46de-af19-4df2815157cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb725bcadc04fe79149e6ef1b22fc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=128):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(code_explain_dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c105efb5-cc45-495e-9e54-bb219e09416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b49011e5de4420da3396c8be91c74e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import apply_chat_template\n",
    "chat_template = \"\"\"\n",
    "{SYSTEM}\n",
    "USER: {INPUT}\n",
    "ASSISTANT: {OUTPUT}\"\"\"\n",
    "\n",
    "default_system_message = \"\"\"You are an expert Python programmer. Write clean, efficient, and well-documented code\n",
    "that follows PEP 8 style guidelines.\"\"\"\n",
    "\n",
    "# Use this system message with the apply_chat_template function\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template,\n",
    "    default_system_message = default_system_message\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb2283ef-8490-4a1e-a6db-2105098f7ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'Explain what this Python code does: def update_views(self):\\n        \"\"\"Update stats views.\"\"\"\\n        # Call the father\\'s method\\n        super(Plugin, self).update_views()\\n\\n        # Add specifics informations\\n        # Alert and log\\n        self.views[\\'used\\'][\\'decoration\\'] = self.get_alert_log(self.stats[\\'used\\'], maximum=self.stats[\\'total\\'])',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Update stats views.', 'role': 'assistant'}],\n",
       " 'text': 'You are an expert Python programmer. Write clean, efficient, and well-documented code\\nthat follows PEP 8 style guidelines.\\nUSER: Explain what this Python code does: def update_views(self):\\n        \"\"\"Update stats views.\"\"\"\\n        # Call the father\\'s method\\n        super(Plugin, self).update_views()\\n\\n        # Add specifics informations\\n        # Alert and log\\n        self.views[\\'used\\'][\\'decoration\\'] = self.get_alert_log(self.stats[\\'used\\'], maximum=self.stats[\\'total\\'])\\nASSISTANT: Update stats views.<|im_end|>'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52855ca1-c8f0-42f4-a717-83fd3c3610c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
